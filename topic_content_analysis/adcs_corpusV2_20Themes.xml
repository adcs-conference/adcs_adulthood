<?xml version="1.0" ?>
<themes>
  <theme id="1" title="Theme 1">
    <words>
      <word weight="4.84344941817">
        search
      </word>
      <word weight="3.86795006179">
        user
      </word>
      <word weight="3.35183907865">
        queri
      </word>
      <word weight="3.09005600939">
        task
      </word>
      <word weight="2.20459425738">
        perform
      </word>
      <word weight="1.71889660468">
        reformul
      </word>
    </words>
    <responses>
      <response id="136" uni="Queensland University of Technology" weight="2.70" year="2012">
        Relationship between the nature of the Search Task Types and Query Reformulation Behaviour  Success of query reformulation and relevant information retrieval depends on many factors, such as users' prior knowledge, age, gender, and cognitive styles. One of the important factors that affect a user's query reformulation behaviour is that of the nature of the search tasks. Limited studies have examined the impact of the search task types on query reformulation behaviour while performing Web searches. This paper examines how the nature of the search tasks affects users' query reformulation behaviour during information searching. The paper reports empirical results from a user study in which 50 participants performed a set of three Web search tasks - exploratory, factorial and abstract. Users' interactions with search engines were logged by using a monitoring program. 872 unique search queries were classified into five query types - New, Add, Remove, Replace and Repeat. Users submitted fewer queries for the factual task, which accounted for 26%. They completed a higher number of queries (40% of the total queries) while carrying out the exploratory task. A one-way MANOVA test indicated a significant effect of search task types on users' query reformulation behaviour. In particular, the search task types influenced the manner in which users reformulated the New and Repeat queries. Keywords Query reformulation behaviour, information behaviour, information retrieval, search task complexity, user studies
      </response>
      <response id="72" uni="RMIT University" weight="1.29" year="2007">
        Predicting Query Performance for User-based Search Tasks  Query performance prediction aims to determine in advance whether a user's search request will return a useful answer set. The success of such prediction attempts are currently evaluated by calculating the correlation between the predicted performance and standard information retrieval metrics of system performance such as average precision. However, recent work suggests that there is little relationship between average precision and the performance of users when carrying out search tasks. Direct measures of user performance offer another way of evaluating the effectiveness of search systems; this is of particular importance in the framework of query prediction, since one of the goals of prediction is to warn users when search results are likely to be poor. We therefore investigate the relationship between current prediction techniques and user-based performance measures. Our preliminary results show that the performance of the predictors differs strongly when using system-based compared to user-based performance measures: predictors that are significantly correlated with one measurement are often not correlated with the other. In general, the predictors are more correlated with average precision rather than with user performance.  Query performance prediction, information retrieval, user study
      </response>
      <response id="114" uni="RMIT University" weight="0.71" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="74" uni="RMIT University" weight="0.59" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.57" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="19" uni="Australian National University  CSIRO Mathematical and Information Sciences" weight="0.55" year="2002">
        Buying bestsellers online: A case study in Search &amp; Searchability  A website's design directly affects how well search engines can crawl, match and rank its pages. For this reason, searchability is an important concern in site design. We study the interaction between search engines and Web sites by means of a case study of online bookstores and general-purpose search engines. The task modelled is that of finding web pages from which a book, described by its title, may be purchased. We first compared the relative effectiveness of search engines in finding pages matching the criterion, regardless of bookstore. Then we compared the relative searchability of the bookstore websites by observing how many times each bookstore contributed useful answers to the search results. Large differences in the performance of both search engines and bookstores were observed. Two of the search engines performed better than their peers, and one bookstore was far more searchable than all others. To further explore these differences we tabulate the total number of pages from each bookshop which are included in the search engine indexes. We conclude with recommendations both to bookstores on how they may improve their Web presence, and to search engines on how they may improve their performance for product searches.  Information Retrieval Web search, evaluation, transactional search
      </response>
      <response id="109" uni="RMIT University" weight="0.51" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="11" uni="The University of Queensland" weight="0.45" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="45" uni="The University of Melbourne" weight="0.44" year="2006">
        Some Observations on User Search Behavior  We explore some issues that arise in the way that users interact with a web search engine, as evidenced by the records of their interaction provided by query and clickthrough log data. Our observations are derived from approximately fifteen million user queries recorded by the search.msn.com search service in May 2006.  Log analysis, user behavior, search.
      </response>
      <response id="96" uni="RMIT University" weight="0.41" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="4" uni="University of Sydney" weight="0.39" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.38" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="68" uni="Cambridge, UK" weight="0.35" year="2007">
        Search and Navigation in Structured Document Retrieval: Comparison of User Behaviour in Search on Document Passages and XML Elements  This paper investigates search and browsing behaviour of users presented with two types of structured document retrieval approaches: passage retrieval and XML element retrieval. Our findings, based on the system logs gathered from 82 participants of the INEX 2006 interactive track experiment (iTrack), indicate that XML element retrieval leads to increased task performance. In addition, qualitative analysis of our video study, where we recorded the interactions of four participants, highlights potential issues with the experimental design employed at iTrack 2006.  XML element retrieval, passage retrieval, INEX interactive track, video user study.
      </response>
      <response id="94" uni="RMIT University" weight="0.35" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="59" uni="Queensland Unversity of Technology, The Pennsylvania State University" weight="0.32" year="2007">
        Multimedia Web Searching on a Meta-Search Engine  This paper provides preliminary results from a major study of multimedia Web searching by Dogpile meta-search engine users, including queries and session characteristics, and changes or differences in image, video and audio searching. The resutls are compares with multimedia Web searching studies from 1997 to 2002. Image and sexual queries are dominant in multimedia We searching. The paper provides important implications for the design of multimedia information retrieval systems.
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.32" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="60" uni="CSIRO ICT Centre Macquarie University" weight="0.28" year="2007">
        Can Requests-for-Action and Commitments-to-Act be Reliably Identified in Email Messages?  This paper reports on the results of an exploratory annotation task where three coders classified the presence and strength of Requests-for- Action (requests) and Commitments-to-Act (promises) in workplace email messages. The purpose of our annotation task was to explore levels of human agreement to establish whether this is a repeatable task that lends itself to automation. The results from our annotation task suggest that there is relatively high agreement about which sentences embody Requests-for-Action (= 0.78), but poorer agreement about Commitments-to-Act (= 0.54). Analysis of cases of coder disagreement highlighted several areas of systematic disagreement which we believe can be addressed through refining our annotation guidelines. Given this scope for improving agreement, we believe the results presented here are encouraging for our intention to perform largerscale annotation work leading to automation of the detection and classification of Requests-for-Action and Commitments-to-Act in email communication.  Email, document workflow, document management, Speech Acts, task management
      </response>
      <response id="133" uni="Funnelback" weight="0.28" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.24" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.23" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.19" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="150" uni="Dublin City University" weight="0.18" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="46" uni="Melbourne" weight="0.15" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="110" uni="University of Otago" weight="0.14" year="2010">
        Efficient Accumulator Initialisation  IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. Traditionally, the performance of search engines is dominated by slow disk I/O, CPU-intensive decompression, complex similarity ranking functions and sorting a large number of candidate documents. However, after we have applied a number of optimisation techniques, our search engine is bottlenecked by accumulator initialisation. In this paper, we propose an efficient accumulator initialisation algorithm, which represents the traditional static accumulator array as a logical two dimensional table and uses a number of flags to track the initialisation status of the accumulators. The efficiency of the algorithm is verified by a simulation program and a search engine. The overall performance can be as good as a 93% increase in throughput.  Accumulator Initialisation, Efficiency, Postings Pruning.
      </response>
      <response id="164" uni="Griffith University" weight="0.14" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="12" uni="The University of Queensland" weight="0.12" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.11" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="5" uni="The University of Sydney" weight="0.11" year="2002">
        Automatic Categorization of Announcements on the Australian Stock Exchange This paper compares the performance of several machine learning algorithms for the automatic categorization of corporate announcements in the Australian Stock Exchange (ASX) Signal G data stream. The article also describes some of the applications that the categorization of corporate announcements may enable. We have performed tests on two categorization tasks: market sensitivity, which indicates whether an announcement will have an impact on the market, and report type, which classifies each announcement into one of the report categories defined by the ASX. We have tried Neural Networks, a Naive Bayes classifier, and Support Vector Machines and achieved good results. Keywords Document Management, Document Workflow
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.11" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="122" uni="Queensland University of Technology, University of Otago" weight="0.11" year="2011">
        Mobile Applications of Focused Link Discovery  Interaction with a mobile device remains difficult due to inherent physical limitations. This difficulty is particularly evident for search, which requires typing. We extend the One-Search-Only search paradigm by adding a novel link-browsing scheme built on top of automatic link discovery. A prototype was built for iPhone and tested with 12 subjects. A post-use interview survey suggests that the extended paradigm improves the mobile information seeking experience.  Focused Link Discovery, Wikipedia, Mobile Information Seeking, User Studies Involving Documents.
      </response>
      <response id="125" uni="RMIT" weight="0.10" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="88" uni="University of Melbourne" weight="0.10" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="160" uni="DSTO, Lloyd-Jones Consulting" weight="0.10" year="1999">
        Automatic document metadata extraction and manipulation: a working system for the Intelligence Analyst  This paper discusses the design and implementation of an operational system to aid health intelligence analysts. The HINTS system provides automated support to undertake tasks such as specific health related research and report writing in the face on an ever-growing body of electronic information, available on the web, and on local file systems. Our approach is to provide automated support for document analysis and discovery from technologies that support ad-hoc searching, consistent filtering for specific pieces of information such as hospital facilities, diseases and locations, and that provide document summarisation and keywording. Document metadata is stored in XML in a data structure that allows a variety of searches and views of the document space to be performed. The user interfaces to the system by web browser and a map-based geospatial application. Document Analysis, Document Databases Information Retrieval, XML, Information Extraction
      </response>
      <response id="69" uni="CSIRO ICT Centre" weight="0.09" year="2007">
        Document Composition and Content Selection Evaluation  Our work is concerned with the design of adaptive hypertext systems that produce documents tailored to their intended reader. In our approach, a system composes document on-the-fly, assembling existing text fragments. One of our challenges in this approach is to support the technical writer who configures the system. The task of the technical writer is to specify the structure of the documents to be generated, together with their applicability conditions. To perform their task, authors need to know what information is available. In this paper, we examine the impact of different strategies for presenting the existing text fragments on the task of document composition. We focus in particular on the impact on the quality of the resulting documents. We found that people compose better documents when existing text fragments are presented in a structured way.  document composition, information reuse, document quality, evaluation, method.
      </response>
      <response id="57" uni="Queensland University of Technology" weight="0.09" year="2006">
        Dual Interactive Information Retrieval  A new task in Interactive Information Retrieval (IIR) is considered - optimization of information retrieval taking into account impact on quality of interaction with the user. Dual IIR is defined.  dual interactive information retrieval, multistage stochastic programming.
      </response>
    </responses>
  </theme>
  <theme id="2" title="Theme 2">
    <words>
      <word weight="5.08568509268">
        document
      </word>
      <word weight="0.479772298334">
        databas
      </word>
      <word weight="0.459663707235">
        structur
      </word>
      <word weight="0.418223498884">
        virtual
      </word>
      <word weight="0.401609182622">
        manag
      </word>
      <word weight="0.360295262282">
        exist
      </word>
    </words>
    <responses>
      <response id="142" uni="INRIA, CSIRO Mathematical and Information Sciences" weight="3.24" year="1997">
        Reuse of Information through virtual documents  This paper explores the issue of representing textual information in the form of virtual documents that include data and fragments of documents from remote sources - especially from databases and SGML document databases, virtual documents are dynamically generated, and therefore always present up-todate information when they are instantiated.. The benefit of this paradigm is that it allows information to be shared, reused, and adapted for various contexts. A Virtual document specification defines how to find and retrieve information objects from databases or from existing documents, and how to assemble it into another document. Virtual documents can be used to create HTML pages that contain information from one or several remote or local databases, to assembly parts of existing documents into a new one, or to define various views of the same information according to various needs. This paper focuses on the prototype implementation of virtual documents from the perspectives of document authoring and architecture. We propose an SGML syntax for Information Object that includes OQL-queries for retrieving fragments of existing documents, transformations on an intermediate tree representation, and output mapping to the virtual do cument structures. Document Databases, Document Management, HTML, Hypermedia, Information Discovery, SGML, virtual documents.
      </response>
      <response id="152" uni="Griffith University" weight="1.98" year="1999">
        DYNAMIC HYPER-LINKING BY QUERYING FOR A FCA-BASED QUERY SYSTEM  This paper presents a mechanism for hyper-linking documents by search-terms. Search-terms are selected by the user interactively building a formal concept lattice. In order to explain this interface we give some background to Formal Concept Analysis and an example is developed which illustrates the use of the concept lattice. Selected search-terms are used to create hyper-links, based on term repetition. As the search-terms differ between queries, we need a mechanism from which to dynamically create the target hyper-linked HTML documents. Therefore, documents are stored in a structure which is based on a word-list rather than plain text format. The documents are represented as links between the individual words within the word-list In so doing the word-list becomes a full-text-retrieval index into each word in each of those documents and therefore provides a good basis for the fast creation of an HTML document set from specific queries by keywords. To have the words in a word-list from which the documents are created also allows easy classification of words which should be hyper-linked within specific HTML documents. Furthermore, both documents and hyper-linking keywords are stored as well in this structure since any word in any document is indexed by the word-list. Document Databases, WWW and Internet.
      </response>
      <response id="162" uni="University of Sydney" weight="1.81" year="2000">
        Keyword Association Network: A Statistical Multi-term Indexing Approach for Document Categorization  A Keyword Association Network (KAN) is the network of keywords extracted from a collection of documents. In this network, the relationship between keywords is represented by a confidence value. It is argued in this paper thai the semantics and importance of a word can be more clearly and accurately measured by making use of other words that are co-occurring in a given document. The term frequency used for measuring the importance of terms in most document categorization methods ignores this important aspect. A KAN is constructed on the basis of co-occurring terms in documents. If two tenns appear more than a certain number of times in the same documents, they are considered as having close relationship. This paper proposes using KAN as a basis for finding informative keywords and using a confidence value in the process of document categorization. The process of constructing and application of KAN for document categorization is presented and the performance comparison with a typical statistical single-term document categorization algorithm - TFIDF classifier - will be shown. The experimental results show that KAN gives significant benefits. Keywords Document Categorization, Machine Learning. Statistical Multi-term indexing, Semantic- Meaning.
      </response>
      <response id="69" uni="CSIRO ICT Centre" weight="1.71" year="2007">
        Document Composition and Content Selection Evaluation  Our work is concerned with the design of adaptive hypertext systems that produce documents tailored to their intended reader. In our approach, a system composes document on-the-fly, assembling existing text fragments. One of our challenges in this approach is to support the technical writer who configures the system. The task of the technical writer is to specify the structure of the documents to be generated, together with their applicability conditions. To perform their task, authors need to know what information is available. In this paper, we examine the impact of different strategies for presenting the existing text fragments on the task of document composition. We focus in particular on the impact on the quality of the resulting documents. We found that people compose better documents when existing text fragments are presented in a structured way.  document composition, information reuse, document quality, evaluation, method.
      </response>
      <response id="32" uni="University of Southern Queensland" weight="1.65" year="2004">
        Towards a new approach to tightly coupled document collaboration  Currently document collaboration typically proceeds using tools such as CVS or vendor-specific Computer Supported Collaborative Work (CSCW) and Electronic Meeting (EM) messaging systems. Both regulate essentially asynchronous loosely coupled collaboration. The prime disadvantages of these technologies are that often documents are checked out or distributed in their entirety and that human interaction is needed in case of unresolvable conflicts. On the side of tightly coupled distributed collaborative work, emerging XML databases are employing database-type concurrency control techniques, but unfortunately tend to lock entire documents preventing simultaneous updates. XML-enabled relational databases have the same intrinsic problems, leading to the question if another way is possible. In this speculative short paper we describe a novel approach toward tightly coupled document collaboration, involving database-style synchronous client-server collaboration tailored to semi-structured documents. It is partly based on previous theoretic results which introduced path locks to control concurrency on semi-structured data. We also describe how clients may use a future communication protocol based on the path locks.  Document and XML Databases, Document Management, Document Collaboration.
      </response>
      <response id="17" uni="RMIT University" weight="1.62" year="2002">
        Studying the Evolution of XML Document Structures  The structure of XML documents often evolve over time, leading to reformulation and new version releases of the document structure. This occurs independently of the different wways in which XML document structure can be expressed. Major structural cahnges can cause version incompatibility issues and ensuing resource costs of updating existing documents. Software applications may be required to accomodate to both the odd and new document structures. We present the case here for a study on the development process and evolution of XML document structures.   XML, XML Scheme, DTD, Document Management
      </response>
      <response id="15" uni="University of Sydney" weight="1.55" year="2002">
        Liquid Miro: Semantic Softlinking to Support Cooperative Document Exploration  In  this  position  paper  we  present  a  non-intrusive mechanisim  for  evolving  the  overall  quality  of semantic  relationships  between  elements  of information  in  hypermedia  document  systems.  The evolutionary  aspect  of  this  work  is  an  application framework  that  includes a  combination of hard  links and temporal soft links between existing documents. A hard  link  completes  the  binding  between  two documents in the system upon creation, wheras a soft link  delays  the  binding  until  some  later  time.  The ablity  to monitor, weight,  integrate,  delay,  and  then order the soft links is what offers the power in Liquid Miro document systems. Here we focus on  the use of hypermedia document systems which support existing online  communities,  ranging  from  social  to professional groups..  Personalised  Documents,  Document Management
      </response>
      <response id="38" uni="RMIT University" weight="1.53" year="2005">
        Document Expansion versus Query Expansion for Ad-hoc Retrieval  In document information retrieval, the terminology given by a user may not match the terminology of a relevant document. Query expansion seeks to address this mismatch; it can significantly increase effectiveness, but is slow and resource-intensive. We investigate the use of document expansion as an alternative, in which documents are augmented with related terms extracted from the corpus during indexing, and the overheads at query time are small. We propose and explore a range of corpus-based document expansion techniques and compare them to corpus-based query expansion on TREC data. These experiments show that document expansion delivers at best limited benefits, while query expansion - including standard techniques and efficient approaches described in recent work - delivers consistent gains. We conclude that document expansion is unpromising, but it is likely that the efficiency of query expansion can be further improved.  Document expansion, automatic query expansion, pseudo relevance feedback, efficiency
      </response>
      <response id="138" uni="National Taiwan University" weight="1.50" year="1997">
        A Multi-party Document Model  A multi-party document (MPD) is one that contains multiple parts intended for multiple recipients in that each recipient does not necessarily need to know the existence, and therefore the corresponding content, of the other parts of the document intended for other recipients. MPDs pose new requirements, namely, transparency and security, to the underlying system. This paper reviews the traditional document models and their shortcomings with respect to the stated requirements and proposes a new model that satisfies those requirements. The major implementation issues including document construction and encryption for MPD supports are also discussed. Shifting these capabilities to the system level will relieve burdens of users and enhance the security measure of the system for protecting the integrity and privacy of document contents. The issues discussed in this paper are important for making a sound underlying support system for the development of information systems, in particular in the Intranets environment. Keywords Document management, document structure, security, encryption, Intranets.
      </response>
      <response id="89" uni="University of Melbourne, Monash University" weight="1.47" year="2008">
        Using Collaboratively Constructed Document Collections to Simulate Real-World Object Comparisons  While the layout of a museum exhibition is largely prescribed by the curator, visitors to museums view connections between exhibits in ways unique to themselves. With the assistance of a large-scale survey of museum visitors we identify that the view taken by museum visitors of a collection of exhibits can be represented by similarity over documents associated with each exhibit. We show that even when using a basic document similarity measure there is a correlation between document similarity and visitors' judgements of relatedness of exhibits aligned to these documents.  User Studies Involving Documents, Web Documents, Cognitive Aspects of Documents.
      </response>
      <response id="159" uni="University of Ballarat, La Trobe University" weight="1.43" year="1999">
        The use of argumentation to assist in the generation of legal documents  Many text documents in the legal domain are created in order to express the reasoning steps a decision maker followed in reaching conclusions. For example, refugee law determinations are documents that express the reasoning steps a member of the Refugee Review Tribunal in Australia followed in order to infer conclusions regarding the status of an applicant. Although, it is reasonable to expect that a mapping between the reasoning steps used by a decision maker and the structure of the document produced would clearly be apparent, a number of authors have discovered that such a mapping is by no means obvious. In order to develop legal knowledge based systems that generate documents from their own reasoning steps, discourse analysis is invoked to bridge the gap and perform the mapping. In this paper, we articulate a heuristic that we use to generate a plausible document structure without the use of discourse analysis. Without discourse analysis, the heuristic cannot contribute to our understanding of the process employed by decision makers to convert reasoning to text. Nevertheless, the heuristic can mimic the process. The heuristic has been trialed with a small sample of refugee law determinations by extracting the reasoning steps from each determination and applying the heuristic to reproduce each document's structure. Figure 11: The watermarked lena image Keywords refugee law.
      </response>
      <response id="155" uni="RMIT" weight="1.35" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="160" uni="DSTO, Lloyd-Jones Consulting" weight="1.32" year="1999">
        Automatic document metadata extraction and manipulation: a working system for the Intelligence Analyst  This paper discusses the design and implementation of an operational system to aid health intelligence analysts. The HINTS system provides automated support to undertake tasks such as specific health related research and report writing in the face on an ever-growing body of electronic information, available on the web, and on local file systems. Our approach is to provide automated support for document analysis and discovery from technologies that support ad-hoc searching, consistent filtering for specific pieces of information such as hospital facilities, diseases and locations, and that provide document summarisation and keywording. Document metadata is stored in XML in a data structure that allows a variety of searches and views of the document space to be performed. The user interfaces to the system by web browser and a map-based geospatial application. Document Analysis, Document Databases Information Retrieval, XML, Information Extraction
      </response>
      <response id="165" uni="Bond University" weight="1.24" year="2000">
        Implementing Shared Document Preparation with Lightweight Editing  Virtually all web pages are read-only, yet the first web browser allowed users to read and edit every page. Special ad-hoc mechanisms are needed to make all or part of a page editable by a user. This paper describes Pardalote lightweight editing, a document management feature for allowing many users to share the editing of a web page using only a web browser. A brief oven'iew of how Pardalote is implemented is followed by examples of shared document preparation using Pardalote. The benefits of such web document management are discussed. Future Pardalote extensions using XML precede the closing remarks. Keywords Shared document management, cooperative document preparation, lightweight editing, I-grains, fraglets, user interface design, computer supported cooperative work.
      </response>
      <response id="147" uni="CSIRO, Division of Mathematical and Information Science, RMIT" weight="1.11" year="1998">
        Evaluation of Indexing Methods for Clustering  In order to synthesize a better answer based on the retrieved documents, we are exploring the use of clustering methods. In this paper, we present an evaluation of some popular indexing methods used for term selection and term weighting. The aim of indexing here is to represent documents, to relate documents with similar topics, and distinguish documents with different topics from each other. Experiments have been conducted to examine how the clustering results are influenced by some index term selection methods, such as the term selection based on the document frequency, and some term weighting methods, such as the inverted document frequency weight, the signal-noise ratio, and the term discrimination value, will influence the result of clustering. Based on our experiments, we recommend the use of the discrimination value weighting method together with a suitable set of indexing terms for the purpose of clustering the retrieved documents. Keywords Index, Term Selection, Term Weighting, Clustering.
      </response>
      <response id="13" uni="University of Sydney" weight="1.10" year="2002">
        Visualisation of Document and Concept Spaces  Collections of documents with conceptual relationships exist in many domains. Teaching systems often contain numerous learning resource documents. University policies are often large collections of related documents. The visualisation of the structure of these collections can be useful as it allows the exploration of the collection. This paper describes a graphical interface for visualising document spaces. The interface makes it simple for the user to explore the documents and the relationships between them. metadata, ITS, ontology extraction, user modelling, visualisation
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.99" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="94" uni="RMIT University" weight="0.90" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="6" uni="CSIRO Mathematical and Information Sciences" weight="0.90" year="2002">
        XML Document Retrieval with PADRE  One paradigm of XML retrieval is database- style querying of semi-structured data. Another paradigm is based on information retrieval involving ranking of documents or document fragments. The INEX project attempts to integrate these paradigms and provide an environment for conducting retrieval experiments on semi-structured data. This paper discusses our participation in the INEX project and what we discovered about combining these paradigms.  Document Databases, Document Standards, Information Retrieval
      </response>
      <response id="92" uni="University of Otago" weight="0.89" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="43" uni="The University of Sydney" weight="0.87" year="2005">
        Cross Training and Under Sampling in Categorization of Company Announcements  To process the documents in a share market is crucial. It is because financial activities are socio-economic driven and text documents contain a lot of valuable information. In this paper, we focus on one of these documents, the Company Announcement. Each of these documents requires to be labelled as price sensitive or not before presenting to the general public. In our experiments, we study two specific issues in this text categorization, namely the effectiveness of a feature vector obtained from the corpus belonging to another market sector and the imbalanced nature of the dataset. Our results indicate that the classification can benefit from a different (but related) set of corpus because of a more diversified and generalised nature of the feature set. Regarding the skewness of the dataset, the under-sampling of the majority class in the training process does not have a strong effect on the performance in the test set, while keeping the computational cost minimised.  Document Management, Text Categorization
      </response>
      <response id="112" uni="RMIT University, The University of Melbourne" weight="0.85" year="2010">
        Relatively Relevant: Assessor Shift in Document Judgements  The evaluation of information retrieval systems relies on relevance judgements - human assessments of whether a document is relevant to a specified search request. In the past, it was demonstrated that test collection assessors disagree with each other to some extent on the relevance of documents and can be inconsistent in themselves. This paper describes a series of investigations on assessor consistency, which demonstrate that the inconsistency of an assessor varies over time. We show that when documents are presented to assessors in a relevance independent order, documents judged as relevant appear to cluster. Examining pairs of documents in a sequence ordered by timeof- judgement, we find that relevance assessors judge highly similar document pairs more consistently when the pairs are seen soon after each other; the consistency reduces when the pairs are judged further apart. We contend that our analysis shows that changes are not due to random error, but instead reflect a relevance shift, whereby the assessor's conception of what constitutes a relevant document changes over time. Studying types of relevance judgement we find that the shift in judgements is greatest between highly and partially relevant documents. We also examine the impact of this inconsistency on how retrieval runs are ranked relative to each other and find that there appears to be a noticeable effect on such rankings.  Information retrieval evaluation, Cranfield approach, Relevance judgements, TREC.
      </response>
      <response id="132" uni="The University of Sydney" weight="0.84" year="2012">
        Putting the Public into Public Health Information Dissemination: Social Media and Health-related Web Pages   Public health information dissemination represents an interesting combination of broadcasting, sharing, and retrieving relevant health information. Social media-based public health information dissemination offers some particularly interesting characteristics, as individual users or members of the public actually carry out the actions that constitute the dissemination. These actions also may inherently provide novel evaluative information from a document computing perspective, providing information in relation to both documents and indeed the social media users or health consumers themselves. This paper discusses the novel aspects of social media-based public health information dissemination, including a comparison of its characteristics with search engine-based Web document retrieval. A preliminary analysis of a sample of public health advice tweets taken from a larger sample of over 4700 tweets sent by Australian health-related organization in February 2012 is described. Various preliminary measures are analyzed from this data to initially suggest possible characteristics of public health information dissemination and document evaluation in micro-blog-based systems based on this sample.  Twitter, Web documents, Public Health
      </response>
      <response id="148" uni="Macquarie University, CSIRO Mathematical and Information Sciences" weight="0.83" year="1998">
        Using Natural Language Generation Techniques to Produce Virtual Documents  With the increasing importance of Web publishing, there has been considerable interest in the production of virtual documents on demand. The bulk of this work has used existing documents annotated with meta-data as a source. We suggest that more flexibility and functionality can be obtained if virtual documents are generated instead from raw data. This capability can be achieved by using natural language generation techniques. In this paper, we describe a project concerned with automatically generating natural language descriptions of museum artefacts directly from a museum's Collection Information System. Natural Language Generation, Databases
      </response>
      <response id="55" uni="University of Otago" weight="0.81" year="2006">
        Element Retrieval Using a Passage Retrieval Approach   Element and passage retrieval systems are able to extract and rank parts of documents and return them to the user rather than the whole document. Element retrieval is used to search XML documents and identify relevant XML elements, while passage retrieval is used to identify relevant passages. This paper reports a series of experiments on element retrieval, using a general passage retrieval algorithm. Firstly, an XML document is divided into overlapping or non-overlapping fixed size windows (passages), then the relevant passages which contain query terms are found. Given the position of a passage in the XML document, the smallest element which contains this passage is found. The experiments were conducted with the INEX 2005 ad hoc test collection and evaluation tool. Two passage extraction methods, three weight functions and various window sizes were tested. A comparison with element retrieval systems was also conducted. The experimental results show that a robust passage retrieval algorithm can yield an acceptable level of performance in XML element retrieval.  Element retrieval, passage retrieval, XML retrieval, INEX.
      </response>
      <response id="95" uni="Queensland University of Technology" weight="0.77" year="2009">
        Random Indexing K-tree   Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using Ktree. The algorithms and data structures are defined, explained and motivated. Specific modifications to Ktree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm. Keywords Random Indexing, K-tree, Dimensionality Reduction, B-tree, Search Tree, Clustering, Document Clustering, Vector Quantization, k-means
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="0.75" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="50" uni="Victoria University" weight="0.72" year="2006">
        Document-related Awareness Elements in Synchronous Collaborative Authoring  Simultaneous collaboration on documents by distributed authors has been supported by numerous synchronous collaborative authoring systems that are widely available. Originally, these tools were found to lack in providing rich enough interaction during authoring. As a result, group awareness in collaborative authoing arose as a very important issue in understanding how to provide comprehensive knowledge about other authors and activities they perform upon the document. To promote effectual authoring of documents simultaneously the best possible understanding of others' work on the document.  This paper reports results about document-related awareness elements from an empirical and experimental study of group awareness. Awareness elements reflect fundamental awareness information in supporting group awareness. Such results teach us what sort of document-related awareness should be provided for collaborative authoring.  Document management, collaborative document authoring, group awareness
      </response>
      <response id="118" uni="Australian National University, CSIRO, Funnelback" weight="0.70" year="2011">
        The usefulness of web spam  Spam comprises at least 60% of the public web, and search engine companies invest considerable effort in rejecting these apparently useless pages. But how bad are spam pages in search results? Can spam be dealt with as a side-effect of dealing with page utility, or is the relationship more complex? Thirty-four volunteer judges rated selected individual documents first on usefulness to a specified task and then on degree of 'spamminess'. Our results show that the relationship between spamminess and utility is far from clear cut; judges found that an important proportion of spam documents were useful. We conclude that evaluation should consider both utility and spamminess, as separate factors; and that search engines should not summarily discard spam pages but should take their utility into account as well.  User Studies Involving Documents; Web Documents
      </response>
      <response id="35" uni="CSIRO ICT Centre" weight="0.70" year="2005">
        Document modelling for customised information delivery  As the amount of information available to people multiplies at an increasing speed, it becomes ever more important to deliver information customised to users' specific needs. Natural Language Generation systems coupled with user modeling techniques have been built to address this issue, to produce information that is relevant to the users . A common approach adopted by such systems is an approach based on planning, starting from a discourse (or communicative) goal, and planning the text to be presented to the users. However, these systems are not easy to build and difficult to change by domain experts. One of the problems is that it is hard to specify the plans employed, because they often require knowledge about writing, domain expertise, knowledge of computational linguistics and, finally, knowledge about how to obtain data from the underlying information sources. In this paper, we present our first step to address this problem.   Document modeling, information retrieval, document generation, personalized documents.
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.70" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="16" uni="Macquarie University" weight="0.66" year="2002">
        How to Write a Document in Controlled Natural Language  This paper shows how a computer-processable document can be written in a controlled natural language (PENG) with the help of a sophisticated lookahead editor (ECOLE). The editor provides syntactic hints after each word form entered and indicates how the author can continue the text. This way the author does not need to learn or to remember the restrictions of the controlled language. PENG documents are automatically translated into first-order logic via discourse representation structures. These formal entities can be checked by a theorem prover for inconsistency or consistency can be revealed by a model builder.  Document Processing, Controlled Languages, Authoring Tools.
      </response>
      <response id="41" uni="The University of Queensland, The University of Wollongong" weight="0.64" year="2005">
        Applying Formal Concept Analysis to Semantic File Systems Leveraging Wordnet  Formal Concept Analysis can be used to obtain both a natural clustering of documents along with a partial ordering over those clusters. The application of Formal Concept Analysis requires input to be in the form of a binary relation between two sets. This paper investigates how a semantic filesystem can be used to generate such binary relations. The manner in which the binary relation is generated impacts how useful the result of Formal Concept Analysis will be for navigating one's filesystem.  Document Databases, Document Management
      </response>
      <response id="33" uni="University of Southern Queensland" weight="0.61" year="2004">
        GOOD Publishing System: Generic Online/Offline Delivery    GOOD is a tailor-made, fully integrated publishing system that creates output documents for multiple media types used in both online and offline teaching modes at the University of Southern Queensland. It is used in the Distance and e-Learning Centre of USQ to create course material for thousands of on-campus, online and external students. Among the end products generated from a single XML input document containing study material for a specific course are study books, introductory books, and web sites in a variety of formats. Future end products currently being investigated include voice rendering. The GOOD system is entirely based on open standards such as XML, XSLT, DOM, and XSL:FO and implemented with JAVA/J2EE technology. Among its features is a smart editing client to allow technically non-proficient staff to edit their own course material.  Document Management and Publishing, XML authoring.
      </response>
      <response id="131" uni="Queensland University of Technology" weight="0.60" year="2012">
        Pairwise Similarity of TopSig Document Signatures  This paper analyses the pairwise distances of signatures produced by the TopSig retrieval model on two document collections. The distribution of the distances are compared to purely random signatures. It explains why TopSig is only competitive with state of the art retrieval models at early precision. Only the local neighbourhood of the signatures is interpretable. We suggest this is a common property of vector space models.   [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval Models Signature Files, Topology, Vector Space IR, Random Indexing, Document Signatures, Search Engines, Document Clustering, Near Duplicate Detection, Relevance Feedback
      </response>
      <response id="149" uni="CSIRO Mathematical and Information Sciences" weight="0.57" year="1998">
        Automatic Document Creation from Software Specifications  Software documentation, and in particular, on-line help is a crucial aspect of a software system. Producing and maintaining it, however, is both labor intensive and tedious, making it a candidate for automation. This paper presents our work on automatically generating hypertext based on-line help, starting from software specifications. Our approach is motivated by practical considerations, such as the impossibility to construct by hand the semantic knowledge base typically required by a generation system. Natural Language Generation, Software documentation, hypertext
      </response>
      <response id="68" uni="Cambridge, UK" weight="0.56" year="2007">
        Search and Navigation in Structured Document Retrieval: Comparison of User Behaviour in Search on Document Passages and XML Elements  This paper investigates search and browsing behaviour of users presented with two types of structured document retrieval approaches: passage retrieval and XML element retrieval. Our findings, based on the system logs gathered from 82 participants of the INEX 2006 interactive track experiment (iTrack), indicate that XML element retrieval leads to increased task performance. In addition, qualitative analysis of our video study, where we recorded the interactions of four participants, highlights potential issues with the experimental design employed at iTrack 2006.  XML element retrieval, passage retrieval, INEX interactive track, video user study.
      </response>
      <response id="29" uni="The University of Sydney" weight="0.53" year="2004">
        Co-training on Textual Documents with a Single Natural Feature Set  Co-training is a semi-supervised technique that allows classifiers to learn with fewer labelled documents by taking advantage of the more abundant unclassified documents. However, conventional cotraining requires the dataset to be described by two disjoint and natural feature sets that are redundantly sufficient. In many practical situations datasets have a single set of features and it is not obvious how to split it into two. This paper investigates the performance of co-training with only one natural feature set in two applications: Web page classification and email filtering. Text categorization, Web page classification, spam filtering, co-training
      </response>
      <response id="56" uni="Macquarie University" weight="0.50" year="2006">
        Differentiating Document Type and Author Personality from Linguistic Features  There are many ways to profile a collection of documents. This paper presents highlight from a body of work that has looked at individual differences in the language of personal weblogs. Firstly, we present a unitary measure of linguistic contextuality based on POS frequency that can be used to profile and rank genres. When applied to weblogs, we will show they are similar to school essays, yet significantly less contextual than e-mail. We then look at individual variation of language, as due to the personality of the author, exploring the use of dictionary based analyses and data-driven n-grams. Under regression, we show that with just a few linguistic features, it is possible to explain significant proportions of variance within personality traits.  Personalised Documents; Multimedia Resource Discovery
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.50" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="39" uni="University of Wollongong" weight="0.49" year="2005">
        ePOC: Mobile Clinical Information Access and Diffusion in Ambulatory Care Service Settings  This paper represents a preliminary overview (work-in-progress) of a mobile e-Health research and development project and the intrinsic considerations which arise when designing such patient data management systems tailored to ambulatory care. Its purpose is to give an outline of the issues that allow technological enablement of electronic patient data management in the delivery of home-based medical care. While the replacement of more traditional paper-based patient data management using Personal Digital Assistants as a collection platform is technically straightforward, the organizational realignment of an electronic document management system requires careful study and deployment in order to maximize success. We outline the methodological considerations for document management diffusion within this e-Health setting and describe the issues, architecture and proposed rollout of an electronic Point-Of-Care (ePOC) system.  e-Health, document management and workflow, information access and diffusion
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.49" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="146" uni="RMIT" weight="0.44" year="1997">
        Conflation-based Comparison of Stemming Algorithms  In text database systems, query terms are stemmed to allow them to be conflated with variant forms of the same word. On the one hand, stemming allows the query mechanism to find documents that would otherwise not contain matches to the query terms; on the other hand, automatic stemming is prone to error, and can lead to retrieval of inappropriate documents. In this paper we investigate several stemming algorithms, measuring their ability to correctly conflate terms from a large text collection. We show that stemming is indeed worthwhile, but that each of the stemming algorithms we consider has distinct advantages and disadvantages; choice of stemming algorithm affects the behaviour of the retrieval mechanism. information retrieval, document databases, digital libraries, word disambiguation.
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.43" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="4" uni="University of Sydney" weight="0.43" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.41" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="88" uni="University of Melbourne" weight="0.40" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="157" uni="University of Sydney" weight="0.40" year="1999">
        Transformation-Based Learning for Automatic Translation from HTML to XML   Format tags implicitly represent content information in the same ambiguous, context dependent manner that words represent semantics in natural language. Translation from format to content markup shares many characteristics with tagging and parsing tasks in computational linguistics. The transformation-based learning (TEL) paradigm has recently been applied to numerous computational linguistics tasks with considerable success. We present a transformation-based translator which automatically learns to translate semistructured HTML documents formatted with a particular style to XML using a small set of training examples. Keywords HTML, XML, markup, document processing, machine translation, machine learning
      </response>
      <response id="65" uni="University of Melbourne" weight="0.39" year="2007">
        Automatic Thread Classification for Linux User Forum Information Access  We experiment with text classification of threads from Linux web user forums, in the context of improving information access to the problems and solutions described in the threads. We specifically focus on classifying threads according to: (1) them describing a specific problem vs. containing a more general discussion; (2) the completeness of the initial post in the thread; and (3) whether problem(s) in the initial post are resolved in the thread or not. We approach these tasks in both classification and regression frameworks using a range of machine learners and evaluation metrics.  Web Documents, Document Management
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.39" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="79" uni="The University of Sydney" weight="0.39" year="2008">
        MetaView: Dynamic metadata based views of user files  Hierarchical file systems are the most common way of organising large collections of documents. However, there are several desirable features they do lack. These include: good support for placing files in multiple locations; dynamic views on the users' data; and explicit ordering of files. This paper introduces MetaView, a new approach to enhancing file systems so that they can present users with a fluid and dynamic view of their files based on metadata. MetaView allows users to describe how they wish to view their files by specifying an organisational structure based on a metadata path. Experiments indicate that this approach is viable for collections of up to several thousand files in size, enabling flexible organisation of substantial parts of a user's file system.  Document Management, Metadata, Nonhierarchical file system.
      </response>
      <response id="102" uni="University of Melbourne" weight="0.37" year="2009">
        You Are What You Post: User-level Features in Threaded Discourse  We develop methods for describing users based on their posts to an online discussion forum. These methods build on existing techniques to describe other aspects of online discussions communities, but the application of these techniques to describing users is novel. We demonstrate the utility of our proposed methods by showing that they are superior to existing methods over a post-level classification task over a published real-world dataset.  Document Management, Information Retrieval, Web Documents
      </response>
      <response id="8" uni="Macquarie University CSIRO Mathematical and Information Sciences" weight="0.36" year="2002">
        Information Extraction in the KELP Framework  In this paper we describe some early steps in a new approach to information extraction The aim of the kelp project is to combine a variety of natural language processing techniques so that we can extract useful elements of information from a collection of documents and then represent this information tailored to the needs of a specific user Our focus here is on how we can build richly structured data objects by extracting information from web pages as an example we describe the extraction of information from web pages that describe laptop computers A principle goal of this work is the separation of different components of the information extraction task so as to increase portability Keywords Information extraction, natural language generation, document personalisation
      </response>
      <response id="47" uni="ICT Centre CSIRO" weight="0.35" year="2006">
        Improving rankings in small-scale web search using click-implied descriptions    When a searcher submits a query Q and clicks on document R in the corresponding result set, we may plausibly interpret the click as a vote that Q is a description of R. We call the Q and R pairing a 'click description'. Click descriptions thus derived from search engine logs can be accumulated into surrogate documents and used to boost retrieval effectiveness in a similar fashion to anchor text. We investigate the usefulness of click description surrogate documents in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92% improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link  Information Storage and Retrieval, Content Analysis and Indexing [Indexing methods]
      </response>
      <response id="98" uni="University of Sydney" weight="0.34" year="2009">
        Feature Selection and Weighting Methods in Sentiment Analysis  Sentiment analysis is the task of identifying whether the opinion expressed in a document is positive or negative about a given topic. Unfortunately, many of the potential applications of sentiment analysis are currently infeasible due to the huge number of features found in standard corpora. In this paper we systematically evaluate a range of feature selectors and feature weights with both Naive Bayes and Support Vector Machine classifiers. This includes the introduction of two new feature selection methods and three new feature weighting methods. Our results show that it is possible to maintain a state-of-the art classification accuracy of 87.15% while using less than 36% of the features.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="9" uni="University of Sydney" weight="0.34" year="2002">
        Workflow Based Just-in-time Training  This paper focuses on the problem of information overload for newcomers in an organisation. We propose to address it by constructing a smart personal training assistant based upon workflow tools to drive temporal management of a just-in time workplace training system which will deliver a personalised and structured presentation of organisational documents.  Document Workflow, Document Management, Information Retrieval.
      </response>
      <response id="105" uni="University of Otago" weight="0.34" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="97" uni="University of Otago" weight="0.34" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="119" uni="CSIRO, KMITL, University of Glasgow" weight="0.34" year="2011">
        Indexing without Spam  The presence of spam in a document ranking is a major issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages that are likely to contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user's query. In this paper we propose removing spam pages at indexing time, therefore obtaining a pruned index that is virtually 'spam-free'. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performance. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection's index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches.  Information Retrieval; Index Pruning; Spam; Web search; Efficiency.
      </response>
      <response id="66" uni="RMIT University" weight="0.33" year="2007">
        Source Code Authorship Attribution using n-grams  Plagiarism and copyright infringement are major problems in academic and corporate environments. Existing solutions for detecting infringements in structured text such as source code are restricted to textual similarity comparisons of two pieces of work. In this paper, we examine authorship attribution as a means for tackling plagiarism detection. Given several samples of work from several authors, we attempt to correctly identify the author of work presented as a query. On a collection of 1 640 documents written by 100 authors, we show that we can attribute authorship in up to 67% of cases. This work can be a valuable additional indicator for the more difficult plagiarism investigations.  Authorship Attribution, Plagiarism Detection, Co-derivative Documents
      </response>
      <response id="156" uni="Cardiff University" weight="0.33" year="1999">
        Effective Reuse of Textual Documents Containing Tabular Information  This paper presents an overview of a toolkit that can facilitate efficient reuse of tables appearing within textual documents [1]. In order to effectively reuse information contained in these documents, it is important that we process the accompanying text as well as any tables that appear. From this text, it may be possible to extract metadata such as descriptions of table content and related formulae, mappings and constraints. This metadata can then be exploited to enhance the value of extracted tables during their subsequent reuse. In this paper we present a discussion of the techniques used to process tables and associated text, both of which rely heavily on the use of regular expressions. Our techniques for locating tables utilise similar visual clues used by other table processing techniques discussed in the literature [5,6,7,8], although our approach to exploiting them is quite different. Our tools have been designed to provide a high level of support for the numerous types of table layout encountered in plain text tables, an area that has previously been somewhat overlooked.
      </response>
      <response id="140" uni="The University of Queensland, CRC for Distributed Systems Technology" weight="0.30" year="1997">
        Applying a Generic Conceptual Workflow Modeling Technique to Document Workflows  The workflow technology is emerging as an appropriate platform for the automated coordination of business activities. The documents represent the primary medium of business communication. Almost all kinds of business activities have some associated documents. The workflow management systems could be applied to coordinate the flow of business documents. However, before this automated document workflows could be implemented, we need to apply some conceptual modeling methodology to capture, analyse, and describe the role and flow of documents in a business process. In this paper, we present a generic conceptual workflow modeling technique that should be applicable to all kinds of workflows. The document workflows represent a specialized application of workflows. We identify basic characteristics of document workflows and discuss some issues that should be targeted during the modeling process. We also apply the proposed conceptual modeling technique to model an example document workflows application for handling postgraduate admission process of a university. conceptual modeling of workflows, document workflows.
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.30" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="5" uni="The University of Sydney" weight="0.30" year="2002">
        Automatic Categorization of Announcements on the Australian Stock Exchange This paper compares the performance of several machine learning algorithms for the automatic categorization of corporate announcements in the Australian Stock Exchange (ASX) Signal G data stream. The article also describes some of the applications that the categorization of corporate announcements may enable. We have performed tests on two categorization tasks: market sensitivity, which indicates whether an announcement will have an impact on the market, and report type, which classifies each announcement into one of the report categories defined by the ASX. We have tried Neural Networks, a Naive Bayes classifier, and Support Vector Machines and achieved good results. Keywords Document Management, Document Workflow
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.30" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="133" uni="Funnelback" weight="0.28" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="60" uni="CSIRO ICT Centre Macquarie University" weight="0.27" year="2007">
        Can Requests-for-Action and Commitments-to-Act be Reliably Identified in Email Messages?  This paper reports on the results of an exploratory annotation task where three coders classified the presence and strength of Requests-for- Action (requests) and Commitments-to-Act (promises) in workplace email messages. The purpose of our annotation task was to explore levels of human agreement to establish whether this is a repeatable task that lends itself to automation. The results from our annotation task suggest that there is relatively high agreement about which sentences embody Requests-for-Action (= 0.78), but poorer agreement about Commitments-to-Act (= 0.54). Analysis of cases of coder disagreement highlighted several areas of systematic disagreement which we believe can be addressed through refining our annotation guidelines. Given this scope for improving agreement, we believe the results presented here are encouraging for our intention to perform largerscale annotation work leading to automation of the detection and classification of Requests-for-Action and Commitments-to-Act in email communication.  Email, document workflow, document management, Speech Acts, task management
      </response>
      <response id="2" uni="University of Sydney" weight="0.25" year="2002">
        Generating and Comparing Models within an Ontology An ontology is useful for providing a conceptually concise basis for developing and communicating knowledge. This paper discusses an application of an automatically constructed ontology of computer science and its use for comparison of models in the computer science domain. We present the architecture, algorithms and current results for MECUREO, a system which builds an extensive model of a computer science entity from limited information. The entity might be a document such as an email message, a course description document or a biography. It is intended to give a measure of the similarity of any two such models. We describe MECUREO's mechanism for constructing and representing models and its present performance in comparing models. Keywords information retrieval, ontologies, acquisition, sharing and reuse of conceptual structures
      </response>
      <response id="96" uni="RMIT University" weight="0.25" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="150" uni="Dublin City University" weight="0.23" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="10" uni="The University of Queensland" weight="0.21" year="2002">
        Visual Displays for Browsing RDF Documents  Hyperbolic browsers are motivated by the 'Circle Limit IV' woodcut of M.C. Escher. The hyperbolic tree view was introduced in graph drawing by Lamping and Rao [2] who observed that large structures could be compactly displayed by projecting a tree onto a hyperbolic plane. The effect of the projection is that components appear diminishing in size and radius exponentially the further they move from the centre of the diagram. The arguments for the hyperbolic display are twofold: an order of magnitude more nodes of a tree can be rendered in the same display space and the focus is maintained on the central vertex of the display and its immediate neighbourhood. The hyperbolic view is particularly useful for hierarchical diagrams with large numbers of leaves and branches and where neighbourhood relationships are meaningful. Examples of the hyperbolic view are INXIGHT's Star Tree1 and HYPERPROF2. Given that the pure hyperbolic geometric projection is patented, projection onto a sphere, and diminishing radial layout views are the drawing approaches we have experimented with.
      </response>
      <response id="135" uni="CSIRO" weight="0.21" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.20" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="7" uni="The University of Sydney" weight="0.19" year="2002">
        A Multi-Learner Approach to Email Classification    The volume of email which most users receive has grown over teh past few years. Most users try to cope with this problem by sorting email into folders. In this paper we look at machine learning approaches to performing this classification automatically. In particular we describe a test and select approach to choosing both single learners and ensembles of learners to classify an individual user's email. The results show that is possible to select the most effective single learner for an individual user, but that selecting an ensemble without overfitting is more difficult. We also show that Widrow-Hoff is a highly effective algorithm for email classification and discuss the reasons for this.  Document Management, Email Management, Text Categorization, Machine Learning
      </response>
      <response id="1" uni="University of Queensland" weight="0.18" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="122" uni="Queensland University of Technology, University of Otago" weight="0.18" year="2011">
        Mobile Applications of Focused Link Discovery  Interaction with a mobile device remains difficult due to inherent physical limitations. This difficulty is particularly evident for search, which requires typing. We extend the One-Search-Only search paradigm by adding a novel link-browsing scheme built on top of automatic link discovery. A prototype was built for iPhone and tested with 12 subjects. A post-use interview survey suggests that the extended paradigm improves the mobile information seeking experience.  Focused Link Discovery, Wikipedia, Mobile Information Seeking, User Studies Involving Documents.
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.18" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="106" uni="RMIT University" weight="0.18" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="14" uni="CRC for Enterprise Distributed Systems Technology (DSTC)" weight="0.17" year="2002">
        The Nexus information hub for exploring social-informational context  The Nexus system is an &quot;information hub&quot; that helps users collaboratively manage and organise &quot;contextualised social notifications&quot;. The purpose of the prototype is to act as a foundation for research into human information-sharing activity. The paper describes contextualised social notifications in more depth, links this prototype to the earlier Scuttlebutt prototype (presented at ADCS-6) and describes the architecture of the system and research questions.  Information Retrieval, Personalised Documents, Social Information Sharing
      </response>
      <response id="25" uni="University of Otago" weight="0.17" year="2004">
        Optimal Structure Weighted Retrieval  Improving ranking functions for structured information retrieval has received much attention since the inception of XML. Weighting document structures is one method providing significant improvement - but how good can these improvements be? Optimal structure weighted retrieval occurs when each query is processed using the optimal set of weights for that query. Optimal retrieval for a set of queries occurs when a set of weights optimized for that set of queries is used. Measuring mean average precision for each of these will give a performance upper bound for document structure weighted retrieval. In this investigation a near optimal set of weights is learned for TREC WSJ collection topics 101-200 using a genetic algorithm. Weights are learned for vector space inner product, naive probability and BM25 ranking functions and a performance upper bound is calculated. The upper bound using a different set of weights for each query, gives mean average precision improvements of about 15% for BM25 and naive probability; about 30% for inner product. This suggests structure weighting might be useful for relevance feedback. Optimal weights for the set of queries shows improvements of about 5% for naive probability and inner product, but of only about 1% for BM25; suggesting this technique is not as effective for ad hoc retrieval.  Information Retrieval.
      </response>
      <response id="51" uni="University of Sydney" weight="0.16" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
      <response id="62" uni="University of Otago" weight="0.15" year="2007">
        IR Evaluation Using Multiple Assessors per Topic  Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006. Traditionally the assessment of a single topic is performed by a single human. Herein we examine the consequences of using multiple assessors per topic. A set of 15 topics were used. The mean topic pool contained 98 documents. Between 3 and 5 separate assessors (per topic) assessed all documents in a pool. One assessor was designated baseline. All were then used to generate 10,000 synthetic multi-assessor assessment sets. The baseline relative rank order of all runs submitted to the INEX 2006 relevant-in-context task was compared to those of the synthetics. The mean Spearman's rank correlation coefficient was 0.986 and all coefficients were above 0.95 - the correlation is very strong. Non matching rank-orders are seen when the mean average precision difference between runs is less than 0.05. In the top 10 runs no significantly different runs were ranked in a different order in more than 5% of the synthetics. Using multiple assessors per topic is very unlikely to affect the outcome of an evaluation forum.  Information Retrieval
      </response>
      <response id="21" uni="The University of Sydney" weight="0.15" year="2002">
        A Framework for Text Categorization In this paper we discuss the architecture of an object-oriented application framework (OOAF) for text categorization. We describe the system requirements and the software engineering strategies that form the basis of the design and implementation of the framework. We show how designing a highly reusable OOAF architecture facilitates the development of new applications. We also highlight the key text categorization features of the framework, as well as practical considerations for application developers. Document Management, Text Categorization, Application Frameworks
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.14" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="127" uni="Queensland University of Technology, University of Otago" weight="0.14" year="2012">
        An English-Translated Parallel Corpus for the CJK Wikipedia Collections  In this paper, we describe a machine-translated parallel English corpus for the NTCIR Chinese, Japanese and Korean (CJK) Wikipedia collections. This document collection is named CJK2E Wikipedia XML corpus. The corpus could be used by the information retrieval research community and knowledge sharing in Wikipedia in many ways; for example, this corpus could be used for experimentations in cross-lingual information retrieval, cross-lingual link discovery, or omni-lingual information retrieval research. Furthermore, the translated CJK articles could be used to further expand the current coverage of the English Wikipedia.  Information Storage and Retrieval Digital Libraries - collection.
      </response>
      <response id="103" uni="Queensland University of Technology" weight="0.12" year="2009">
        Investigating the use of Association Rules in Improving Recommender Systems  Recommender systems are widely used online to help users find other products, items etc that they may be interested in based on what is known about that user in their profile. Often however user profiles may be short on information and thus when there is not sufficient knowledge on a user it is difficult for a recommender system to make quality recommendations. This problem is often referred to as the cold-start problem. Here we investigate whether association rules can be used as a source of information to expand a user profile and thus avoid this problem, leading to improved recommendations to users. Our pilot study shows that indeed it is possible to use association rules to improve the performance of a recommender system. This we believe can lead to further work in utilising appropriate association rules to lessen the impact of the cold-start problem.  Information Retrieval, Personalised Documents, Recommender Systems, Association Rules.
      </response>
      <response id="110" uni="University of Otago" weight="0.11" year="2010">
        Efficient Accumulator Initialisation  IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. Traditionally, the performance of search engines is dominated by slow disk I/O, CPU-intensive decompression, complex similarity ranking functions and sorting a large number of candidate documents. However, after we have applied a number of optimisation techniques, our search engine is bottlenecked by accumulator initialisation. In this paper, we propose an efficient accumulator initialisation algorithm, which represents the traditional static accumulator array as a logical two dimensional table and uses a number of flags to track the initialisation status of the accumulators. The efficiency of the algorithm is verified by a simulation program and a search engine. The overall performance can be as good as a 93% increase in throughput.  Accumulator Initialisation, Efficiency, Postings Pruning.
      </response>
      <response id="123" uni="University of Waikato" weight="0.11" year="2011">
        A Workflow for Document Level Interoperability  This article describes a software environment called the Exchange Center that helps digital librarians manage the workflow of sourcing documents and metadata from various repositories. The software is built on Greenstone but does not require its use as the final digital library server. After describing the software architecture we provide two scenarios of its use: a private library of recipes, which ultimately involves collaboration with other cooks; and a digital library that aggregates the collections of various host institutions that use different repository software.  Digital Library Interoperability, Software Architecture, Workflow
      </response>
      <response id="27" uni="Victoria University" weight="0.11" year="2004">
        Novel Group Awareness Mechanisms for Real-time Collaborative Document Authoring  Group awareness has become important in improving the usability of real-time, distributed, collaborative writing systems. However, the current set of implemented awareness mechanisms is insufficient in providing extensive and comprehensive awareness in collaborative document authoring. Certainly, current mechanisms, such as telepointers and multi-user scrollbars, have contributed in providing awareness support in collaborative authoring. Yet, given the shortcomings of these mechanisms and the difficulty in providing rich interaction found in face-to-face collaboration, much more support needs to be provided for group awareness during authoring. This research extends the pool of all known awareness mechanisms (including those that have been discovered before but have yet to be implemented). This research discovered several awareness mechanisms not found and reported elsewhere, through conducting usability experiments with a realtime cooperative editor. This paper covers three of the mechanisms-Structure-based Multi-page View, Point Jumping Mechanism and User Info List- discovered from the experiments. The paper also provides quantitative results supporting implementation of such mechanisms.  Group awareness, awareness mechanisms, real-time collaborative document authoring.
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="0.11" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
      <response id="115" uni="Macquarie University" weight="0.11" year="2010">
        A Rule-based Approach for Automatic Identification of Publication Types of Medical Papers  The medical domain has an abundance of textual resources of varying quality. The quality of medical articles depends largely on their publication types. However, identifying high-quality medical articles from search results is till date a manual and time-consuming process. We present a simple, rule-based, post-retrieval approach to automatically identify medical articles belonging to three high-quality publication types. Our approach simply uses title and abstract information of the articles to perform this. Our experiments show that such a rule-based approach has close to 100% precision and recall for the three publication types.   Medical Document Classification, Postretrieval Classification, Rule-based Classification, Evidence-based Medicine
      </response>
      <response id="12" uni="The University of Queensland" weight="0.11" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="163" uni="Cardiff University" weight="0.10" year="2000">
        The TREATS Approach to Reuse of Tables in Plain Text Documents  In this paper we present the table processing approach employed by the TREATS (Table Recognition, Extraction, Analysis and Tranformation System) software toolkit. In order to support the large variety of table layouts that appear in plain text documents, our system aims to identify the layout of cells that exist within a table and to tailor processing accordingly. This results in more effective processing than preexisting approaches that apply one general technique to all types of table. The classification process is the key to processing and exploits a cellular automaton (CA) based approach to the identification of cell structure within a table. The input to the CA is a simple representation of the content of the source table. This is evolved via the application of intelligent transformation rules, resulting in a representation of the cell structure that exists within the table. Based on the combination of cell types that appear in this representation, the layout of the table can be detennined and appropriate processing can be performed. During this processing, the content of the source table is transformed into a relational form suitable for reuse in other applications. discovery.
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.10" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.09" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.09" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.09" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="109" uni="RMIT University" weight="0.09" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
    </responses>
  </theme>
  <theme id="3" title="Theme 3">
    <words>
      <word weight="4.19103789544">
        retriev
      </word>
      <word weight="1.81939300541">
        passag
      </word>
      <word weight="1.75049194448">
        element
      </word>
      <word weight="1.39278548486">
        xml
      </word>
      <word weight="0.52664955043">
        relev
      </word>
      <word weight="0.462544529159">
        inex
      </word>
    </words>
    <responses>
      <response id="55" uni="University of Otago" weight="3.82" year="2006">
        Element Retrieval Using a Passage Retrieval Approach   Element and passage retrieval systems are able to extract and rank parts of documents and return them to the user rather than the whole document. Element retrieval is used to search XML documents and identify relevant XML elements, while passage retrieval is used to identify relevant passages. This paper reports a series of experiments on element retrieval, using a general passage retrieval algorithm. Firstly, an XML document is divided into overlapping or non-overlapping fixed size windows (passages), then the relevant passages which contain query terms are found. Given the position of a passage in the XML document, the smallest element which contains this passage is found. The experiments were conducted with the INEX 2005 ad hoc test collection and evaluation tool. Two passage extraction methods, three weight functions and various window sizes were tested. A comparison with element retrieval systems was also conducted. The experimental results show that a robust passage retrieval algorithm can yield an acceptable level of performance in XML element retrieval.  Element retrieval, passage retrieval, XML retrieval, INEX.
      </response>
      <response id="68" uni="Cambridge, UK" weight="1.72" year="2007">
        Search and Navigation in Structured Document Retrieval: Comparison of User Behaviour in Search on Document Passages and XML Elements  This paper investigates search and browsing behaviour of users presented with two types of structured document retrieval approaches: passage retrieval and XML element retrieval. Our findings, based on the system logs gathered from 82 participants of the INEX 2006 interactive track experiment (iTrack), indicate that XML element retrieval leads to increased task performance. In addition, qualitative analysis of our video study, where we recorded the interactions of four participants, highlights potential issues with the experimental design employed at iTrack 2006.  XML element retrieval, passage retrieval, INEX interactive track, video user study.
      </response>
      <response id="120" uni="CSIRO, Australian National University, University of Applied Science Technikum Wien, Funnelback" weight="1.45" year="2011">
        Automatic identification of the most important elements in an XML collection  An important problem in XML retrieval is determining the most useful element types to retrieve - e.g. book, chapter, section, paragraph or caption. An automated system for doing this could be based on features of element types related to size, depth, frequency of occurrence, etc. We consider a large number of such features and assess their usefulness in predicting the types of elements judged relevant in INEX evaluations for the IEEE and Wikipedia 2006 corpora. For each feature we automatically assign Useful / Not-Useful labels to element types using Fuzzy c-Means Clustering. We then rank the features by the accuracy with which they predict the manual judgments. We find strong overlap between the top-ten most predictive features for the two collections and that seven features achieve high average accuracy (F-measure &gt; 65%) acrosss them. We hypothesize that an XML retrieval system working on an unlabelled corpus could use these features to decide which retrieval units are most appropriate to return to the user.  XML Retrieval, Fuzzy C-Means Clustering, F-Measure.
      </response>
      <response id="106" uni="RMIT University" weight="1.12" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="84" uni="RMIT University" weight="0.87" year="2008">
        The Effect of Using Pitch and Duration for Symbolic Music Retrieval  Quite reasonable retrieval effectiveness is achieved for retrieving polyphonic (multiple notes at once) music that is symbolically encoded via melody queries, using relatively simple pattern matching techniques based on pitch sequences. Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files.  Music information retrieval, Information retrieval, Multimedia resource discovery, Pattern matching
      </response>
      <response id="54" uni="CSIRO ICT Centre" weight="0.82" year="2006">
        InexBib - Retrieving XML elements based on external evidence  Creating a scientific bibliography on a given topic is currently a task which requires a great deal of manual effort. We attempt to reduce this effort by developing a tool for automatically generating a bibliography from a collection of articles represented in XML. We evaluate the use of elements around the references as anchortexts to improve search results. We find that users of the tool prefer lists generated using anchortext over those generated from the bibliography entry only and that the preference is statistically significant. We tentatively find no significant preference for results generated using paragraph as opposed to sentence level anchortext, but note that this finding may result from lack of sophistication in resolving text including multiple references.  Information Retrieval, XML, Element Retrieval, Bibliography
      </response>
      <response id="125" uni="RMIT" weight="0.81" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="141" uni="Monash University" weight="0.81" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="6" uni="CSIRO Mathematical and Information Sciences" weight="0.76" year="2002">
        XML Document Retrieval with PADRE  One paradigm of XML retrieval is database- style querying of semi-structured data. Another paradigm is based on information retrieval involving ranking of documents or document fragments. The INEX project attempts to integrate these paradigms and provide an environment for conducting retrieval experiments on semi-structured data. This paper discusses our participation in the INEX project and what we discovered about combining these paradigms.  Document Databases, Document Standards, Information Retrieval
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.75" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.72" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="0.71" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.67" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="127" uni="Queensland University of Technology, University of Otago" weight="0.58" year="2012">
        An English-Translated Parallel Corpus for the CJK Wikipedia Collections  In this paper, we describe a machine-translated parallel English corpus for the NTCIR Chinese, Japanese and Korean (CJK) Wikipedia collections. This document collection is named CJK2E Wikipedia XML corpus. The corpus could be used by the information retrieval research community and knowledge sharing in Wikipedia in many ways; for example, this corpus could be used for experimentations in cross-lingual information retrieval, cross-lingual link discovery, or omni-lingual information retrieval research. Furthermore, the translated CJK articles could be used to further expand the current coverage of the English Wikipedia.  Information Storage and Retrieval Digital Libraries - collection.
      </response>
      <response id="30" uni="RMIT University" weight="0.58" year="2004">
        A Testbed for Indonesian Text Retrieval  Indonesia is the fourth most populous country and a close neighbour of Australia. However, despite media and intelligence interest in Indonesia, little work has been done on evaluating Information Retrieval techniques for Indonesian, and no standard testbed exists for such a purpose. An effective testbed should include a collection of documents, realistic queries, and relevance judgements. The TREC and TDT testbeds have provided such an environment for the evaluation of English, Mandarin, and Arabic text retrieval techniques. The NTCIR testbed provides a similar environment for Chinese, Korean, Japanese, and English. This paper describes an Indonesian TREC-like testbed we have constructed and made available for the evaluation of ad hoc retrieval techniques. To illustrate how the test collection is used, we briefly report the effect of stemming for Indonesian text retrieval, showing - similarly to English - that it has little effect on accuracy.  Indonesian, queries, collection, relevance judgements, stemming
      </response>
      <response id="131" uni="Queensland University of Technology" weight="0.56" year="2012">
        Pairwise Similarity of TopSig Document Signatures  This paper analyses the pairwise distances of signatures produced by the TopSig retrieval model on two document collections. The distribution of the distances are compared to purely random signatures. It explains why TopSig is only competitive with state of the art retrieval models at early precision. Only the local neighbourhood of the signatures is interpretable. We suggest this is a common property of vector space models.   [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval Models Signature Files, Topology, Vector Space IR, Random Indexing, Document Signatures, Search Engines, Document Clustering, Near Duplicate Detection, Relevance Feedback
      </response>
      <response id="57" uni="Queensland University of Technology" weight="0.46" year="2006">
        Dual Interactive Information Retrieval  A new task in Interactive Information Retrieval (IIR) is considered - optimization of information retrieval taking into account impact on quality of interaction with the user. Dual IIR is defined.  dual interactive information retrieval, multistage stochastic programming.
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.46" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="74" uni="RMIT University" weight="0.46" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.41" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="150" uni="Dublin City University" weight="0.39" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="155" uni="RMIT" weight="0.35" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.34" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="119" uni="CSIRO, KMITL, University of Glasgow" weight="0.33" year="2011">
        Indexing without Spam  The presence of spam in a document ranking is a major issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages that are likely to contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user's query. In this paper we propose removing spam pages at indexing time, therefore obtaining a pruned index that is virtually 'spam-free'. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performance. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection's index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches.  Information Retrieval; Index Pruning; Spam; Web search; Efficiency.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.33" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="146" uni="RMIT" weight="0.32" year="1997">
        Conflation-based Comparison of Stemming Algorithms  In text database systems, query terms are stemmed to allow them to be conflated with variant forms of the same word. On the one hand, stemming allows the query mechanism to find documents that would otherwise not contain matches to the query terms; on the other hand, automatic stemming is prone to error, and can lead to retrieval of inappropriate documents. In this paper we investigate several stemming algorithms, measuring their ability to correctly conflate terms from a large text collection. We show that stemming is indeed worthwhile, but that each of the stemming algorithms we consider has distinct advantages and disadvantages; choice of stemming algorithm affects the behaviour of the retrieval mechanism. information retrieval, document databases, digital libraries, word disambiguation.
      </response>
      <response id="161" uni="France" weight="0.27" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="42" uni="University of Sydney" weight="0.27" year="2005">
        Biomedical Named Entity Recognition System  We propose a machine learning approach, using a Maximum Entropy (ME) model to construct a Named Entity Recognition (NER) classifier to retrieve biomedical names from texts. In experiments, we utilize a blend of various linguistic features incorporated into the ME model to assign class labels and location within an entity sequence, and a postprocessing strategy for corrections to sequences of tags to produce a state of the art solution. The experimental results on the GENIA corpus achieved an F-score of 68.2% for semantic classification of 23 categories and achieved F-score of 78.1% on identification.  Named Entity Recognition, ME model, Information Retrieval.
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.26" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.26" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="77" uni="RMIT University" weight="0.24" year="2007">
        Querying Image Ontology  Content-based image retrieval has been used in various application domains, but the semantic gap problem remains a challenge to be overcome. One possible way to overcome this problem is to represent the knowledge extracted from the low-level image features through semantic concepts. In this paper we describe how we use an image ontology to this end. We show that we are able to retrieve desired images by using basic ontology queries.  Ontology, CBIR, semantic gap
      </response>
      <response id="12" uni="The University of Queensland" weight="0.24" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="22" uni="RMIT University" weight="0.23" year="2002">
        Improved use of Contextual Information in Cross-language Information Retrieval  In this paper, we explore Dictionary based context sensitive translation, a framework for query translation to reduce the translation ambiguity and improve the translation quality in English to Chinese cross-language information retrieval (CLIR). Our paper explores the effect of the context window size on translation effectiveness. We assume that the correct translations of the query key terms tend to co-occur together at a high frequency and incorrect translations do not. Our experimental results showed that when using a window size of 10, context sensitive translation results in a dramatic improvement in retrieval performance, it brings about a 30% improvement compared to the results of previous Dictionary based approaches that used only Immediately adjacent words for context.
      </response>
      <response id="75" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.23" year="2007">
        Score Standardization for Robust Comparison of Retrieval Systems  Information retrieval systems are evaluated by applying them to standard test collections of documents, topics, and relevance judgements. An evaluation metric is then used to score a system's output for each topic; these scores are averaged to obtain an overall measure of effectiveness. However, different topics have differing degrees of difficulty and differing variability in scores, leading to inconsistent contributions to aggregate system scores and problems in comparing scores between different test collections. In this paper, we propose that per-topic scores be standardized on the observed score distributions of the runs submitted to the original experiment from which the test collection was created. We demonstrate that standardization equalizes topic contributions to system effectiveness scores and improves inter-collection comparability.  Retrieval system evaluation, average precision, standardization.
      </response>
      <response id="67" uni="The Australian National University, CSIRO ICT Centre" weight="0.21" year="2007">
        A Framework for Measuring the Impact of Web Spam  Web spam potentially causes three deleterious effects: unnecessary work for crawlers and search engines; diversion of traffic away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here we focus on the deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web spam. We index the 80 million page UK2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments.  Web Information Retrieval, Web Spam, Adversarial Information Retrieval
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.19" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="4" uni="University of Sydney" weight="0.17" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="105" uni="University of Otago" weight="0.17" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="33" uni="University of Southern Queensland" weight="0.13" year="2004">
        GOOD Publishing System: Generic Online/Offline Delivery    GOOD is a tailor-made, fully integrated publishing system that creates output documents for multiple media types used in both online and offline teaching modes at the University of Southern Queensland. It is used in the Distance and e-Learning Centre of USQ to create course material for thousands of on-campus, online and external students. Among the end products generated from a single XML input document containing study material for a specific course are study books, introductory books, and web sites in a variety of formats. Future end products currently being investigated include voice rendering. The GOOD system is entirely based on open standards such as XML, XSLT, DOM, and XSL:FO and implemented with JAVA/J2EE technology. Among its features is a smart editing client to allow technically non-proficient staff to edit their own course material.  Document Management and Publishing, XML authoring.
      </response>
      <response id="1" uni="University of Queensland" weight="0.13" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="17" uni="RMIT University" weight="0.12" year="2002">
        Studying the Evolution of XML Document Structures  The structure of XML documents often evolve over time, leading to reformulation and new version releases of the document structure. This occurs independently of the different wways in which XML document structure can be expressed. Major structural cahnges can cause version incompatibility issues and ensuing resource costs of updating existing documents. Software applications may be required to accomodate to both the odd and new document structures. We present the case here for a study on the development process and evolution of XML document structures.   XML, XML Scheme, DTD, Document Management
      </response>
      <response id="143" uni="RMIT" weight="0.12" year="1997">
        Supporting the Answering Process  This paper is concerned with the way information access systerns support the question answering process. This process includes three stages: question formulation, information gathering, and analysis and synthesis. Standard information access technologies are mainly concerned with the second of these stages, providing little or no support for the last stage. However, the raw information gathered at this point can seldom be used directly as an answer. This paper discusses issues relating to the support of the analysis and synthesis stage, and suggests how information access systems might be extended to better support it. This paper also describes a WWW-based experimental interface that permits the evaluation of alternate ways of supporting this analysis and synthesis stage of the answering process. Information Retrieval, Question Answering, Passage Retrieval, Answer Presentation, Hypertext, WWW.
      </response>
      <response id="64" uni="CSIRO ICT Centre, Australian National University" weight="0.12" year="2007">
        Does brandname influence perceived search result quality? Yahoo!, Google, and WebKumara  Improving the quality of search engine results is the goal of costly efforts by major Web search engine companies. Using in situ side-by-side result set comparisons and random assignment of brandnames to result sets, we investigated whether perceptions of quality were influenced by brand association. In the first experiment (15 searchers) we found no significant preference for or against results labelled 'Google' relative to those labelled 'Yahoo!'. In the second experiment (20 searchers) result sets were again generated by Google and Yahoo! but were randomly labelled 'Yahoo!' or 'WebKumara' (a fictitious name). Again, we found no significant preference for one brandname label over the other. Contrary to previous findings, we found a statistically significant preference for Googlegenerated results over those of Yahoo! when data from three separate experiments (total 70 subjects) was combined.  Information retrieval
      </response>
      <response id="102" uni="University of Melbourne" weight="0.11" year="2009">
        You Are What You Post: User-level Features in Threaded Discourse  We develop methods for describing users based on their posts to an online discussion forum. These methods build on existing techniques to describe other aspects of online discussions communities, but the application of these techniques to describing users is novel. We demonstrate the utility of our proposed methods by showing that they are superior to existing methods over a post-level classification task over a published real-world dataset.  Document Management, Information Retrieval, Web Documents
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.11" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="107" uni="The University of Melbourne, University of Malaya" weight="0.11" year="2010">
        Estimating System Effectiveness ScoresWith Incomplete Evidence  It is common for only partial relevance judgments to be used when comparing retrieval system effectiveness, in order to control experimental cost. Using TREC data, we consider the uncertainty introduced into per-topic effectiveness scores by pooled judgments, and measure the effect that incomplete evidence has on both the systems scores that are generated, and also on the quality of paired system comparisons. We measure system behavior from three different points of view: the trend in effectiveness scores; the separability of system pairs; and the number of reversals in significance outcomes as the depth of judgments increases. Our results show that when shallow pooled judgments are used system separability remains relatively high, but that there is also a high rate of significance reversal. We then show that explicitly adjusting effectiveness scores to allow for the known amount of uncertainty gives a reduced number of reversals, and hence more consistent experimental outcomes.  Retrieval evaluation, effectiveness metric, pooling
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.10" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="37" uni="The University of Melbourne" weight="0.10" year="2005">
        In Search of Reliable Retrieval Experiments  There are several ways in which an 'improved' technique for solving some computational problem can be defended: by mathematical argument; by simulation; and by experimental validation. Each of these has risks. In this paper we describe some of the issues that arose during an experimental validation of architectures for distributed text query evaluation, and the approaches that were taken to resolve them. In particular, collections and clusters must be scaled in a way that maximizes comparability between different data sizes; query sets must be appropriate to the target collection; and hardware issues such as file placement on disk must also be considered. Our intention is to report on our experience in a practical sense, and thereby assist others to avoid the same problems.
      </response>
      <response id="97" uni="University of Otago" weight="0.10" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="48" uni="CSIRO ICT Centre" weight="0.10" year="2006">
        My Instant Expert  This paper gives an overview of a mobile device question answering application that has recently been developed in the CSIRO ICT Centre. The application makes use of data in an open-domain encyclopaedia to answer general knowledge questions. The paper presents the techniques used, results and error analysis on the project.  Information retrieval, Question answering
      </response>
      <response id="72" uni="RMIT University" weight="0.09" year="2007">
        Predicting Query Performance for User-based Search Tasks  Query performance prediction aims to determine in advance whether a user's search request will return a useful answer set. The success of such prediction attempts are currently evaluated by calculating the correlation between the predicted performance and standard information retrieval metrics of system performance such as average precision. However, recent work suggests that there is little relationship between average precision and the performance of users when carrying out search tasks. Direct measures of user performance offer another way of evaluating the effectiveness of search systems; this is of particular importance in the framework of query prediction, since one of the goals of prediction is to warn users when search results are likely to be poor. We therefore investigate the relationship between current prediction techniques and user-based performance measures. Our preliminary results show that the performance of the predictors differs strongly when using system-based compared to user-based performance measures: predictors that are significantly correlated with one measurement are often not correlated with the other. In general, the predictors are more correlated with average precision rather than with user performance.  Query performance prediction, information retrieval, user study
      </response>
    </responses>
  </theme>
  <theme id="4" title="Theme 4">
    <words>
      <word weight="3.76576255713">
        user
      </word>
      <word weight="1.79475526305">
        tag
      </word>
      <word weight="1.32640121979">
        recommend
      </word>
      <word weight="1.14889320914">
        profil
      </word>
      <word weight="1.14881780778">
        approach
      </word>
      <word weight="1.07555343202">
        base
      </word>
    </words>
    <responses>
      <response id="90" uni="Queensland University of Technology" weight="3.64" year="2009">
        Collaborative Filtering Recommender Systems based on Popular Tags  The social tags in web 2.0 are becoming another important information source to profile users' interests and preferences for making personalized recommendations. However, the uncontrolled vocabulary causes a lot of problems to profile users accurately, such as ambiguity, synonyms, misspelling, low information sharing etc. To solve these problems, this paper proposes to use popular tags to represent the actual topics of tags, the content of items, and also the topic interests of users. A novel user profiling approach is proposed in this paper that first identifies popular tags, then represents users' original tags using the popular tags, finally generates users' topic interests based on the popular tags. A collaborative filtering based recommender system has been developed that builds the user profile using the proposed approach. The user profile generated using the proposed approach can represent user interests more accurately and the information sharing among users in the profile is also increased. Consequently the neighborhood of a user, which plays a crucial role in collaborative filtering based recommenders, can be much more accurately determined. The experimental results based on real world data obtained from Amazon.com show that the proposed approach outperforms other approaches.  Information Retrieval, recommender systems, social tags, web 2.0
      </response>
      <response id="103" uni="Queensland University of Technology" weight="1.41" year="2009">
        Investigating the use of Association Rules in Improving Recommender Systems  Recommender systems are widely used online to help users find other products, items etc that they may be interested in based on what is known about that user in their profile. Often however user profiles may be short on information and thus when there is not sufficient knowledge on a user it is difficult for a recommender system to make quality recommendations. This problem is often referred to as the cold-start problem. Here we investigate whether association rules can be used as a source of information to expand a user profile and thus avoid this problem, leading to improved recommendations to users. Our pilot study shows that indeed it is possible to use association rules to improve the performance of a recommender system. This we believe can lead to further work in utilising appropriate association rules to lessen the impact of the cold-start problem.  Information Retrieval, Personalised Documents, Recommender Systems, Association Rules.
      </response>
      <response id="97" uni="University of Otago" weight="1.39" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="1.18" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="79" uni="The University of Sydney" weight="0.92" year="2008">
        MetaView: Dynamic metadata based views of user files  Hierarchical file systems are the most common way of organising large collections of documents. However, there are several desirable features they do lack. These include: good support for placing files in multiple locations; dynamic views on the users' data; and explicit ordering of files. This paper introduces MetaView, a new approach to enhancing file systems so that they can present users with a fluid and dynamic view of their files based on metadata. MetaView allows users to describe how they wish to view their files by specifying an organisational structure based on a metadata path. Experiments indicate that this approach is viable for collections of up to several thousand files in size, enabling flexible organisation of substantial parts of a user's file system.  Document Management, Metadata, Nonhierarchical file system.
      </response>
      <response id="92" uni="University of Otago" weight="0.89" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.83" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="63" uni="Queensland University of Technology" weight="0.79" year="2007">
        Integration of Information Filtering and Data Mining Process for Web Information Retrieval  This paper examines a new approach to Web information retrieval, and proposes a new two stage scheme. The aim of the first stage is to quickly filter irrelevant information based on the user profiles. The proposed user profiles learning algorithm are very efficient and effective within a relevance feedback framework. The aim of the second stage is to apply data mining techniques to rationalize the data relevance on the reduced data set. Our experiments on RCV1 (Reuters Corpus Volume 1) data collection which is used by TREC in 2002 for filtering track show that more effective and efficient access Web information has been achieved by combining the strength of information filtering and data mining method.  Information filtering, User profiles, Data mining, Pattern taxonomic model
      </response>
      <response id="51" uni="University of Sydney" weight="0.74" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
      <response id="4" uni="University of Sydney" weight="0.67" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="7" uni="The University of Sydney" weight="0.65" year="2002">
        A Multi-Learner Approach to Email Classification    The volume of email which most users receive has grown over teh past few years. Most users try to cope with this problem by sorting email into folders. In this paper we look at machine learning approaches to performing this classification automatically. In particular we describe a test and select approach to choosing both single learners and ensembles of learners to classify an individual user's email. The results show that is possible to select the most effective single learner for an individual user, but that selecting an ensemble without overfitting is more difficult. We also show that Widrow-Hoff is a highly effective algorithm for email classification and discuss the reasons for this.  Document Management, Email Management, Text Categorization, Machine Learning
      </response>
      <response id="35" uni="CSIRO ICT Centre" weight="0.62" year="2005">
        Document modelling for customised information delivery  As the amount of information available to people multiplies at an increasing speed, it becomes ever more important to deliver information customised to users' specific needs. Natural Language Generation systems coupled with user modeling techniques have been built to address this issue, to produce information that is relevant to the users . A common approach adopted by such systems is an approach based on planning, starting from a discourse (or communicative) goal, and planning the text to be presented to the users. However, these systems are not easy to build and difficult to change by domain experts. One of the problems is that it is hard to specify the plans employed, because they often require knowledge about writing, domain expertise, knowledge of computational linguistics and, finally, knowledge about how to obtain data from the underlying information sources. In this paper, we present our first step to address this problem.   Document modeling, information retrieval, document generation, personalized documents.
      </response>
      <response id="161" uni="France" weight="0.61" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="165" uni="Bond University" weight="0.54" year="2000">
        Implementing Shared Document Preparation with Lightweight Editing  Virtually all web pages are read-only, yet the first web browser allowed users to read and edit every page. Special ad-hoc mechanisms are needed to make all or part of a page editable by a user. This paper describes Pardalote lightweight editing, a document management feature for allowing many users to share the editing of a web page using only a web browser. A brief oven'iew of how Pardalote is implemented is followed by examples of shared document preparation using Pardalote. The benefits of such web document management are discussed. Future Pardalote extensions using XML precede the closing remarks. Keywords Shared document management, cooperative document preparation, lightweight editing, I-grains, fraglets, user interface design, computer supported cooperative work.
      </response>
      <response id="109" uni="RMIT University" weight="0.42" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.40" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="76" uni="Queensland University of Technology" weight="0.36" year="2007">
        Efficient Neighbourhood Estimation for Recommenders with Large Datasets  In this paper, we present a novel neighbourhood estimation method which is not only both memory and computation efficient but can also achieves better estimation accuracy than other cluster based neighbourhood formation techniques. In this paper we have successfully incorporated the proposed technique with a taxonomy based product recommender, and with the proposed neighbourhood formation technique both time efficiency and recommendation quality of the recommender are improved.  Recommender Systems, Neighbourhood Estimation, Product Taxonomy
      </response>
      <response id="65" uni="University of Melbourne" weight="0.35" year="2007">
        Automatic Thread Classification for Linux User Forum Information Access  We experiment with text classification of threads from Linux web user forums, in the context of improving information access to the problems and solutions described in the threads. We specifically focus on classifying threads according to: (1) them describing a specific problem vs. containing a more general discussion; (2) the completeness of the initial post in the thread; and (3) whether problem(s) in the initial post are resolved in the thread or not. We approach these tasks in both classification and regression frameworks using a range of machine learners and evaluation metrics.  Web Documents, Document Management
      </response>
      <response id="96" uni="RMIT University" weight="0.34" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="102" uni="University of Melbourne" weight="0.30" year="2009">
        You Are What You Post: User-level Features in Threaded Discourse  We develop methods for describing users based on their posts to an online discussion forum. These methods build on existing techniques to describe other aspects of online discussions communities, but the application of these techniques to describing users is novel. We demonstrate the utility of our proposed methods by showing that they are superior to existing methods over a post-level classification task over a published real-world dataset.  Document Management, Information Retrieval, Web Documents
      </response>
      <response id="132" uni="The University of Sydney" weight="0.28" year="2012">
        Putting the Public into Public Health Information Dissemination: Social Media and Health-related Web Pages   Public health information dissemination represents an interesting combination of broadcasting, sharing, and retrieving relevant health information. Social media-based public health information dissemination offers some particularly interesting characteristics, as individual users or members of the public actually carry out the actions that constitute the dissemination. These actions also may inherently provide novel evaluative information from a document computing perspective, providing information in relation to both documents and indeed the social media users or health consumers themselves. This paper discusses the novel aspects of social media-based public health information dissemination, including a comparison of its characteristics with search engine-based Web document retrieval. A preliminary analysis of a sample of public health advice tweets taken from a larger sample of over 4700 tweets sent by Australian health-related organization in February 2012 is described. Various preliminary measures are analyzed from this data to initially suggest possible characteristics of public health information dissemination and document evaluation in micro-blog-based systems based on this sample.  Twitter, Web documents, Public Health
      </response>
      <response id="135" uni="CSIRO" weight="0.25" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="114" uni="RMIT University" weight="0.25" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="44" uni="University of Otago" weight="0.24" year="2005">
        Recommending Geocaches  Players downloading GPS coordinates from the internet, hiking to the given spot, and hunting for a hidden box - this is the new sport of geocaching. Today there are nearly 200,000 such boxes in over 200 countries. With so many to find, a recommender is needed, one that takes into account not only the boxes, but also the geospatial and temporal nature of the sport. A database of geocaches in the South Island of New Zealand is made by trawling a prominent geocaching web site. This is then used to estimate the home-coordinates (geospatial playing centre) of players. Predictions are verified against a set of correct coordinates solicited from players. Several geocache recommenders are discussed and compared. The precision, computed using mean of mean reciprocal rank (MMRR), of each is measured. The best method tried is a collaborative filter using intersection over mean to find similar players and a voting scheme to recommend geocaches. This method is proposed as a replacement for the currently used distance from home-coordinate; doing so will increase the precision of existing systems such as geocaching. com.  Information Retrieval.
      </response>
      <response id="45" uni="The University of Melbourne" weight="0.24" year="2006">
        Some Observations on User Search Behavior  We explore some issues that arise in the way that users interact with a web search engine, as evidenced by the records of their interaction provided by query and clickthrough log data. Our observations are derived from approximately fifteen million user queries recorded by the search.msn.com search service in May 2006.  Log analysis, user behavior, search.
      </response>
      <response id="154" uni="University of Sydney" weight="0.24" year="1999">
        Building rich metadata from critical reviews for a scrutable filtering system  We describe the Review Coder system for creating rich metadata for a scrutable filtering system. A scrutable system maintains explanations of the data and processes that drove the system operation. In the current paper we use Review Coder as part of a filtering systems for movies: the scrutability of the system means that a user can determine why the system recommended a particular movie or not. The filtering process is based upon movie reviews and metadata built in association with them. These provide high quality information about the movie objects. From these, the filtering system is intended to build stereotypic models of reviewer's preferences for movies. These can drive the filtering process and the user can scrutinise both these models and the actual reviews which were used to construct them. Keywords: Multimedia Resource Discovery, Multimedia Filtering, Scrutable Filtering, Extraction of Metadata
      </response>
      <response id="32" uni="University of Southern Queensland" weight="0.22" year="2004">
        Towards a new approach to tightly coupled document collaboration  Currently document collaboration typically proceeds using tools such as CVS or vendor-specific Computer Supported Collaborative Work (CSCW) and Electronic Meeting (EM) messaging systems. Both regulate essentially asynchronous loosely coupled collaboration. The prime disadvantages of these technologies are that often documents are checked out or distributed in their entirety and that human interaction is needed in case of unresolvable conflicts. On the side of tightly coupled distributed collaborative work, emerging XML databases are employing database-type concurrency control techniques, but unfortunately tend to lock entire documents preventing simultaneous updates. XML-enabled relational databases have the same intrinsic problems, leading to the question if another way is possible. In this speculative short paper we describe a novel approach toward tightly coupled document collaboration, involving database-style synchronous client-server collaboration tailored to semi-structured documents. It is partly based on previous theoretic results which introduced path locks to control concurrency on semi-structured data. We also describe how clients may use a future communication protocol based on the path locks.  Document and XML Databases, Document Management, Document Collaboration.
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.22" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="119" uni="CSIRO, KMITL, University of Glasgow" weight="0.21" year="2011">
        Indexing without Spam  The presence of spam in a document ranking is a major issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages that are likely to contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user's query. In this paper we propose removing spam pages at indexing time, therefore obtaining a pruned index that is virtually 'spam-free'. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performance. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection's index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches.  Information Retrieval; Index Pruning; Spam; Web search; Efficiency.
      </response>
      <response id="12" uni="The University of Queensland" weight="0.19" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="151" uni="University of New South Wales, Murdoch University, University of Incheon, Korea" weight="0.17" year="1999">
        Reader's Preferences in the Formats of Web-based Academic Articles  No standard format exists for the many academic articles available on the Web and little is known about user reading patterns. This paper explores these issues using data from two online surveys: one email-based, the other Web-based. Our results suggests that people take an overview from the screen, and then, if they are interested in an article, print it out in order to read it properly. The simple two-frame format was regarded as the best by 47% of the respondents, whereas the cascade page format was regarded as the worst by 65% of the respondents. Interestingly, 26% considered the paper-like format, widely used in Web-based articles, to be the worst. Different results were obtained when interactive examples were embedded in the survey. Keywords: Web-based articles; Reading patterns and formats; Web-based survey; Digital library
      </response>
      <response id="13" uni="University of Sydney" weight="0.17" year="2002">
        Visualisation of Document and Concept Spaces  Collections of documents with conceptual relationships exist in many domains. Teaching systems often contain numerous learning resource documents. University policies are often large collections of related documents. The visualisation of the structure of these collections can be useful as it allows the exploration of the collection. This paper describes a graphical interface for visualising document spaces. The interface makes it simple for the user to explore the documents and the relationships between them. metadata, ITS, ontology extraction, user modelling, visualisation
      </response>
      <response id="106" uni="RMIT University" weight="0.16" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="117" uni="Queensland University of Technology, CSIRO" weight="0.16" year="2010">
        Rule-based Approach for Identifying Assertions in Clinical Free-Text Data  A rule-based approach for classifying previously identified medical concepts in the clinical free text into an assertion category is presented.There are six different categories of assertions for the task: Present, Absent, Possible, Conditional, Hypothetical and Not associated with the patient. The assertion classification algorithms were largely based on extending the popular NegEx and Context algorithms. In addition, a health based clinical terminology called SNOMED CT and other publicly available dictionaries were used to classify assertions, which did not fit the NegEx/Context model. The data for this task includes discharge summaries from Partners HealthCare and from Beth Israel Deaconess Medical Centre, as well as discharge summaries and progress notes from University of Pittsburgh Medical Centre. The set consists of 349 discharge reports, each with pairs of ground truth concept and assertion files for system development, and 477 reports for evaluation. The system's performance on the evaluation data set was 0.83, 0.83 and 0.83 for recall, precision and F1-measure, respectively. Although the rule-based system shows promise, further improvements can be made by incorporating machine learning approaches.  rule-based, medical concept, assertion, NegEx, Context, SNOMED CT.
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.15" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="14" uni="CRC for Enterprise Distributed Systems Technology (DSTC)" weight="0.15" year="2002">
        The Nexus information hub for exploring social-informational context  The Nexus system is an &quot;information hub&quot; that helps users collaboratively manage and organise &quot;contextualised social notifications&quot;. The purpose of the prototype is to act as a foundation for research into human information-sharing activity. The paper describes contextualised social notifications in more depth, links this prototype to the earlier Scuttlebutt prototype (presented at ADCS-6) and describes the architecture of the system and research questions.  Information Retrieval, Personalised Documents, Social Information Sharing
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.15" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="56" uni="Macquarie University" weight="0.14" year="2006">
        Differentiating Document Type and Author Personality from Linguistic Features  There are many ways to profile a collection of documents. This paper presents highlight from a body of work that has looked at individual differences in the language of personal weblogs. Firstly, we present a unitary measure of linguistic contextuality based on POS frequency that can be used to profile and rank genres. When applied to weblogs, we will show they are similar to school essays, yet significantly less contextual than e-mail. We then look at individual variation of language, as due to the personality of the author, exploring the use of dictionary based analyses and data-driven n-grams. Under regression, we show that with just a few linguistic features, it is possible to explain significant proportions of variance within personality traits.  Personalised Documents; Multimedia Resource Discovery
      </response>
      <response id="164" uni="Griffith University" weight="0.14" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="54" uni="CSIRO ICT Centre" weight="0.13" year="2006">
        InexBib - Retrieving XML elements based on external evidence  Creating a scientific bibliography on a given topic is currently a task which requires a great deal of manual effort. We attempt to reduce this effort by developing a tool for automatically generating a bibliography from a collection of articles represented in XML. We evaluate the use of elements around the references as anchortexts to improve search results. We find that users of the tool prefer lists generated using anchortext over those generated from the bibliography entry only and that the preference is statistically significant. We tentatively find no significant preference for results generated using paragraph as opposed to sentence level anchortext, but note that this finding may result from lack of sophistication in resolving text including multiple references.  Information Retrieval, XML, Element Retrieval, Bibliography
      </response>
      <response id="11" uni="The University of Queensland" weight="0.12" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.12" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="47" uni="ICT Centre CSIRO" weight="0.11" year="2006">
        Improving rankings in small-scale web search using click-implied descriptions    When a searcher submits a query Q and clicks on document R in the corresponding result set, we may plausibly interpret the click as a vote that Q is a description of R. We call the Q and R pairing a 'click description'. Click descriptions thus derived from search engine logs can be accumulated into surrogate documents and used to boost retrieval effectiveness in a similar fashion to anchor text. We investigate the usefulness of click description surrogate documents in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92% improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link  Information Storage and Retrieval, Content Analysis and Indexing [Indexing methods]
      </response>
      <response id="67" uni="The Australian National University, CSIRO ICT Centre" weight="0.10" year="2007">
        A Framework for Measuring the Impact of Web Spam  Web spam potentially causes three deleterious effects: unnecessary work for crawlers and search engines; diversion of traffic away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here we focus on the deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web spam. We index the 80 million page UK2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments.  Web Information Retrieval, Web Spam, Adversarial Information Retrieval
      </response>
      <response id="68" uni="Cambridge, UK" weight="0.10" year="2007">
        Search and Navigation in Structured Document Retrieval: Comparison of User Behaviour in Search on Document Passages and XML Elements  This paper investigates search and browsing behaviour of users presented with two types of structured document retrieval approaches: passage retrieval and XML element retrieval. Our findings, based on the system logs gathered from 82 participants of the INEX 2006 interactive track experiment (iTrack), indicate that XML element retrieval leads to increased task performance. In addition, qualitative analysis of our video study, where we recorded the interactions of four participants, highlights potential issues with the experimental design employed at iTrack 2006.  XML element retrieval, passage retrieval, INEX interactive track, video user study.
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.10" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="141" uni="Monash University" weight="0.09" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="18" uni="University of South Australia" weight="0.09" year="2002">
        Towards a System to Improve Administrative Processes for Front-Line Academic Staff  Current  economic  pressures  are  causing  severe problems for many enterprises in maintaining service standards  with  shrinking  headcounts.  Groupware, Workflow  and  Agent  technologies  have  been  widely advocated  as  a  solution,  but  there  are  few  reported success  stories.  The  project  described  in  this  paper addresses  the  case  of  running  large  undergraduate courses. A preliminary vision of a possible integrated administrative  support  system  is  presented,  and  the future  activities  necessary  to  advance  such  a  vision are outlined.  Administrative  Applications,  User Interface Design, Software Agents
      </response>
      <response id="89" uni="University of Melbourne, Monash University" weight="0.09" year="2008">
        Using Collaboratively Constructed Document Collections to Simulate Real-World Object Comparisons  While the layout of a museum exhibition is largely prescribed by the curator, visitors to museums view connections between exhibits in ways unique to themselves. With the assistance of a large-scale survey of museum visitors we identify that the view taken by museum visitors of a collection of exhibits can be represented by similarity over documents associated with each exhibit. We show that even when using a basic document similarity measure there is a correlation between document similarity and visitors' judgements of relatedness of exhibits aligned to these documents.  User Studies Involving Documents, Web Documents, Cognitive Aspects of Documents.
      </response>
      <response id="9" uni="University of Sydney" weight="0.09" year="2002">
        Workflow Based Just-in-time Training  This paper focuses on the problem of information overload for newcomers in an organisation. We propose to address it by constructing a smart personal training assistant based upon workflow tools to drive temporal management of a just-in time workplace training system which will deliver a personalised and structured presentation of organisational documents.  Document Workflow, Document Management, Information Retrieval.
      </response>
      <response id="149" uni="CSIRO Mathematical and Information Sciences" weight="0.08" year="1998">
        Automatic Document Creation from Software Specifications  Software documentation, and in particular, on-line help is a crucial aspect of a software system. Producing and maintaining it, however, is both labor intensive and tedious, making it a candidate for automation. This paper presents our work on automatically generating hypertext based on-line help, starting from software specifications. Our approach is motivated by practical considerations, such as the impossibility to construct by hand the semantic knowledge base typically required by a generation system. Natural Language Generation, Software documentation, hypertext
      </response>
    </responses>
  </theme>
  <theme id="5" title="Theme 5">
    <words>
      <word weight="5.34855778207">
        inform
      </word>
      <word weight="0.950832791722">
        retriev
      </word>
      <word weight="0.804136474051">
        data
      </word>
      <word weight="0.779575109997">
        access
      </word>
      <word weight="0.687913112701">
        public
      </word>
      <word weight="0.682453524785">
        health
      </word>
    </words>
    <responses>
      <response id="52" uni="CSIRO ICT Centre" weight="1.91" year="2006">
        Information Access Efficiency: a Measure and a Case Study  One of the advantages we claim for information synthesis and aggregation is that it results in more efficient information access for end users, especially when the relevant information comes from multiple heterogeneous data sources. Although that claim is plausible, it has not been verified by any qualitative studies. It is even unclear how one would quantify the efficiency of information access. In this paper, we propose a measure and then report on a study to identify the information access efficiency gain of a potential application involving information synthesis and aggregation.1   information access efficiency, information aggregation, information access time and speed, information relevance.
      </response>
      <response id="132" uni="The University of Sydney" weight="1.85" year="2012">
        Putting the Public into Public Health Information Dissemination: Social Media and Health-related Web Pages   Public health information dissemination represents an interesting combination of broadcasting, sharing, and retrieving relevant health information. Social media-based public health information dissemination offers some particularly interesting characteristics, as individual users or members of the public actually carry out the actions that constitute the dissemination. These actions also may inherently provide novel evaluative information from a document computing perspective, providing information in relation to both documents and indeed the social media users or health consumers themselves. This paper discusses the novel aspects of social media-based public health information dissemination, including a comparison of its characteristics with search engine-based Web document retrieval. A preliminary analysis of a sample of public health advice tweets taken from a larger sample of over 4700 tweets sent by Australian health-related organization in February 2012 is described. Various preliminary measures are analyzed from this data to initially suggest possible characteristics of public health information dissemination and document evaluation in micro-blog-based systems based on this sample.  Twitter, Web documents, Public Health
      </response>
      <response id="142" uni="INRIA, CSIRO Mathematical and Information Sciences" weight="1.57" year="1997">
        Reuse of Information through virtual documents  This paper explores the issue of representing textual information in the form of virtual documents that include data and fragments of documents from remote sources - especially from databases and SGML document databases, virtual documents are dynamically generated, and therefore always present up-todate information when they are instantiated.. The benefit of this paradigm is that it allows information to be shared, reused, and adapted for various contexts. A Virtual document specification defines how to find and retrieve information objects from databases or from existing documents, and how to assemble it into another document. Virtual documents can be used to create HTML pages that contain information from one or several remote or local databases, to assembly parts of existing documents into a new one, or to define various views of the same information according to various needs. This paper focuses on the prototype implementation of virtual documents from the perspectives of document authoring and architecture. We propose an SGML syntax for Information Object that includes OQL-queries for retrieving fragments of existing documents, transformations on an intermediate tree representation, and output mapping to the virtual do cument structures. Document Databases, Document Management, HTML, Hypermedia, Information Discovery, SGML, virtual documents.
      </response>
      <response id="137" uni="The Boeing Company" weight="1.32" year="1997">
        Analyzing Image Content for a Large Scale Hypermedia System  The Boeing Company maintains tens of millions of pages of information associated with the manufacture and delivery of its products. Much of this information must be made available electronically. We have developed tools to automatically convert and integrate electronic data into industry standard formats. Some of the technical challenges include I) handling a wide variety of source formats, 2) making sure that the tools scale up to handle millions of pages of information, and 3) adding functionality to graphics. Our system contains over four million pages of text including tens of thousands of graphics. In this paper we describe tools that recognize and use information within airplane-related vector and raster images. Such images include troubleshooting charts, fault reporting diagrams, component location diagrams, component index tables, wiring diagrams, system schematics, parts illustrations, standards tables, and structural and tooling drawings. Each airplane requires conversion of over 20,000 graphics including over 900,000 pieces of cross-referenced information. We are also exploring visual information retrieval strategies, including content-based and similarity-based methods for both vector and raster graphics. information retrieval, hypermedia
      </response>
      <response id="63" uni="Queensland University of Technology" weight="1.31" year="2007">
        Integration of Information Filtering and Data Mining Process for Web Information Retrieval  This paper examines a new approach to Web information retrieval, and proposes a new two stage scheme. The aim of the first stage is to quickly filter irrelevant information based on the user profiles. The proposed user profiles learning algorithm are very efficient and effective within a relevance feedback framework. The aim of the second stage is to apply data mining techniques to rationalize the data relevance on the reduced data set. Our experiments on RCV1 (Reuters Corpus Volume 1) data collection which is used by TREC in 2002 for filtering track show that more effective and efficient access Web information has been achieved by combining the strength of information filtering and data mining method.  Information filtering, User profiles, Data mining, Pattern taxonomic model
      </response>
      <response id="143" uni="RMIT" weight="1.27" year="1997">
        Supporting the Answering Process  This paper is concerned with the way information access systerns support the question answering process. This process includes three stages: question formulation, information gathering, and analysis and synthesis. Standard information access technologies are mainly concerned with the second of these stages, providing little or no support for the last stage. However, the raw information gathered at this point can seldom be used directly as an answer. This paper discusses issues relating to the support of the analysis and synthesis stage, and suggests how information access systems might be extended to better support it. This paper also describes a WWW-based experimental interface that permits the evaluation of alternate ways of supporting this analysis and synthesis stage of the answering process. Information Retrieval, Question Answering, Passage Retrieval, Answer Presentation, Hypertext, WWW.
      </response>
      <response id="8" uni="Macquarie University CSIRO Mathematical and Information Sciences" weight="1.20" year="2002">
        Information Extraction in the KELP Framework  In this paper we describe some early steps in a new approach to information extraction The aim of the kelp project is to combine a variety of natural language processing techniques so that we can extract useful elements of information from a collection of documents and then represent this information tailored to the needs of a specific user Our focus here is on how we can build richly structured data objects by extracting information from web pages as an example we describe the extraction of information from web pages that describe laptop computers A principle goal of this work is the separation of different components of the information extraction task so as to increase portability Keywords Information extraction, natural language generation, document personalisation
      </response>
      <response id="35" uni="CSIRO ICT Centre" weight="0.83" year="2005">
        Document modelling for customised information delivery  As the amount of information available to people multiplies at an increasing speed, it becomes ever more important to deliver information customised to users' specific needs. Natural Language Generation systems coupled with user modeling techniques have been built to address this issue, to produce information that is relevant to the users . A common approach adopted by such systems is an approach based on planning, starting from a discourse (or communicative) goal, and planning the text to be presented to the users. However, these systems are not easy to build and difficult to change by domain experts. One of the problems is that it is hard to specify the plans employed, because they often require knowledge about writing, domain expertise, knowledge of computational linguistics and, finally, knowledge about how to obtain data from the underlying information sources. In this paper, we present our first step to address this problem.   Document modeling, information retrieval, document generation, personalized documents.
      </response>
      <response id="100" uni="Queensland University of Technology University of Otago" weight="0.82" year="2009">
        Word Segmentation for Chinese Wikipedia Using N-Gram Mutual Information  In this paper, we propose an unsupervised segmentation approach, named &quot;n-gram mutual information&quot;, or NGMI, which is used to segment Chinese documents into n-character words or phrases, using langauge statistics drwan from the hinese Wikipedia corpus. This approach alleviates the themendous effor that is required in preparing and maintaining the manually segmented Chinese test for training purposes, and manually maintaining ever expanding lexicons. Previously, mutual information was used to achieve automated segmentation into 2-character words. The NGMI approach extends the approach to handle longer n-character words. Experiments with heterogeneous documents from the Chinese Wikipedia collection show good results.   Chinese word segmentation, mutual information, n-gram mutual information, boundary confidence
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.82" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="127" uni="Queensland University of Technology, University of Otago" weight="0.71" year="2012">
        An English-Translated Parallel Corpus for the CJK Wikipedia Collections  In this paper, we describe a machine-translated parallel English corpus for the NTCIR Chinese, Japanese and Korean (CJK) Wikipedia collections. This document collection is named CJK2E Wikipedia XML corpus. The corpus could be used by the information retrieval research community and knowledge sharing in Wikipedia in many ways; for example, this corpus could be used for experimentations in cross-lingual information retrieval, cross-lingual link discovery, or omni-lingual information retrieval research. Furthermore, the translated CJK articles could be used to further expand the current coverage of the English Wikipedia.  Information Storage and Retrieval Digital Libraries - collection.
      </response>
      <response id="84" uni="RMIT University" weight="0.69" year="2008">
        The Effect of Using Pitch and Duration for Symbolic Music Retrieval  Quite reasonable retrieval effectiveness is achieved for retrieving polyphonic (multiple notes at once) music that is symbolically encoded via melody queries, using relatively simple pattern matching techniques based on pitch sequences. Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files.  Music information retrieval, Information retrieval, Multimedia resource discovery, Pattern matching
      </response>
      <response id="14" uni="CRC for Enterprise Distributed Systems Technology (DSTC)" weight="0.68" year="2002">
        The Nexus information hub for exploring social-informational context  The Nexus system is an &quot;information hub&quot; that helps users collaboratively manage and organise &quot;contextualised social notifications&quot;. The purpose of the prototype is to act as a foundation for research into human information-sharing activity. The paper describes contextualised social notifications in more depth, links this prototype to the earlier Scuttlebutt prototype (presented at ADCS-6) and describes the architecture of the system and research questions.  Information Retrieval, Personalised Documents, Social Information Sharing
      </response>
      <response id="12" uni="The University of Queensland" weight="0.67" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="101" uni="University of Sydney" weight="0.67" year="2009">
        An Automatic Question Generation Tool for Supporting Sourcing and Integration in Students' Essays   This paper presents a domain independent Automatic Question Generation (AQG) tool that generates questions which can be used as a form of support for students to revise their essay. The focus here is on generating questions based on semantic and syntactic information acquired from citations. The semantic information includes the author's name, the citation type (describing the aim of the cited study, its results or an opinion), the author's expressed sentiment, and the syntactic information of the citation. Pedagogically, the question templates are designed using Bloom's learning taxonomy where the questions reach the Analysis Level. We used 40 undergraduate students essays for our experiment and the Name Entity Recognition component is trained on 20 essays. The result of our experiment shows that the question coverage is 96% and accuracy of generated questions can reach 78%. This AQG tool will be integrated into our peer review system to scaffold feedback from peers.  Question Generation, Electronic Feedback System for Sourcing and Integration in Students' Essay
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.65" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="160" uni="DSTO, Lloyd-Jones Consulting" weight="0.64" year="1999">
        Automatic document metadata extraction and manipulation: a working system for the Intelligence Analyst  This paper discusses the design and implementation of an operational system to aid health intelligence analysts. The HINTS system provides automated support to undertake tasks such as specific health related research and report writing in the face on an ever-growing body of electronic information, available on the web, and on local file systems. Our approach is to provide automated support for document analysis and discovery from technologies that support ad-hoc searching, consistent filtering for specific pieces of information such as hospital facilities, diseases and locations, and that provide document summarisation and keywording. Document metadata is stored in XML in a data structure that allows a variety of searches and views of the document space to be performed. The user interfaces to the system by web browser and a map-based geospatial application. Document Analysis, Document Databases Information Retrieval, XML, Information Extraction
      </response>
      <response id="57" uni="Queensland University of Technology" weight="0.61" year="2006">
        Dual Interactive Information Retrieval  A new task in Interactive Information Retrieval (IIR) is considered - optimization of information retrieval taking into account impact on quality of interaction with the user. Dual IIR is defined.  dual interactive information retrieval, multistage stochastic programming.
      </response>
      <response id="133" uni="Funnelback" weight="0.60" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="85" uni="NICTA Victoria Research Laboratory The University of Melbourne" weight="0.57" year="2008">
        Extraction of Named Entities from Tables in Gene Mutation Literature  Information extraction and text mining are receiving growing attention as useful techniques for addressing the crucial information bottleneck in the biomedical domain. We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how this technique can be leveraged to improve on existing named entity detection systems for mutations.
      </response>
      <response id="94" uni="RMIT University" weight="0.57" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="109" uni="RMIT University" weight="0.44" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="39" uni="University of Wollongong" weight="0.44" year="2005">
        ePOC: Mobile Clinical Information Access and Diffusion in Ambulatory Care Service Settings  This paper represents a preliminary overview (work-in-progress) of a mobile e-Health research and development project and the intrinsic considerations which arise when designing such patient data management systems tailored to ambulatory care. Its purpose is to give an outline of the issues that allow technological enablement of electronic patient data management in the delivery of home-based medical care. While the replacement of more traditional paper-based patient data management using Personal Digital Assistants as a collection platform is technically straightforward, the organizational realignment of an electronic document management system requires careful study and deployment in order to maximize success. We outline the methodological considerations for document management diffusion within this e-Health setting and describe the issues, architecture and proposed rollout of an electronic Point-Of-Care (ePOC) system.  e-Health, document management and workflow, information access and diffusion
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.38" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="90" uni="Queensland University of Technology" weight="0.38" year="2009">
        Collaborative Filtering Recommender Systems based on Popular Tags  The social tags in web 2.0 are becoming another important information source to profile users' interests and preferences for making personalized recommendations. However, the uncontrolled vocabulary causes a lot of problems to profile users accurately, such as ambiguity, synonyms, misspelling, low information sharing etc. To solve these problems, this paper proposes to use popular tags to represent the actual topics of tags, the content of items, and also the topic interests of users. A novel user profiling approach is proposed in this paper that first identifies popular tags, then represents users' original tags using the popular tags, finally generates users' topic interests based on the popular tags. A collaborative filtering based recommender system has been developed that builds the user profile using the proposed approach. The user profile generated using the proposed approach can represent user interests more accurately and the information sharing among users in the profile is also increased. Consequently the neighborhood of a user, which plays a crucial role in collaborative filtering based recommenders, can be much more accurately determined. The experimental results based on real world data obtained from Amazon.com show that the proposed approach outperforms other approaches.  Information Retrieval, recommender systems, social tags, web 2.0
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.37" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.36" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="6" uni="CSIRO Mathematical and Information Sciences" weight="0.36" year="2002">
        XML Document Retrieval with PADRE  One paradigm of XML retrieval is database- style querying of semi-structured data. Another paradigm is based on information retrieval involving ranking of documents or document fragments. The INEX project attempts to integrate these paradigms and provide an environment for conducting retrieval experiments on semi-structured data. This paper discusses our participation in the INEX project and what we discovered about combining these paradigms.  Document Databases, Document Standards, Information Retrieval
      </response>
      <response id="28" uni="RMIT University" weight="0.35" year="2004">
        Is CORI Effective for Collection Selection? An Exploration of Parameters, Queries, and Data  In distributed information retrieval, a wide range of techniques have been proposed for choosing collections to interrogate. Many of these collection-selection techniques are based on ranking the lexicons; of these, arguably the best known is the CORI collection ranking metric, which includes several parameters that, in principle, should be tuned for different data sets. However, parameters chosen in early work on CORI have been used without alteration in almost all subsequent work, despite drastic differences in the data collections. We have explored the behaviour of CORI for a range of data sets and parameter values. It appears that parameters cannot reliably be chosen for CORI: not only do the optimal choices vary between data sets, but they also vary between query types and, indeed, vary wildly within query sets. Coupled with the observation that even CORI with optimal parameters is usually less effective than other methods, we conclude that the use of CORI as a benchmark collection selection method is inappropriate. Keywords Lexicon indexing, distributed retrieval, information retrieval.
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="0.33" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="105" uni="University of Otago" weight="0.33" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="155" uni="RMIT" weight="0.33" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.32" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="65" uni="University of Melbourne" weight="0.32" year="2007">
        Automatic Thread Classification for Linux User Forum Information Access  We experiment with text classification of threads from Linux web user forums, in the context of improving information access to the problems and solutions described in the threads. We specifically focus on classifying threads according to: (1) them describing a specific problem vs. containing a more general discussion; (2) the completeness of the initial post in the thread; and (3) whether problem(s) in the initial post are resolved in the thread or not. We approach these tasks in both classification and regression frameworks using a range of machine learners and evaluation metrics.  Web Documents, Document Management
      </response>
      <response id="96" uni="RMIT University" weight="0.32" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="78" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.31" year="2008">
        Term-Frequency Surrogates in Text Similarity Computations  Inverted indexes on external storage perform best when accesses are ordered and data is read sequentially, so that seek times are minimized. As a consequence, the various items required to compute Boolean, ranked and phrase queries are often interleaved in the inverted lists. While suitable for query types in which all items are required, this arrangement has the drawback that other query types - notably pure ranked queries and conjunctive Boolean queries - do not require access to word position information, and that component of each posting must be bypassed when these queries are being handled. In this paper we show that the term frequency component of each posting can be completely replaced by a surrogate that allows skipping of positional information interleaved in inverted lists, and obtain significant speedups in ranked query execution without increasing the index size, and without harming retrieval effectiveness. We also explore two methods of reconstituting approximations to the original term frequencies that can be employed if use of the surrogates is deemed too risky. Our simple improvement can thus be used with all ranking functions that make use of term frequencies.  Information retrieval, inverted index, skip pointer, proximity query, efficiency, effectiveness.
      </response>
      <response id="9" uni="University of Sydney" weight="0.30" year="2002">
        Workflow Based Just-in-time Training  This paper focuses on the problem of information overload for newcomers in an organisation. We propose to address it by constructing a smart personal training assistant based upon workflow tools to drive temporal management of a just-in time workplace training system which will deliver a personalised and structured presentation of organisational documents.  Document Workflow, Document Management, Information Retrieval.
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.30" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="122" uni="Queensland University of Technology, University of Otago" weight="0.29" year="2011">
        Mobile Applications of Focused Link Discovery  Interaction with a mobile device remains difficult due to inherent physical limitations. This difficulty is particularly evident for search, which requires typing. We extend the One-Search-Only search paradigm by adding a novel link-browsing scheme built on top of automatic link discovery. A prototype was built for iPhone and tested with 12 subjects. A post-use interview survey suggests that the extended paradigm improves the mobile information seeking experience.  Focused Link Discovery, Wikipedia, Mobile Information Seeking, User Studies Involving Documents.
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.28" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="48" uni="CSIRO ICT Centre" weight="0.27" year="2006">
        My Instant Expert  This paper gives an overview of a mobile device question answering application that has recently been developed in the CSIRO ICT Centre. The application makes use of data in an open-domain encyclopaedia to answer general knowledge questions. The paper presents the techniques used, results and error analysis on the project.  Information retrieval, Question answering
      </response>
      <response id="156" uni="Cardiff University" weight="0.27" year="1999">
        Effective Reuse of Textual Documents Containing Tabular Information  This paper presents an overview of a toolkit that can facilitate efficient reuse of tables appearing within textual documents [1]. In order to effectively reuse information contained in these documents, it is important that we process the accompanying text as well as any tables that appear. From this text, it may be possible to extract metadata such as descriptions of table content and related formulae, mappings and constraints. This metadata can then be exploited to enhance the value of extracted tables during their subsequent reuse. In this paper we present a discussion of the techniques used to process tables and associated text, both of which rely heavily on the use of regular expressions. Our techniques for locating tables utilise similar visual clues used by other table processing techniques discussed in the literature [5,6,7,8], although our approach to exploiting them is quite different. Our tools have been designed to provide a high level of support for the numerous types of table layout encountered in plain text tables, an area that has previously been somewhat overlooked.
      </response>
      <response id="67" uni="The Australian National University, CSIRO ICT Centre" weight="0.27" year="2007">
        A Framework for Measuring the Impact of Web Spam  Web spam potentially causes three deleterious effects: unnecessary work for crawlers and search engines; diversion of traffic away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here we focus on the deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web spam. We index the 80 million page UK2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments.  Web Information Retrieval, Web Spam, Adversarial Information Retrieval
      </response>
      <response id="131" uni="Queensland University of Technology" weight="0.25" year="2012">
        Pairwise Similarity of TopSig Document Signatures  This paper analyses the pairwise distances of signatures produced by the TopSig retrieval model on two document collections. The distribution of the distances are compared to purely random signatures. It explains why TopSig is only competitive with state of the art retrieval models at early precision. Only the local neighbourhood of the signatures is interpretable. We suggest this is a common property of vector space models.   [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval Models Signature Files, Topology, Vector Space IR, Random Indexing, Document Signatures, Search Engines, Document Clustering, Near Duplicate Detection, Relevance Feedback
      </response>
      <response id="103" uni="Queensland University of Technology" weight="0.25" year="2009">
        Investigating the use of Association Rules in Improving Recommender Systems  Recommender systems are widely used online to help users find other products, items etc that they may be interested in based on what is known about that user in their profile. Often however user profiles may be short on information and thus when there is not sufficient knowledge on a user it is difficult for a recommender system to make quality recommendations. This problem is often referred to as the cold-start problem. Here we investigate whether association rules can be used as a source of information to expand a user profile and thus avoid this problem, leading to improved recommendations to users. Our pilot study shows that indeed it is possible to use association rules to improve the performance of a recommender system. This we believe can lead to further work in utilising appropriate association rules to lessen the impact of the cold-start problem.  Information Retrieval, Personalised Documents, Recommender Systems, Association Rules.
      </response>
      <response id="2" uni="University of Sydney" weight="0.25" year="2002">
        Generating and Comparing Models within an Ontology An ontology is useful for providing a conceptually concise basis for developing and communicating knowledge. This paper discusses an application of an automatically constructed ontology of computer science and its use for comparison of models in the computer science domain. We present the architecture, algorithms and current results for MECUREO, a system which builds an extensive model of a computer science entity from limited information. The entity might be a document such as an email message, a course description document or a biography. It is intended to give a measure of the similarity of any two such models. We describe MECUREO's mechanism for constructing and representing models and its present performance in comparing models. Keywords information retrieval, ontologies, acquisition, sharing and reuse of conceptual structures
      </response>
      <response id="125" uni="RMIT" weight="0.23" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.22" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="69" uni="CSIRO ICT Centre" weight="0.22" year="2007">
        Document Composition and Content Selection Evaluation  Our work is concerned with the design of adaptive hypertext systems that produce documents tailored to their intended reader. In our approach, a system composes document on-the-fly, assembling existing text fragments. One of our challenges in this approach is to support the technical writer who configures the system. The task of the technical writer is to specify the structure of the documents to be generated, together with their applicability conditions. To perform their task, authors need to know what information is available. In this paper, we examine the impact of different strategies for presenting the existing text fragments on the task of document composition. We focus in particular on the impact on the quality of the resulting documents. We found that people compose better documents when existing text fragments are presented in a structured way.  document composition, information reuse, document quality, evaluation, method.
      </response>
      <response id="164" uni="Griffith University" weight="0.21" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="46" uni="Melbourne" weight="0.21" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="148" uni="Macquarie University, CSIRO Mathematical and Information Sciences" weight="0.21" year="1998">
        Using Natural Language Generation Techniques to Produce Virtual Documents  With the increasing importance of Web publishing, there has been considerable interest in the production of virtual documents on demand. The bulk of this work has used existing documents annotated with meta-data as a source. We suggest that more flexibility and functionality can be obtained if virtual documents are generated instead from raw data. This capability can be achieved by using natural language generation techniques. In this paper, we describe a project concerned with automatically generating natural language descriptions of museum artefacts directly from a museum's Collection Information System. Natural Language Generation, Databases
      </response>
      <response id="74" uni="RMIT University" weight="0.20" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="126" uni="University of Otago" weight="0.19" year="2012">
        Effects of Spam Removal on Search Engine Efficiency and Effectiveness  Spam has long been identified as a problem that web search engines are required to deal with. Large collection sizes are also an increasing issue for institutions that do not have the necessary resources to process them in their entirety. In this paper we investigate the effect that withholding documents identified as spam has on the resources required to process large collections. We also investigate the resulting search effectiveness and efficiency when different amounts of spam are withheld. We find that by removing spam at indexing time we are able to decrease the index size without affecting the indexing throughput, and are able to improve search precision for some thresholds.  Information Search and Retrieval, Content Analysis and Indexing - Indexing methods Information Filtering
      </response>
      <response id="22" uni="RMIT University" weight="0.18" year="2002">
        Improved use of Contextual Information in Cross-language Information Retrieval  In this paper, we explore Dictionary based context sensitive translation, a framework for query translation to reduce the translation ambiguity and improve the translation quality in English to Chinese cross-language information retrieval (CLIR). Our paper explores the effect of the context window size on translation effectiveness. We assume that the correct translations of the query key terms tend to co-occur together at a high frequency and incorrect translations do not. Our experimental results showed that when using a window size of 10, context sensitive translation results in a dramatic improvement in retrieval performance, it brings about a 30% improvement compared to the results of previous Dictionary based approaches that used only Immediately adjacent words for context.
      </response>
      <response id="108" uni="Gunma University" weight="0.18" year="2010">
        Composition and Decomposition of Japanese Katakana and Kanji Morphemes for Decision Rule Induction from Patent Documents  We propose a new method to construct a word list for rule induction from Japanese patent documents. For word segmentation in Japanese, statistical morphological analyzers have been used in many applications. However, the output of these morphological analyzers presents defects when analyzing unknown words, specifically words that contain Kanji/Katakana morphemes. Some words are overly segmented, and their original meanings are obscured. Furthermore, boundaries between compound nouns are uncertain, which impedes investigation in the initial stages of the application. In our method, we first perform morphological analysis to segment sentences into morphemes. Second, segmented compound words are filtered by character types and Katakana/Kanji morphemes in the compound words are concatenated. Third, the concatenated morphemes are truncated to reduce verbosity. Then, words comprising Katakana/Kanji are retained for use in a word list for rule induction. The experiment results show that our method is effective for extracting decision rules for patent classification.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="115" uni="Macquarie University" weight="0.17" year="2010">
        A Rule-based Approach for Automatic Identification of Publication Types of Medical Papers  The medical domain has an abundance of textual resources of varying quality. The quality of medical articles depends largely on their publication types. However, identifying high-quality medical articles from search results is till date a manual and time-consuming process. We present a simple, rule-based, post-retrieval approach to automatically identify medical articles belonging to three high-quality publication types. Our approach simply uses title and abstract information of the articles to perform this. Our experiments show that such a rule-based approach has close to 100% precision and recall for the three publication types.   Medical Document Classification, Postretrieval Classification, Rule-based Classification, Evidence-based Medicine
      </response>
      <response id="15" uni="University of Sydney" weight="0.15" year="2002">
        Liquid Miro: Semantic Softlinking to Support Cooperative Document Exploration  In  this  position  paper  we  present  a  non-intrusive mechanisim  for  evolving  the  overall  quality  of semantic  relationships  between  elements  of information  in  hypermedia  document  systems.  The evolutionary  aspect  of  this  work  is  an  application framework  that  includes a  combination of hard  links and temporal soft links between existing documents. A hard  link  completes  the  binding  between  two documents in the system upon creation, wheras a soft link  delays  the  binding  until  some  later  time.  The ablity  to monitor, weight,  integrate,  delay,  and  then order the soft links is what offers the power in Liquid Miro document systems. Here we focus on  the use of hypermedia document systems which support existing online  communities,  ranging  from  social  to professional groups..  Personalised  Documents,  Document Management
      </response>
      <response id="43" uni="The University of Sydney" weight="0.15" year="2005">
        Cross Training and Under Sampling in Categorization of Company Announcements  To process the documents in a share market is crucial. It is because financial activities are socio-economic driven and text documents contain a lot of valuable information. In this paper, we focus on one of these documents, the Company Announcement. Each of these documents requires to be labelled as price sensitive or not before presenting to the general public. In our experiments, we study two specific issues in this text categorization, namely the effectiveness of a feature vector obtained from the corpus belonging to another market sector and the imbalanced nature of the dataset. Our results indicate that the classification can benefit from a different (but related) set of corpus because of a more diversified and generalised nature of the feature set. Regarding the skewness of the dataset, the under-sampling of the majority class in the training process does not have a strong effect on the performance in the test set, while keeping the computational cost minimised.  Document Management, Text Categorization
      </response>
      <response id="135" uni="CSIRO" weight="0.15" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="30" uni="RMIT University" weight="0.14" year="2004">
        A Testbed for Indonesian Text Retrieval  Indonesia is the fourth most populous country and a close neighbour of Australia. However, despite media and intelligence interest in Indonesia, little work has been done on evaluating Information Retrieval techniques for Indonesian, and no standard testbed exists for such a purpose. An effective testbed should include a collection of documents, realistic queries, and relevance judgements. The TREC and TDT testbeds have provided such an environment for the evaluation of English, Mandarin, and Arabic text retrieval techniques. The NTCIR testbed provides a similar environment for Chinese, Korean, Japanese, and English. This paper describes an Indonesian TREC-like testbed we have constructed and made available for the evaluation of ad hoc retrieval techniques. To illustrate how the test collection is used, we briefly report the effect of stemming for Indonesian text retrieval, showing - similarly to English - that it has little effect on accuracy.  Indonesian, queries, collection, relevance judgements, stemming
      </response>
      <response id="98" uni="University of Sydney" weight="0.13" year="2009">
        Feature Selection and Weighting Methods in Sentiment Analysis  Sentiment analysis is the task of identifying whether the opinion expressed in a document is positive or negative about a given topic. Unfortunately, many of the potential applications of sentiment analysis are currently infeasible due to the huge number of features found in standard corpora. In this paper we systematically evaluate a range of feature selectors and feature weights with both Naive Bayes and Support Vector Machine classifiers. This includes the introduction of two new feature selection methods and three new feature weighting methods. Our results show that it is possible to maintain a state-of-the art classification accuracy of 87.15% while using less than 36% of the features.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="44" uni="University of Otago" weight="0.13" year="2005">
        Recommending Geocaches  Players downloading GPS coordinates from the internet, hiking to the given spot, and hunting for a hidden box - this is the new sport of geocaching. Today there are nearly 200,000 such boxes in over 200 countries. With so many to find, a recommender is needed, one that takes into account not only the boxes, but also the geospatial and temporal nature of the sport. A database of geocaches in the South Island of New Zealand is made by trawling a prominent geocaching web site. This is then used to estimate the home-coordinates (geospatial playing centre) of players. Predictions are verified against a set of correct coordinates solicited from players. Several geocache recommenders are discussed and compared. The precision, computed using mean of mean reciprocal rank (MMRR), of each is measured. The best method tried is a collaborative filter using intersection over mean to find similar players and a voting scheme to recommend geocaches. This method is proposed as a replacement for the currently used distance from home-coordinate; doing so will increase the precision of existing systems such as geocaching. com.  Information Retrieval.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.13" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="102" uni="University of Melbourne" weight="0.13" year="2009">
        You Are What You Post: User-level Features in Threaded Discourse  We develop methods for describing users based on their posts to an online discussion forum. These methods build on existing techniques to describe other aspects of online discussions communities, but the application of these techniques to describing users is novel. We demonstrate the utility of our proposed methods by showing that they are superior to existing methods over a post-level classification task over a published real-world dataset.  Document Management, Information Retrieval, Web Documents
      </response>
      <response id="42" uni="University of Sydney" weight="0.12" year="2005">
        Biomedical Named Entity Recognition System  We propose a machine learning approach, using a Maximum Entropy (ME) model to construct a Named Entity Recognition (NER) classifier to retrieve biomedical names from texts. In experiments, we utilize a blend of various linguistic features incorporated into the ME model to assign class labels and location within an entity sequence, and a postprocessing strategy for corrections to sequences of tags to produce a state of the art solution. The experimental results on the GENIA corpus achieved an F-score of 68.2% for semantic classification of 23 categories and achieved F-score of 78.1% on identification.  Named Entity Recognition, ME model, Information Retrieval.
      </response>
      <response id="82" uni="RMIT University, University of Technology Dresden" weight="0.12" year="2008">
        WebKnox: Web Knowledge Extraction  The paper describes and evaluates a system for extracting knowledge from the web that uses a domain independent fact extraction approach and a self supervised learning algorithm. Using a trust algorithm, the precision of the system is improved to over 70% compared with a baseline of 52%.  Information Extraction, Web Mining
      </response>
      <response id="54" uni="CSIRO ICT Centre" weight="0.12" year="2006">
        InexBib - Retrieving XML elements based on external evidence  Creating a scientific bibliography on a given topic is currently a task which requires a great deal of manual effort. We attempt to reduce this effort by developing a tool for automatically generating a bibliography from a collection of articles represented in XML. We evaluate the use of elements around the references as anchortexts to improve search results. We find that users of the tool prefer lists generated using anchortext over those generated from the bibliography entry only and that the preference is statistically significant. We tentatively find no significant preference for results generated using paragraph as opposed to sentence level anchortext, but note that this finding may result from lack of sophistication in resolving text including multiple references.  Information Retrieval, XML, Element Retrieval, Bibliography
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.12" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.11" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="62" uni="University of Otago" weight="0.11" year="2007">
        IR Evaluation Using Multiple Assessors per Topic  Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006. Traditionally the assessment of a single topic is performed by a single human. Herein we examine the consequences of using multiple assessors per topic. A set of 15 topics were used. The mean topic pool contained 98 documents. Between 3 and 5 separate assessors (per topic) assessed all documents in a pool. One assessor was designated baseline. All were then used to generate 10,000 synthetic multi-assessor assessment sets. The baseline relative rank order of all runs submitted to the INEX 2006 relevant-in-context task was compared to those of the synthetics. The mean Spearman's rank correlation coefficient was 0.986 and all coefficients were above 0.95 - the correlation is very strong. Non matching rank-orders are seen when the mean average precision difference between runs is less than 0.05. In the top 10 runs no significantly different runs were ranked in a different order in more than 5% of the synthetics. Using multiple assessors per topic is very unlikely to affect the outcome of an evaluation forum.  Information Retrieval
      </response>
      <response id="1" uni="University of Queensland" weight="0.11" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.11" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.10" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.10" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.10" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="36" uni="RMIT" weight="0.10" year="2005">
        Readability of French as a Foreign Language and its Uses  Reading is an important means of foreign language acquisition, particularly for vocabulary. Providing reading material that is of a suitable level of difficulty allows users to acquire vocabulary the most efficiently. Thus an on-line reading material recommender system for language learners requires a readability measure so that the difficulty of texts can be automatically assessed. However, most readability measures were developed for native child speakers of English. In this article I discuss an experiment in readability for learners of French. I conclude that using the average number of words per sentence correlates more closely with human judgements than many commonly available readability measures. I propose a new readability measure for learners of French that have English as their main language, which combines sentence length with the number of words that are similar in both languages (cognates). This measure slightly improves on sentence length for modelling French readability.  Text readability, Information retrieval
      </response>
      <response id="138" uni="National Taiwan University" weight="0.09" year="1997">
        A Multi-party Document Model  A multi-party document (MPD) is one that contains multiple parts intended for multiple recipients in that each recipient does not necessarily need to know the existence, and therefore the corresponding content, of the other parts of the document intended for other recipients. MPDs pose new requirements, namely, transparency and security, to the underlying system. This paper reviews the traditional document models and their shortcomings with respect to the stated requirements and proposes a new model that satisfies those requirements. The major implementation issues including document construction and encryption for MPD supports are also discussed. Shifting these capabilities to the system level will relieve burdens of users and enhance the security measure of the system for protecting the integrity and privacy of document contents. The issues discussed in this paper are important for making a sound underlying support system for the development of information systems, in particular in the Intranets environment. Keywords Document management, document structure, security, encryption, Intranets.
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="0.09" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
    </responses>
  </theme>
  <theme id="6" title="Theme 6">
    <words>
      <word weight="4.00041893692">
        queri
      </word>
      <word weight="1.44583732081">
        click
      </word>
      <word weight="1.32058764124">
        similar
      </word>
      <word weight="1.22617327204">
        retriev
      </word>
      <word weight="1.12042283695">
        descript
      </word>
      <word weight="1.11811283559">
        rank
      </word>
    </words>
    <responses>
      <response id="47" uni="ICT Centre CSIRO" weight="2.62" year="2006">
        Improving rankings in small-scale web search using click-implied descriptions    When a searcher submits a query Q and clicks on document R in the corresponding result set, we may plausibly interpret the click as a vote that Q is a description of R. We call the Q and R pairing a 'click description'. Click descriptions thus derived from search engine logs can be accumulated into surrogate documents and used to boost retrieval effectiveness in a similar fashion to anchor text. We investigate the usefulness of click description surrogate documents in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92% improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link  Information Storage and Retrieval, Content Analysis and Indexing [Indexing methods]
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="2.53" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="78" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="1.52" year="2008">
        Term-Frequency Surrogates in Text Similarity Computations  Inverted indexes on external storage perform best when accesses are ordered and data is read sequentially, so that seek times are minimized. As a consequence, the various items required to compute Boolean, ranked and phrase queries are often interleaved in the inverted lists. While suitable for query types in which all items are required, this arrangement has the drawback that other query types - notably pure ranked queries and conjunctive Boolean queries - do not require access to word position information, and that component of each posting must be bypassed when these queries are being handled. In this paper we show that the term frequency component of each posting can be completely replaced by a surrogate that allows skipping of positional information interleaved in inverted lists, and obtain significant speedups in ranked query execution without increasing the index size, and without harming retrieval effectiveness. We also explore two methods of reconstituting approximations to the original term frequencies that can be employed if use of the surrogates is deemed too risky. Our simple improvement can thus be used with all ranking functions that make use of term frequencies.  Information retrieval, inverted index, skip pointer, proximity query, efficiency, effectiveness.
      </response>
      <response id="38" uni="RMIT University" weight="1.43" year="2005">
        Document Expansion versus Query Expansion for Ad-hoc Retrieval  In document information retrieval, the terminology given by a user may not match the terminology of a relevant document. Query expansion seeks to address this mismatch; it can significantly increase effectiveness, but is slow and resource-intensive. We investigate the use of document expansion as an alternative, in which documents are augmented with related terms extracted from the corpus during indexing, and the overheads at query time are small. We propose and explore a range of corpus-based document expansion techniques and compare them to corpus-based query expansion on TREC data. These experiments show that document expansion delivers at best limited benefits, while query expansion - including standard techniques and efficient approaches described in recent work - delivers consistent gains. We conclude that document expansion is unpromising, but it is likely that the efficiency of query expansion can be further improved.  Document expansion, automatic query expansion, pseudo relevance feedback, efficiency
      </response>
      <response id="125" uni="RMIT" weight="0.76" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.62" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="88" uni="University of Melbourne" weight="0.46" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="136" uni="Queensland University of Technology" weight="0.45" year="2012">
        Relationship between the nature of the Search Task Types and Query Reformulation Behaviour  Success of query reformulation and relevant information retrieval depends on many factors, such as users' prior knowledge, age, gender, and cognitive styles. One of the important factors that affect a user's query reformulation behaviour is that of the nature of the search tasks. Limited studies have examined the impact of the search task types on query reformulation behaviour while performing Web searches. This paper examines how the nature of the search tasks affects users' query reformulation behaviour during information searching. The paper reports empirical results from a user study in which 50 participants performed a set of three Web search tasks - exploratory, factorial and abstract. Users' interactions with search engines were logged by using a monitoring program. 872 unique search queries were classified into five query types - New, Add, Remove, Replace and Repeat. Users submitted fewer queries for the factual task, which accounted for 26%. They completed a higher number of queries (40% of the total queries) while carrying out the exploratory task. A one-way MANOVA test indicated a significant effect of search task types on users' query reformulation behaviour. In particular, the search task types influenced the manner in which users reformulated the New and Repeat queries. Keywords Query reformulation behaviour, information behaviour, information retrieval, search task complexity, user studies
      </response>
      <response id="30" uni="RMIT University" weight="0.44" year="2004">
        A Testbed for Indonesian Text Retrieval  Indonesia is the fourth most populous country and a close neighbour of Australia. However, despite media and intelligence interest in Indonesia, little work has been done on evaluating Information Retrieval techniques for Indonesian, and no standard testbed exists for such a purpose. An effective testbed should include a collection of documents, realistic queries, and relevance judgements. The TREC and TDT testbeds have provided such an environment for the evaluation of English, Mandarin, and Arabic text retrieval techniques. The NTCIR testbed provides a similar environment for Chinese, Korean, Japanese, and English. This paper describes an Indonesian TREC-like testbed we have constructed and made available for the evaluation of ad hoc retrieval techniques. To illustrate how the test collection is used, we briefly report the effect of stemming for Indonesian text retrieval, showing - similarly to English - that it has little effect on accuracy.  Indonesian, queries, collection, relevance judgements, stemming
      </response>
      <response id="146" uni="RMIT" weight="0.43" year="1997">
        Conflation-based Comparison of Stemming Algorithms  In text database systems, query terms are stemmed to allow them to be conflated with variant forms of the same word. On the one hand, stemming allows the query mechanism to find documents that would otherwise not contain matches to the query terms; on the other hand, automatic stemming is prone to error, and can lead to retrieval of inappropriate documents. In this paper we investigate several stemming algorithms, measuring their ability to correctly conflate terms from a large text collection. We show that stemming is indeed worthwhile, but that each of the stemming algorithms we consider has distinct advantages and disadvantages; choice of stemming algorithm affects the behaviour of the retrieval mechanism. information retrieval, document databases, digital libraries, word disambiguation.
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.37" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="97" uni="University of Otago" weight="0.36" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="152" uni="Griffith University" weight="0.36" year="1999">
        DYNAMIC HYPER-LINKING BY QUERYING FOR A FCA-BASED QUERY SYSTEM  This paper presents a mechanism for hyper-linking documents by search-terms. Search-terms are selected by the user interactively building a formal concept lattice. In order to explain this interface we give some background to Formal Concept Analysis and an example is developed which illustrates the use of the concept lattice. Selected search-terms are used to create hyper-links, based on term repetition. As the search-terms differ between queries, we need a mechanism from which to dynamically create the target hyper-linked HTML documents. Therefore, documents are stored in a structure which is based on a word-list rather than plain text format. The documents are represented as links between the individual words within the word-list In so doing the word-list becomes a full-text-retrieval index into each word in each of those documents and therefore provides a good basis for the fast creation of an HTML document set from specific queries by keywords. To have the words in a word-list from which the documents are created also allows easy classification of words which should be hyper-linked within specific HTML documents. Furthermore, both documents and hyper-linking keywords are stored as well in this structure since any word in any document is indexed by the word-list. Document Databases, WWW and Internet.
      </response>
      <response id="28" uni="RMIT University" weight="0.36" year="2004">
        Is CORI Effective for Collection Selection? An Exploration of Parameters, Queries, and Data  In distributed information retrieval, a wide range of techniques have been proposed for choosing collections to interrogate. Many of these collection-selection techniques are based on ranking the lexicons; of these, arguably the best known is the CORI collection ranking metric, which includes several parameters that, in principle, should be tuned for different data sets. However, parameters chosen in early work on CORI have been used without alteration in almost all subsequent work, despite drastic differences in the data collections. We have explored the behaviour of CORI for a range of data sets and parameter values. It appears that parameters cannot reliably be chosen for CORI: not only do the optimal choices vary between data sets, but they also vary between query types and, indeed, vary wildly within query sets. Coupled with the observation that even CORI with optimal parameters is usually less effective than other methods, we conclude that the use of CORI as a benchmark collection selection method is inappropriate. Keywords Lexicon indexing, distributed retrieval, information retrieval.
      </response>
      <response id="11" uni="The University of Queensland" weight="0.34" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.33" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.31" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="22" uni="RMIT University" weight="0.31" year="2002">
        Improved use of Contextual Information in Cross-language Information Retrieval  In this paper, we explore Dictionary based context sensitive translation, a framework for query translation to reduce the translation ambiguity and improve the translation quality in English to Chinese cross-language information retrieval (CLIR). Our paper explores the effect of the context window size on translation effectiveness. We assume that the correct translations of the query key terms tend to co-occur together at a high frequency and incorrect translations do not. Our experimental results showed that when using a window size of 10, context sensitive translation results in a dramatic improvement in retrieval performance, it brings about a 30% improvement compared to the results of previous Dictionary based approaches that used only Immediately adjacent words for context.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.28" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="70" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.26" year="2007">
        Hybrid Bitvector Index Compression  Bitvector index representations provide fast resolution of conjunctive Boolean queries, but require a great deal of storage space. On the other hand, compressed index representations are space-efficient, but query evaluation tends to be slower than bitvector evaluation, because of the need for sequential or pseudo-random access into the compressed index lists. Here we investigate a simple hybrid mechanism that stores only a small fraction of the inverted lists as bitvectors and has no or negligible effect on compressed index size compared to the use of byte codes, but improves query processing throughput compared to both byte coded representations and entirely-bitvector arrangements.  Index compression, bitvector, byte code, intersection algorithm.
      </response>
      <response id="73" uni="RMIT University, INRIA" weight="0.26" year="2007">
        Use of Wikipedia Categories in Entity Ranking  Wikipedia is a useful source of knowledge that has many applications in language processing and knowledge representation. The Wikipedia category graph can be compared with the class hierarchy in an ontology; it has some characteristics in common as well as some differences. In this paper, we present our approach for answering entity ranking queries from the Wikipedia. In particular, we explore how to make use of Wikipedia categories to improve entity ranking effectiveness. Our experiments show that using categories of example entities works significantly better than using loosely defined target categories.
      </response>
      <response id="77" uni="RMIT University" weight="0.25" year="2007">
        Querying Image Ontology  Content-based image retrieval has been used in various application domains, but the semantic gap problem remains a challenge to be overcome. One possible way to overcome this problem is to represent the knowledge extracted from the low-level image features through semantic concepts. In this paper we describe how we use an image ontology to this end. We show that we are able to retrieve desired images by using basic ontology queries.  Ontology, CBIR, semantic gap
      </response>
      <response id="46" uni="Melbourne" weight="0.24" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.24" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="37" uni="The University of Melbourne" weight="0.23" year="2005">
        In Search of Reliable Retrieval Experiments  There are several ways in which an 'improved' technique for solving some computational problem can be defended: by mathematical argument; by simulation; and by experimental validation. Each of these has risks. In this paper we describe some of the issues that arose during an experimental validation of architectures for distributed text query evaluation, and the approaches that were taken to resolve them. In particular, collections and clusters must be scaled in a way that maximizes comparability between different data sizes; query sets must be appropriate to the target collection; and hardware issues such as file placement on disk must also be considered. Our intention is to report on our experience in a practical sense, and thereby assist others to avoid the same problems.
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.23" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.21" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="66" uni="RMIT University" weight="0.20" year="2007">
        Source Code Authorship Attribution using n-grams  Plagiarism and copyright infringement are major problems in academic and corporate environments. Existing solutions for detecting infringements in structured text such as source code are restricted to textual similarity comparisons of two pieces of work. In this paper, we examine authorship attribution as a means for tackling plagiarism detection. Given several samples of work from several authors, we attempt to correctly identify the author of work presented as a query. On a collection of 1 640 documents written by 100 authors, we show that we can attribute authorship in up to 67% of cases. This work can be a valuable additional indicator for the more difficult plagiarism investigations.  Authorship Attribution, Plagiarism Detection, Co-derivative Documents
      </response>
      <response id="106" uni="RMIT University" weight="0.19" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.18" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="84" uni="RMIT University" weight="0.18" year="2008">
        The Effect of Using Pitch and Duration for Symbolic Music Retrieval  Quite reasonable retrieval effectiveness is achieved for retrieving polyphonic (multiple notes at once) music that is symbolically encoded via melody queries, using relatively simple pattern matching techniques based on pitch sequences. Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files.  Music information retrieval, Information retrieval, Multimedia resource discovery, Pattern matching
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.15" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="6" uni="CSIRO Mathematical and Information Sciences" weight="0.15" year="2002">
        XML Document Retrieval with PADRE  One paradigm of XML retrieval is database- style querying of semi-structured data. Another paradigm is based on information retrieval involving ranking of documents or document fragments. The INEX project attempts to integrate these paradigms and provide an environment for conducting retrieval experiments on semi-structured data. This paper discusses our participation in the INEX project and what we discovered about combining these paradigms.  Document Databases, Document Standards, Information Retrieval
      </response>
      <response id="150" uni="Dublin City University" weight="0.13" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="110" uni="University of Otago" weight="0.11" year="2010">
        Efficient Accumulator Initialisation  IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. Traditionally, the performance of search engines is dominated by slow disk I/O, CPU-intensive decompression, complex similarity ranking functions and sorting a large number of candidate documents. However, after we have applied a number of optimisation techniques, our search engine is bottlenecked by accumulator initialisation. In this paper, we propose an efficient accumulator initialisation algorithm, which represents the traditional static accumulator array as a logical two dimensional table and uses a number of flags to track the initialisation status of the accumulators. The efficiency of the algorithm is verified by a simulation program and a search engine. The overall performance can be as good as a 93% increase in throughput.  Accumulator Initialisation, Efficiency, Postings Pruning.
      </response>
      <response id="56" uni="Macquarie University" weight="0.11" year="2006">
        Differentiating Document Type and Author Personality from Linguistic Features  There are many ways to profile a collection of documents. This paper presents highlight from a body of work that has looked at individual differences in the language of personal weblogs. Firstly, we present a unitary measure of linguistic contextuality based on POS frequency that can be used to profile and rank genres. When applied to weblogs, we will show they are similar to school essays, yet significantly less contextual than e-mail. We then look at individual variation of language, as due to the personality of the author, exploring the use of dictionary based analyses and data-driven n-grams. Under regression, we show that with just a few linguistic features, it is possible to explain significant proportions of variance within personality traits.  Personalised Documents; Multimedia Resource Discovery
      </response>
      <response id="144" uni="RMIT" weight="0.10" year="1997">
        Collection Selection via Lexicon Inspection  A distributed text database consists of multiple individual text collections. When a query is posed to a distributed text database, significant computational resources can be saved by identifing the individual collections that are the most likely to contain answers, as unnecessary accesses to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be effective, but depends on how performance is measured.
      </response>
      <response id="161" uni="France" weight="0.09" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="3" uni="Bond University" weight="0.09" year="2002">
        Managing Literature References with Topic Maps This article introduces Topic Map (TM) authoring and ontology engineering. With a running example of a simple TM-based literature reference database we show how a localized ontology can be defined to constrain and describe our knowledge domain. We then use the same technique to describe the structure of the BibTEX format. These two ontologies can then be used to formalize a mapping between them. For this purpose we use an experimental TM query language. Topic Maps, References, Ontology
      </response>
      <response id="89" uni="University of Melbourne, Monash University" weight="0.08" year="2008">
        Using Collaboratively Constructed Document Collections to Simulate Real-World Object Comparisons  While the layout of a museum exhibition is largely prescribed by the curator, visitors to museums view connections between exhibits in ways unique to themselves. With the assistance of a large-scale survey of museum visitors we identify that the view taken by museum visitors of a collection of exhibits can be represented by similarity over documents associated with each exhibit. We show that even when using a basic document similarity measure there is a correlation between document similarity and visitors' judgements of relatedness of exhibits aligned to these documents.  User Studies Involving Documents, Web Documents, Cognitive Aspects of Documents.
      </response>
      <response id="107" uni="The University of Melbourne, University of Malaya" weight="0.08" year="2010">
        Estimating System Effectiveness ScoresWith Incomplete Evidence  It is common for only partial relevance judgments to be used when comparing retrieval system effectiveness, in order to control experimental cost. Using TREC data, we consider the uncertainty introduced into per-topic effectiveness scores by pooled judgments, and measure the effect that incomplete evidence has on both the systems scores that are generated, and also on the quality of paired system comparisons. We measure system behavior from three different points of view: the trend in effectiveness scores; the separability of system pairs; and the number of reversals in significance outcomes as the depth of judgments increases. Our results show that when shallow pooled judgments are used system separability remains relatively high, but that there is also a high rate of significance reversal. We then show that explicitly adjusting effectiveness scores to allow for the known amount of uncertainty gives a reduced number of reversals, and hence more consistent experimental outcomes.  Retrieval evaluation, effectiveness metric, pooling
      </response>
    </responses>
  </theme>
  <theme id="7" title="Theme 7">
    <words>
      <word weight="4.41063895742">
        translat
      </word>
      <word weight="3.21321421378">
        extract
      </word>
      <word weight="2.06398131224">
        mlu
      </word>
      <word weight="2.04079760712">
        languag
      </word>
      <word weight="1.60422316916">
        approach
      </word>
      <word weight="1.41786506182">
        inform
      </word>
    </words>
    <responses>
      <response id="61" uni="Queensland University of Technology" weight="2.12" year="2007">
        A Bottom-up Term Extraction Approach forWeb-based Translation in Chinese-English IR Systems  The extraction of Multiword Lexical Units (MLUs) in lexica is important to language related methods such as Natural Language Processing (NLP) and machine translation. As one word in one language may be translated into an MLU in another language, the extraction of MLUs plays an important role in Cross-Language Information Retrieval (CLIR), especially in finding the translation for words that are not in a dictionary. Web mining has been used for translating the query terms that are missing from dictionaries. MLU extraction is one of the key parts in search engine based translation. The MLU extraction result will finally affect the transition quality. Most statistical approaches to MLU extraction rely on large statistical information from huge corpora. In the case of search engine based translation, those approaches do not perform well because the size of corpus returned from a search engine is usually small. In this paper, we present a new string measurement and new Chinese MLU extraction process that works well on small corpora.   Cross-language Information retrieval, CLIR, query translation, web mining, OOV problem, term extraction
      </response>
      <response id="22" uni="RMIT University" weight="1.02" year="2002">
        Improved use of Contextual Information in Cross-language Information Retrieval  In this paper, we explore Dictionary based context sensitive translation, a framework for query translation to reduce the translation ambiguity and improve the translation quality in English to Chinese cross-language information retrieval (CLIR). Our paper explores the effect of the context window size on translation effectiveness. We assume that the correct translations of the query key terms tend to co-occur together at a high frequency and incorrect translations do not. Our experimental results showed that when using a window size of 10, context sensitive translation results in a dramatic improvement in retrieval performance, it brings about a 30% improvement compared to the results of previous Dictionary based approaches that used only Immediately adjacent words for context.
      </response>
      <response id="58" uni="Queensland University of Technology" weight="0.65" year="2006">
        Enhanced web-based translation extraction for English- Chinese CLIR  Dictionary based translation is a traditional approach in use by cross-language information retrieval systems. However, significant performance degradation is often observed when queries contain words that do not appear in the dictionary. This is called the Out of Vocabulary (OOV) problem. The common methods for translation selection for web-based translation always rely on word frequency calculation but the results are not always satisfactory. Our experiments show marked improvement in translation accuracy over other commonly used approaches.
      </response>
      <response id="157" uni="University of Sydney" weight="0.58" year="1999">
        Transformation-Based Learning for Automatic Translation from HTML to XML   Format tags implicitly represent content information in the same ambiguous, context dependent manner that words represent semantics in natural language. Translation from format to content markup shares many characteristics with tagging and parsing tasks in computational linguistics. The transformation-based learning (TEL) paradigm has recently been applied to numerous computational linguistics tasks with considerable success. We present a transformation-based translator which automatically learns to translate semistructured HTML documents formatted with a particular style to XML using a small set of training examples. Keywords HTML, XML, markup, document processing, machine translation, machine learning
      </response>
      <response id="8" uni="Macquarie University CSIRO Mathematical and Information Sciences" weight="0.49" year="2002">
        Information Extraction in the KELP Framework  In this paper we describe some early steps in a new approach to information extraction The aim of the kelp project is to combine a variety of natural language processing techniques so that we can extract useful elements of information from a collection of documents and then represent this information tailored to the needs of a specific user Our focus here is on how we can build richly structured data objects by extracting information from web pages as an example we describe the extraction of information from web pages that describe laptop computers A principle goal of this work is the separation of different components of the information extraction task so as to increase portability Keywords Information extraction, natural language generation, document personalisation
      </response>
      <response id="82" uni="RMIT University, University of Technology Dresden" weight="0.27" year="2008">
        WebKnox: Web Knowledge Extraction  The paper describes and evaluates a system for extracting knowledge from the web that uses a domain independent fact extraction approach and a self supervised learning algorithm. Using a trust algorithm, the precision of the system is improved to over 70% compared with a baseline of 52%.  Information Extraction, Web Mining
      </response>
      <response id="162" uni="University of Sydney" weight="0.23" year="2000">
        Keyword Association Network: A Statistical Multi-term Indexing Approach for Document Categorization  A Keyword Association Network (KAN) is the network of keywords extracted from a collection of documents. In this network, the relationship between keywords is represented by a confidence value. It is argued in this paper thai the semantics and importance of a word can be more clearly and accurately measured by making use of other words that are co-occurring in a given document. The term frequency used for measuring the importance of terms in most document categorization methods ignores this important aspect. A KAN is constructed on the basis of co-occurring terms in documents. If two tenns appear more than a certain number of times in the same documents, they are considered as having close relationship. This paper proposes using KAN as a basis for finding informative keywords and using a confidence value in the process of document categorization. The process of constructing and application of KAN for document categorization is presented and the performance comparison with a typical statistical single-term document categorization algorithm - TFIDF classifier - will be shown. The experimental results show that KAN gives significant benefits. Keywords Document Categorization, Machine Learning. Statistical Multi-term indexing, Semantic- Meaning.
      </response>
      <response id="16" uni="Macquarie University" weight="0.23" year="2002">
        How to Write a Document in Controlled Natural Language  This paper shows how a computer-processable document can be written in a controlled natural language (PENG) with the help of a sophisticated lookahead editor (ECOLE). The editor provides syntactic hints after each word form entered and indicates how the author can continue the text. This way the author does not need to learn or to remember the restrictions of the controlled language. PENG documents are automatically translated into first-order logic via discourse representation structures. These formal entities can be checked by a theorem prover for inconsistency or consistency can be revealed by a model builder.  Document Processing, Controlled Languages, Authoring Tools.
      </response>
      <response id="85" uni="NICTA Victoria Research Laboratory The University of Melbourne" weight="0.23" year="2008">
        Extraction of Named Entities from Tables in Gene Mutation Literature  Information extraction and text mining are receiving growing attention as useful techniques for addressing the crucial information bottleneck in the biomedical domain. We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how this technique can be leveraged to improve on existing named entity detection systems for mutations.
      </response>
      <response id="36" uni="RMIT" weight="0.20" year="2005">
        Readability of French as a Foreign Language and its Uses  Reading is an important means of foreign language acquisition, particularly for vocabulary. Providing reading material that is of a suitable level of difficulty allows users to acquire vocabulary the most efficiently. Thus an on-line reading material recommender system for language learners requires a readability measure so that the difficulty of texts can be automatically assessed. However, most readability measures were developed for native child speakers of English. In this article I discuss an experiment in readability for learners of French. I conclude that using the average number of words per sentence correlates more closely with human judgements than many commonly available readability measures. I propose a new readability measure for learners of French that have English as their main language, which combines sentence length with the number of words that are similar in both languages (cognates). This measure slightly improves on sentence length for modelling French readability.  Text readability, Information retrieval
      </response>
      <response id="88" uni="University of Melbourne" weight="0.20" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="11" uni="The University of Queensland" weight="0.18" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="148" uni="Macquarie University, CSIRO Mathematical and Information Sciences" weight="0.17" year="1998">
        Using Natural Language Generation Techniques to Produce Virtual Documents  With the increasing importance of Web publishing, there has been considerable interest in the production of virtual documents on demand. The bulk of this work has used existing documents annotated with meta-data as a source. We suggest that more flexibility and functionality can be obtained if virtual documents are generated instead from raw data. This capability can be achieved by using natural language generation techniques. In this paper, we describe a project concerned with automatically generating natural language descriptions of museum artefacts directly from a museum's Collection Information System. Natural Language Generation, Databases
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.16" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="56" uni="Macquarie University" weight="0.13" year="2006">
        Differentiating Document Type and Author Personality from Linguistic Features  There are many ways to profile a collection of documents. This paper presents highlight from a body of work that has looked at individual differences in the language of personal weblogs. Firstly, we present a unitary measure of linguistic contextuality based on POS frequency that can be used to profile and rank genres. When applied to weblogs, we will show they are similar to school essays, yet significantly less contextual than e-mail. We then look at individual variation of language, as due to the personality of the author, exploring the use of dictionary based analyses and data-driven n-grams. Under regression, we show that with just a few linguistic features, it is possible to explain significant proportions of variance within personality traits.  Personalised Documents; Multimedia Resource Discovery
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.13" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="164" uni="Griffith University" weight="0.13" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.12" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="117" uni="Queensland University of Technology, CSIRO" weight="0.12" year="2010">
        Rule-based Approach for Identifying Assertions in Clinical Free-Text Data  A rule-based approach for classifying previously identified medical concepts in the clinical free text into an assertion category is presented.There are six different categories of assertions for the task: Present, Absent, Possible, Conditional, Hypothetical and Not associated with the patient. The assertion classification algorithms were largely based on extending the popular NegEx and Context algorithms. In addition, a health based clinical terminology called SNOMED CT and other publicly available dictionaries were used to classify assertions, which did not fit the NegEx/Context model. The data for this task includes discharge summaries from Partners HealthCare and from Beth Israel Deaconess Medical Centre, as well as discharge summaries and progress notes from University of Pittsburgh Medical Centre. The set consists of 349 discharge reports, each with pairs of ground truth concept and assertion files for system development, and 477 reports for evaluation. The system's performance on the evaluation data set was 0.83, 0.83 and 0.83 for recall, precision and F1-measure, respectively. Although the rule-based system shows promise, further improvements can be made by incorporating machine learning approaches.  rule-based, medical concept, assertion, NegEx, Context, SNOMED CT.
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.12" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.12" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="106" uni="RMIT University" weight="0.11" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.11" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="0.10" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.10" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="105" uni="University of Otago" weight="0.10" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="51" uni="University of Sydney" weight="0.09" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
      <response id="104" uni="Queensland University of Technology, University of Otago" weight="0.09" year="2009">
        The Methodology of Manual Assessment in the Evaluation of Link Discovery  The link graph extracted from the Wikipedia has often been used as the ground truth for measuring the performance of automated link discovery systems. Extensive manual assessments experiments at INEX 2008 recently showed that this is unsound and that manual assessment is essential. This paper describes the methodology for link discovery evaluation which was developed for use in the INEX 2009 Link-the-Wiki track. In this approach both manual and automatic assessment sets are generated and runs are evaluated using both. The approach offers a more reliable evaluation of link discovery methods than just automatic assessment. A new evaluation measure for focused link discovery is also introduced.  Wikipedia, Link Quality, Manual Assessment, Evaluation.
      </response>
      <response id="100" uni="Queensland University of Technology University of Otago" weight="0.09" year="2009">
        Word Segmentation for Chinese Wikipedia Using N-Gram Mutual Information  In this paper, we propose an unsupervised segmentation approach, named &quot;n-gram mutual information&quot;, or NGMI, which is used to segment Chinese documents into n-character words or phrases, using langauge statistics drwan from the hinese Wikipedia corpus. This approach alleviates the themendous effor that is required in preparing and maintaining the manually segmented Chinese test for training purposes, and manually maintaining ever expanding lexicons. Previously, mutual information was used to achieve automated segmentation into 2-character words. The NGMI approach extends the approach to handle longer n-character words. Experiments with heterogeneous documents from the Chinese Wikipedia collection show good results.   Chinese word segmentation, mutual information, n-gram mutual information, boundary confidence
      </response>
      <response id="149" uni="CSIRO Mathematical and Information Sciences" weight="0.09" year="1998">
        Automatic Document Creation from Software Specifications  Software documentation, and in particular, on-line help is a crucial aspect of a software system. Producing and maintaining it, however, is both labor intensive and tedious, making it a candidate for automation. This paper presents our work on automatically generating hypertext based on-line help, starting from software specifications. Our approach is motivated by practical considerations, such as the impossibility to construct by hand the semantic knowledge base typically required by a generation system. Natural Language Generation, Software documentation, hypertext
      </response>
      <response id="127" uni="Queensland University of Technology, University of Otago" weight="0.08" year="2012">
        An English-Translated Parallel Corpus for the CJK Wikipedia Collections  In this paper, we describe a machine-translated parallel English corpus for the NTCIR Chinese, Japanese and Korean (CJK) Wikipedia collections. This document collection is named CJK2E Wikipedia XML corpus. The corpus could be used by the information retrieval research community and knowledge sharing in Wikipedia in many ways; for example, this corpus could be used for experimentations in cross-lingual information retrieval, cross-lingual link discovery, or omni-lingual information retrieval research. Furthermore, the translated CJK articles could be used to further expand the current coverage of the English Wikipedia.  Information Storage and Retrieval Digital Libraries - collection.
      </response>
    </responses>
  </theme>
  <theme id="8" title="Theme 8">
    <words>
      <word weight="3.11011012213">
        awar
      </word>
      <word weight="2.90161126419">
        author
      </word>
      <word weight="2.80486245923">
        collabor
      </word>
      <word weight="1.81134766575">
        mechan
      </word>
      <word weight="1.41540999578">
        group
      </word>
      <word weight="1.17704203641">
        provid
      </word>
    </words>
    <responses>
      <response id="27" uni="Victoria University" weight="2.73" year="2004">
        Novel Group Awareness Mechanisms for Real-time Collaborative Document Authoring  Group awareness has become important in improving the usability of real-time, distributed, collaborative writing systems. However, the current set of implemented awareness mechanisms is insufficient in providing extensive and comprehensive awareness in collaborative document authoring. Certainly, current mechanisms, such as telepointers and multi-user scrollbars, have contributed in providing awareness support in collaborative authoring. Yet, given the shortcomings of these mechanisms and the difficulty in providing rich interaction found in face-to-face collaboration, much more support needs to be provided for group awareness during authoring. This research extends the pool of all known awareness mechanisms (including those that have been discovered before but have yet to be implemented). This research discovered several awareness mechanisms not found and reported elsewhere, through conducting usability experiments with a realtime cooperative editor. This paper covers three of the mechanisms-Structure-based Multi-page View, Point Jumping Mechanism and User Info List- discovered from the experiments. The paper also provides quantitative results supporting implementation of such mechanisms.  Group awareness, awareness mechanisms, real-time collaborative document authoring.
      </response>
      <response id="50" uni="Victoria University" weight="2.26" year="2006">
        Document-related Awareness Elements in Synchronous Collaborative Authoring  Simultaneous collaboration on documents by distributed authors has been supported by numerous synchronous collaborative authoring systems that are widely available. Originally, these tools were found to lack in providing rich enough interaction during authoring. As a result, group awareness in collaborative authoing arose as a very important issue in understanding how to provide comprehensive knowledge about other authors and activities they perform upon the document. To promote effectual authoring of documents simultaneously the best possible understanding of others' work on the document.  This paper reports results about document-related awareness elements from an empirical and experimental study of group awareness. Awareness elements reflect fundamental awareness information in supporting group awareness. Such results teach us what sort of document-related awareness should be provided for collaborative authoring.  Document management, collaborative document authoring, group awareness
      </response>
      <response id="40" uni="Victoria University" weight="1.32" year="2005">
        An Experimental Study of Workflow and Collaborative Document Authoring in Medical Research  Workflow is asynchronous technology widely used in the automation of organisational processes. Workflow provides benefits such as greater efficiency in an organisation, better worker productivity and greater process control. Synchronous collaborative authoring tools are technologies that allow a group of dispersed authors to write a document at the same time. These tools are beneficial in assisting authors to write some proportion, if not all of a document from an experiment.  This paper presents findings from an experiment combining both workflow and collaborative authoring tools workflow and collaborative authoring tools in a medical research environment. Studies investigating the combination of these tools are few, resulting in a lack of understanding of how this combination can effectively assist organisations in document-based processes. Overall, the combined workflow/collaborative authoring solution was found effective in the generation of a medical research paper.  Workflow, collaborative document authoring, medical research, experimental study
      </response>
      <response id="32" uni="University of Southern Queensland" weight="0.72" year="2004">
        Towards a new approach to tightly coupled document collaboration  Currently document collaboration typically proceeds using tools such as CVS or vendor-specific Computer Supported Collaborative Work (CSCW) and Electronic Meeting (EM) messaging systems. Both regulate essentially asynchronous loosely coupled collaboration. The prime disadvantages of these technologies are that often documents are checked out or distributed in their entirety and that human interaction is needed in case of unresolvable conflicts. On the side of tightly coupled distributed collaborative work, emerging XML databases are employing database-type concurrency control techniques, but unfortunately tend to lock entire documents preventing simultaneous updates. XML-enabled relational databases have the same intrinsic problems, leading to the question if another way is possible. In this speculative short paper we describe a novel approach toward tightly coupled document collaboration, involving database-style synchronous client-server collaboration tailored to semi-structured documents. It is partly based on previous theoretic results which introduced path locks to control concurrency on semi-structured data. We also describe how clients may use a future communication protocol based on the path locks.  Document and XML Databases, Document Management, Document Collaboration.
      </response>
      <response id="101" uni="University of Sydney" weight="0.38" year="2009">
        An Automatic Question Generation Tool for Supporting Sourcing and Integration in Students' Essays   This paper presents a domain independent Automatic Question Generation (AQG) tool that generates questions which can be used as a form of support for students to revise their essay. The focus here is on generating questions based on semantic and syntactic information acquired from citations. The semantic information includes the author's name, the citation type (describing the aim of the cited study, its results or an opinion), the author's expressed sentiment, and the syntactic information of the citation. Pedagogically, the question templates are designed using Bloom's learning taxonomy where the questions reach the Analysis Level. We used 40 undergraduate students essays for our experiment and the Name Entity Recognition component is trained on 20 essays. The result of our experiment shows that the question coverage is 96% and accuracy of generated questions can reach 78%. This AQG tool will be integrated into our peer review system to scaffold feedback from peers.  Question Generation, Electronic Feedback System for Sourcing and Integration in Students' Essay
      </response>
      <response id="16" uni="Macquarie University" weight="0.25" year="2002">
        How to Write a Document in Controlled Natural Language  This paper shows how a computer-processable document can be written in a controlled natural language (PENG) with the help of a sophisticated lookahead editor (ECOLE). The editor provides syntactic hints after each word form entered and indicates how the author can continue the text. This way the author does not need to learn or to remember the restrictions of the controlled language. PENG documents are automatically translated into first-order logic via discourse representation structures. These formal entities can be checked by a theorem prover for inconsistency or consistency can be revealed by a model builder.  Document Processing, Controlled Languages, Authoring Tools.
      </response>
      <response id="66" uni="RMIT University" weight="0.25" year="2007">
        Source Code Authorship Attribution using n-grams  Plagiarism and copyright infringement are major problems in academic and corporate environments. Existing solutions for detecting infringements in structured text such as source code are restricted to textual similarity comparisons of two pieces of work. In this paper, we examine authorship attribution as a means for tackling plagiarism detection. Given several samples of work from several authors, we attempt to correctly identify the author of work presented as a query. On a collection of 1 640 documents written by 100 authors, we show that we can attribute authorship in up to 67% of cases. This work can be a valuable additional indicator for the more difficult plagiarism investigations.  Authorship Attribution, Plagiarism Detection, Co-derivative Documents
      </response>
      <response id="64" uni="CSIRO ICT Centre, Australian National University" weight="0.20" year="2007">
        Does brandname influence perceived search result quality? Yahoo!, Google, and WebKumara  Improving the quality of search engine results is the goal of costly efforts by major Web search engine companies. Using in situ side-by-side result set comparisons and random assignment of brandnames to result sets, we investigated whether perceptions of quality were influenced by brand association. In the first experiment (15 searchers) we found no significant preference for or against results labelled 'Google' relative to those labelled 'Yahoo!'. In the second experiment (20 searchers) result sets were again generated by Google and Yahoo! but were randomly labelled 'Yahoo!' or 'WebKumara' (a fictitious name). Again, we found no significant preference for one brandname label over the other. Contrary to previous findings, we found a statistically significant preference for Googlegenerated results over those of Yahoo! when data from three separate experiments (total 70 subjects) was combined.  Information retrieval
      </response>
      <response id="143" uni="RMIT" weight="0.15" year="1997">
        Supporting the Answering Process  This paper is concerned with the way information access systerns support the question answering process. This process includes three stages: question formulation, information gathering, and analysis and synthesis. Standard information access technologies are mainly concerned with the second of these stages, providing little or no support for the last stage. However, the raw information gathered at this point can seldom be used directly as an answer. This paper discusses issues relating to the support of the analysis and synthesis stage, and suggests how information access systems might be extended to better support it. This paper also describes a WWW-based experimental interface that permits the evaluation of alternate ways of supporting this analysis and synthesis stage of the answering process. Information Retrieval, Question Answering, Passage Retrieval, Answer Presentation, Hypertext, WWW.
      </response>
      <response id="161" uni="France" weight="0.15" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="4" uni="University of Sydney" weight="0.14" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="56" uni="Macquarie University" weight="0.13" year="2006">
        Differentiating Document Type and Author Personality from Linguistic Features  There are many ways to profile a collection of documents. This paper presents highlight from a body of work that has looked at individual differences in the language of personal weblogs. Firstly, we present a unitary measure of linguistic contextuality based on POS frequency that can be used to profile and rank genres. When applied to weblogs, we will show they are similar to school essays, yet significantly less contextual than e-mail. We then look at individual variation of language, as due to the personality of the author, exploring the use of dictionary based analyses and data-driven n-grams. Under regression, we show that with just a few linguistic features, it is possible to explain significant proportions of variance within personality traits.  Personalised Documents; Multimedia Resource Discovery
      </response>
      <response id="135" uni="CSIRO" weight="0.13" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="164" uni="Griffith University" weight="0.12" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="14" uni="CRC for Enterprise Distributed Systems Technology (DSTC)" weight="0.09" year="2002">
        The Nexus information hub for exploring social-informational context  The Nexus system is an &quot;information hub&quot; that helps users collaboratively manage and organise &quot;contextualised social notifications&quot;. The purpose of the prototype is to act as a foundation for research into human information-sharing activity. The paper describes contextualised social notifications in more depth, links this prototype to the earlier Scuttlebutt prototype (presented at ADCS-6) and describes the architecture of the system and research questions.  Information Retrieval, Personalised Documents, Social Information Sharing
      </response>
      <response id="51" uni="University of Sydney" weight="0.08" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
    </responses>
  </theme>
  <theme id="9" title="Theme 9">
    <words>
      <word weight="3.62240311412">
        model
      </word>
      <word weight="2.73942940547">
        term
      </word>
      <word weight="2.15651649909">
        associ
      </word>
      <word weight="1.76842304609">
        expans
      </word>
      <word weight="1.71642038965">
        depend
      </word>
      <word weight="1.69228549376">
        relev
      </word>
    </words>
    <responses>
      <response id="130" uni="CSIRO, Queensland University of Technology" weight="3.42" year="2012">
        Is the Unigram Relevance Model Term Independent? Classifying Term Dependencies in Query Expansion  This paper develops a framework for classifying term dependencies in query expansion with respect to the role terms play in structural linguistic associations. The framework is used to classify and compare the query expansion terms produced by the unigram and positional relevance models. As the unigram relevance model does not explicitly model term dependencies in its estimation process it is often thought to ignore dependencies that exist between words in natural language. The framework presented in this paper is underpinned by two types of linguistic association, namely syntagmatic and paradigmatic associations. It was found that syntagmatic associations were a more prevalent form of linguistic association used in query expansion. Paradoxically, it was the unigram model that exhibited this association more than the positional relevance model. This surprising finding has two potential implications for information retrieval models: (1) if linguistic associations underpin query expansion, then a probabilistic term dependence assumption based on position is inadequate for capturing them; (2) the unigram relevance model captures more term dependency information than its underlying theoretical model suggests, so its normative position as a baseline that ignores term dependencies should perhaps be reviewed.   [Information Search and Retrieval]: Retrieval Models Algorithms, Experimentation, Theory Query expansion, relevance models, linguistic associations
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.56" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="2" uni="University of Sydney" weight="0.51" year="2002">
        Generating and Comparing Models within an Ontology An ontology is useful for providing a conceptually concise basis for developing and communicating knowledge. This paper discusses an application of an automatically constructed ontology of computer science and its use for comparison of models in the computer science domain. We present the architecture, algorithms and current results for MECUREO, a system which builds an extensive model of a computer science entity from limited information. The entity might be a document such as an email message, a course description document or a biography. It is intended to give a measure of the similarity of any two such models. We describe MECUREO's mechanism for constructing and representing models and its present performance in comparing models. Keywords information retrieval, ontologies, acquisition, sharing and reuse of conceptual structures
      </response>
      <response id="38" uni="RMIT University" weight="0.47" year="2005">
        Document Expansion versus Query Expansion for Ad-hoc Retrieval  In document information retrieval, the terminology given by a user may not match the terminology of a relevant document. Query expansion seeks to address this mismatch; it can significantly increase effectiveness, but is slow and resource-intensive. We investigate the use of document expansion as an alternative, in which documents are augmented with related terms extracted from the corpus during indexing, and the overheads at query time are small. We propose and explore a range of corpus-based document expansion techniques and compare them to corpus-based query expansion on TREC data. These experiments show that document expansion delivers at best limited benefits, while query expansion - including standard techniques and efficient approaches described in recent work - delivers consistent gains. We conclude that document expansion is unpromising, but it is likely that the efficiency of query expansion can be further improved.  Document expansion, automatic query expansion, pseudo relevance feedback, efficiency
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.43" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="147" uni="CSIRO, Division of Mathematical and Information Science, RMIT" weight="0.42" year="1998">
        Evaluation of Indexing Methods for Clustering  In order to synthesize a better answer based on the retrieved documents, we are exploring the use of clustering methods. In this paper, we present an evaluation of some popular indexing methods used for term selection and term weighting. The aim of indexing here is to represent documents, to relate documents with similar topics, and distinguish documents with different topics from each other. Experiments have been conducted to examine how the clustering results are influenced by some index term selection methods, such as the term selection based on the document frequency, and some term weighting methods, such as the inverted document frequency weight, the signal-noise ratio, and the term discrimination value, will influence the result of clustering. Based on our experiments, we recommend the use of the discrimination value weighting method together with a suitable set of indexing terms for the purpose of clustering the retrieved documents. Keywords Index, Term Selection, Term Weighting, Clustering.
      </response>
      <response id="131" uni="Queensland University of Technology" weight="0.36" year="2012">
        Pairwise Similarity of TopSig Document Signatures  This paper analyses the pairwise distances of signatures produced by the TopSig retrieval model on two document collections. The distribution of the distances are compared to purely random signatures. It explains why TopSig is only competitive with state of the art retrieval models at early precision. Only the local neighbourhood of the signatures is interpretable. We suggest this is a common property of vector space models.   [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval Models Signature Files, Topology, Vector Space IR, Random Indexing, Document Signatures, Search Engines, Document Clustering, Near Duplicate Detection, Relevance Feedback
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.33" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="42" uni="University of Sydney" weight="0.29" year="2005">
        Biomedical Named Entity Recognition System  We propose a machine learning approach, using a Maximum Entropy (ME) model to construct a Named Entity Recognition (NER) classifier to retrieve biomedical names from texts. In experiments, we utilize a blend of various linguistic features incorporated into the ME model to assign class labels and location within an entity sequence, and a postprocessing strategy for corrections to sequences of tags to produce a state of the art solution. The experimental results on the GENIA corpus achieved an F-score of 68.2% for semantic classification of 23 categories and achieved F-score of 78.1% on identification.  Named Entity Recognition, ME model, Information Retrieval.
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.29" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="35" uni="CSIRO ICT Centre" weight="0.28" year="2005">
        Document modelling for customised information delivery  As the amount of information available to people multiplies at an increasing speed, it becomes ever more important to deliver information customised to users' specific needs. Natural Language Generation systems coupled with user modeling techniques have been built to address this issue, to produce information that is relevant to the users . A common approach adopted by such systems is an approach based on planning, starting from a discourse (or communicative) goal, and planning the text to be presented to the users. However, these systems are not easy to build and difficult to change by domain experts. One of the problems is that it is hard to specify the plans employed, because they often require knowledge about writing, domain expertise, knowledge of computational linguistics and, finally, knowledge about how to obtain data from the underlying information sources. In this paper, we present our first step to address this problem.   Document modeling, information retrieval, document generation, personalized documents.
      </response>
      <response id="96" uni="RMIT University" weight="0.26" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.24" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="162" uni="University of Sydney" weight="0.24" year="2000">
        Keyword Association Network: A Statistical Multi-term Indexing Approach for Document Categorization  A Keyword Association Network (KAN) is the network of keywords extracted from a collection of documents. In this network, the relationship between keywords is represented by a confidence value. It is argued in this paper thai the semantics and importance of a word can be more clearly and accurately measured by making use of other words that are co-occurring in a given document. The term frequency used for measuring the importance of terms in most document categorization methods ignores this important aspect. A KAN is constructed on the basis of co-occurring terms in documents. If two tenns appear more than a certain number of times in the same documents, they are considered as having close relationship. This paper proposes using KAN as a basis for finding informative keywords and using a confidence value in the process of document categorization. The process of constructing and application of KAN for document categorization is presented and the performance comparison with a typical statistical single-term document categorization algorithm - TFIDF classifier - will be shown. The experimental results show that KAN gives significant benefits. Keywords Document Categorization, Machine Learning. Statistical Multi-term indexing, Semantic- Meaning.
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.23" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.21" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="140" uni="The University of Queensland, CRC for Distributed Systems Technology" weight="0.20" year="1997">
        Applying a Generic Conceptual Workflow Modeling Technique to Document Workflows  The workflow technology is emerging as an appropriate platform for the automated coordination of business activities. The documents represent the primary medium of business communication. Almost all kinds of business activities have some associated documents. The workflow management systems could be applied to coordinate the flow of business documents. However, before this automated document workflows could be implemented, we need to apply some conceptual modeling methodology to capture, analyse, and describe the role and flow of documents in a business process. In this paper, we present a generic conceptual workflow modeling technique that should be applicable to all kinds of workflows. The document workflows represent a specialized application of workflows. We identify basic characteristics of document workflows and discuss some issues that should be targeted during the modeling process. We also apply the proposed conceptual modeling technique to model an example document workflows application for handling postgraduate admission process of a university. conceptual modeling of workflows, document workflows.
      </response>
      <response id="146" uni="RMIT" weight="0.19" year="1997">
        Conflation-based Comparison of Stemming Algorithms  In text database systems, query terms are stemmed to allow them to be conflated with variant forms of the same word. On the one hand, stemming allows the query mechanism to find documents that would otherwise not contain matches to the query terms; on the other hand, automatic stemming is prone to error, and can lead to retrieval of inappropriate documents. In this paper we investigate several stemming algorithms, measuring their ability to correctly conflate terms from a large text collection. We show that stemming is indeed worthwhile, but that each of the stemming algorithms we consider has distinct advantages and disadvantages; choice of stemming algorithm affects the behaviour of the retrieval mechanism. information retrieval, document databases, digital libraries, word disambiguation.
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.19" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="103" uni="Queensland University of Technology" weight="0.18" year="2009">
        Investigating the use of Association Rules in Improving Recommender Systems  Recommender systems are widely used online to help users find other products, items etc that they may be interested in based on what is known about that user in their profile. Often however user profiles may be short on information and thus when there is not sufficient knowledge on a user it is difficult for a recommender system to make quality recommendations. This problem is often referred to as the cold-start problem. Here we investigate whether association rules can be used as a source of information to expand a user profile and thus avoid this problem, leading to improved recommendations to users. Our pilot study shows that indeed it is possible to use association rules to improve the performance of a recommender system. This we believe can lead to further work in utilising appropriate association rules to lessen the impact of the cold-start problem.  Information Retrieval, Personalised Documents, Recommender Systems, Association Rules.
      </response>
      <response id="138" uni="National Taiwan University" weight="0.17" year="1997">
        A Multi-party Document Model  A multi-party document (MPD) is one that contains multiple parts intended for multiple recipients in that each recipient does not necessarily need to know the existence, and therefore the corresponding content, of the other parts of the document intended for other recipients. MPDs pose new requirements, namely, transparency and security, to the underlying system. This paper reviews the traditional document models and their shortcomings with respect to the stated requirements and proposes a new model that satisfies those requirements. The major implementation issues including document construction and encryption for MPD supports are also discussed. Shifting these capabilities to the system level will relieve burdens of users and enhance the security measure of the system for protecting the integrity and privacy of document contents. The issues discussed in this paper are important for making a sound underlying support system for the development of information systems, in particular in the Intranets environment. Keywords Document management, document structure, security, encryption, Intranets.
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.14" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.12" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="0.12" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.11" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="63" uni="Queensland University of Technology" weight="0.11" year="2007">
        Integration of Information Filtering and Data Mining Process for Web Information Retrieval  This paper examines a new approach to Web information retrieval, and proposes a new two stage scheme. The aim of the first stage is to quickly filter irrelevant information based on the user profiles. The proposed user profiles learning algorithm are very efficient and effective within a relevance feedback framework. The aim of the second stage is to apply data mining techniques to rationalize the data relevance on the reduced data set. Our experiments on RCV1 (Reuters Corpus Volume 1) data collection which is used by TREC in 2002 for filtering track show that more effective and efficient access Web information has been achieved by combining the strength of information filtering and data mining method.  Information filtering, User profiles, Data mining, Pattern taxonomic model
      </response>
      <response id="88" uni="University of Melbourne" weight="0.10" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="78" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.10" year="2008">
        Term-Frequency Surrogates in Text Similarity Computations  Inverted indexes on external storage perform best when accesses are ordered and data is read sequentially, so that seek times are minimized. As a consequence, the various items required to compute Boolean, ranked and phrase queries are often interleaved in the inverted lists. While suitable for query types in which all items are required, this arrangement has the drawback that other query types - notably pure ranked queries and conjunctive Boolean queries - do not require access to word position information, and that component of each posting must be bypassed when these queries are being handled. In this paper we show that the term frequency component of each posting can be completely replaced by a surrogate that allows skipping of positional information interleaved in inverted lists, and obtain significant speedups in ranked query execution without increasing the index size, and without harming retrieval effectiveness. We also explore two methods of reconstituting approximations to the original term frequencies that can be employed if use of the surrogates is deemed too risky. Our simple improvement can thus be used with all ranking functions that make use of term frequencies.  Information retrieval, inverted index, skip pointer, proximity query, efficiency, effectiveness.
      </response>
      <response id="154" uni="University of Sydney" weight="0.09" year="1999">
        Building rich metadata from critical reviews for a scrutable filtering system  We describe the Review Coder system for creating rich metadata for a scrutable filtering system. A scrutable system maintains explanations of the data and processes that drove the system operation. In the current paper we use Review Coder as part of a filtering systems for movies: the scrutability of the system means that a user can determine why the system recommended a particular movie or not. The filtering process is based upon movie reviews and metadata built in association with them. These provide high quality information about the movie objects. From these, the filtering system is intended to build stereotypic models of reviewer's preferences for movies. These can drive the filtering process and the user can scrutinise both these models and the actual reviews which were used to construct them. Keywords: Multimedia Resource Discovery, Multimedia Filtering, Scrutable Filtering, Extraction of Metadata
      </response>
      <response id="11" uni="The University of Queensland" weight="0.08" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
    </responses>
  </theme>
  <theme id="10" title="Theme 10">
    <words>
      <word weight="3.96217109228">
        watermark
      </word>
      <word weight="3.38148107821">
        bit
      </word>
      <word weight="2.64223020574">
        imag
      </word>
      <word weight="1.97737715344">
        rate
      </word>
      <word weight="1.9216956462">
        inform
      </word>
      <word weight="1.18825213009">
        domain
      </word>
    </words>
    <responses>
      <response id="158" uni="University of New England" weight="3.39" year="1999">
        High Bit rate Wavelet Domain Digital Watermarking of Images and Compression Tolerance  The increased commercial activity in internet and media industry demands protection of media such as images, video and audio against unauthorised processing and use. Watermarking is a technique to hide information in media such that the hidden information (watermark) is invisible. This hidden information can be a small sequence of bits resulting in low bit rate watermarking. In low bit rate watermarking, each information bit is represented by an invisible broadband signal when added to the image. The hidden information may be in the form of images in which case large number of bits must be embedded. Such watermarks are known as high bit rate watermarks and are usually binary images. Unlike low bit rate watermarking where each information bit is transformed to a broadband signal by generating a suitable pseudo-random sequence, high bit rate watermarking demands some other means of embedding and detection due to large number of information bits to be embedded. We use a binary feature based watermarking technique on wavelet domain. Our work is inspired by the work in [2]. Our motivation for this research is twofold. Firstly, we embed the watermark in wavelet domain rather than DCT domain motivated by the fact that the wavelet transform is used in jpeg2000 standardisation process. Secondly, high bit rate watermarks tend to get destroyed, even before average lossless compression ratios between 20% to 32%. We wanted to get the watermark breakup point up to at least within 20% to 32%. Watermarking, image compression, image authentication.
      </response>
      <response id="161" uni="France" weight="0.38" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="141" uni="Monash University" weight="0.26" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="114" uni="RMIT University" weight="0.25" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="150" uni="Dublin City University" weight="0.25" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="77" uni="RMIT University" weight="0.25" year="2007">
        Querying Image Ontology  Content-based image retrieval has been used in various application domains, but the semantic gap problem remains a challenge to be overcome. One possible way to overcome this problem is to represent the knowledge extracted from the low-level image features through semantic concepts. In this paper we describe how we use an image ontology to this end. We show that we are able to retrieve desired images by using basic ontology queries.  Ontology, CBIR, semantic gap
      </response>
      <response id="159" uni="University of Ballarat, La Trobe University" weight="0.21" year="1999">
        The use of argumentation to assist in the generation of legal documents  Many text documents in the legal domain are created in order to express the reasoning steps a decision maker followed in reaching conclusions. For example, refugee law determinations are documents that express the reasoning steps a member of the Refugee Review Tribunal in Australia followed in order to infer conclusions regarding the status of an applicant. Although, it is reasonable to expect that a mapping between the reasoning steps used by a decision maker and the structure of the document produced would clearly be apparent, a number of authors have discovered that such a mapping is by no means obvious. In order to develop legal knowledge based systems that generate documents from their own reasoning steps, discourse analysis is invoked to bridge the gap and perform the mapping. In this paper, we articulate a heuristic that we use to generate a plausible document structure without the use of discourse analysis. Without discourse analysis, the heuristic cannot contribute to our understanding of the process employed by decision makers to convert reasoning to text. Nevertheless, the heuristic can mimic the process. The heuristic has been trialed with a small sample of refugee law determinations by extracting the reasoning steps from each determination and applying the heuristic to reproduce each document's structure. Figure 11: The watermarked lena image Keywords refugee law.
      </response>
      <response id="137" uni="The Boeing Company" weight="0.13" year="1997">
        Analyzing Image Content for a Large Scale Hypermedia System  The Boeing Company maintains tens of millions of pages of information associated with the manufacture and delivery of its products. Much of this information must be made available electronically. We have developed tools to automatically convert and integrate electronic data into industry standard formats. Some of the technical challenges include I) handling a wide variety of source formats, 2) making sure that the tools scale up to handle millions of pages of information, and 3) adding functionality to graphics. Our system contains over four million pages of text including tens of thousands of graphics. In this paper we describe tools that recognize and use information within airplane-related vector and raster images. Such images include troubleshooting charts, fault reporting diagrams, component location diagrams, component index tables, wiring diagrams, system schematics, parts illustrations, standards tables, and structural and tooling drawings. Each airplane requires conversion of over 20,000 graphics including over 900,000 pieces of cross-referenced information. We are also exploring visual information retrieval strategies, including content-based and similarity-based methods for both vector and raster graphics. information retrieval, hypermedia
      </response>
      <response id="164" uni="Griffith University" weight="0.11" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="0.08" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
    </responses>
  </theme>
  <theme id="11" title="Theme 11">
    <words>
      <word weight="5.69296829551">
        system
      </word>
      <word weight="1.34438062084">
        score
      </word>
      <word weight="1.23081573579">
        evalu
      </word>
      <word weight="1.04356903546">
        measur
      </word>
      <word weight="0.882360712739">
        review
      </word>
      <word weight="0.821167389072">
        filter
      </word>
    </words>
    <responses>
      <response id="154" uni="University of Sydney" weight="1.63" year="1999">
        Building rich metadata from critical reviews for a scrutable filtering system  We describe the Review Coder system for creating rich metadata for a scrutable filtering system. A scrutable system maintains explanations of the data and processes that drove the system operation. In the current paper we use Review Coder as part of a filtering systems for movies: the scrutability of the system means that a user can determine why the system recommended a particular movie or not. The filtering process is based upon movie reviews and metadata built in association with them. These provide high quality information about the movie objects. From these, the filtering system is intended to build stereotypic models of reviewer's preferences for movies. These can drive the filtering process and the user can scrutinise both these models and the actual reviews which were used to construct them. Keywords: Multimedia Resource Discovery, Multimedia Filtering, Scrutable Filtering, Extraction of Metadata
      </response>
      <response id="96" uni="RMIT University" weight="1.29" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="107" uni="The University of Melbourne, University of Malaya" weight="1.22" year="2010">
        Estimating System Effectiveness ScoresWith Incomplete Evidence  It is common for only partial relevance judgments to be used when comparing retrieval system effectiveness, in order to control experimental cost. Using TREC data, we consider the uncertainty introduced into per-topic effectiveness scores by pooled judgments, and measure the effect that incomplete evidence has on both the systems scores that are generated, and also on the quality of paired system comparisons. We measure system behavior from three different points of view: the trend in effectiveness scores; the separability of system pairs; and the number of reversals in significance outcomes as the depth of judgments increases. Our results show that when shallow pooled judgments are used system separability remains relatively high, but that there is also a high rate of significance reversal. We then show that explicitly adjusting effectiveness scores to allow for the known amount of uncertainty gives a reduced number of reversals, and hence more consistent experimental outcomes.  Retrieval evaluation, effectiveness metric, pooling
      </response>
      <response id="75" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="1.19" year="2007">
        Score Standardization for Robust Comparison of Retrieval Systems  Information retrieval systems are evaluated by applying them to standard test collections of documents, topics, and relevance judgements. An evaluation metric is then used to score a system's output for each topic; these scores are averaged to obtain an overall measure of effectiveness. However, different topics have differing degrees of difficulty and differing variability in scores, leading to inconsistent contributions to aggregate system scores and problems in comparing scores between different test collections. In this paper, we propose that per-topic scores be standardized on the observed score distributions of the runs submitted to the original experiment from which the test collection was created. We demonstrate that standardization equalizes topic contributions to system effectiveness scores and improves inter-collection comparability.  Retrieval system evaluation, average precision, standardization.
      </response>
      <response id="87" uni="NICTA Victoria Laboratory, The University of Melbourne" weight="0.86" year="">
        Parameter Sensitivity in Rank-Biased Precision  Rank-Biased Precision (RBP) is a retrieval evaluation metric that assigns an effectiveness score to a ranking by computing a geometricly weighted sum of document relevance values, with the monotonicly decreasing weights in the geometric distribution determined via a persistence parameter p. Despite exhibiting various advantageous traits over well known existing measures such as Average Precision, RBP has the drawback of requiring the designer of any experiment to choose a value for p. Here we present a method that allows retrieval systems evaluated using RBP with different p values to be compared. The proposed approach involves calculating two critical bounding relevance vectors for the original RBP score, and using those vectors to calculate the range of possible RBP scores for any other value of p. Those bounds may then be sufficient to allow the outright superiority of one system over the other to be established. In addition, the process can be modified to handle any RBP residuals associated with either of the two systems. We believe the adoption of the comparison process described in this paper will greatly aid the uptake of RBP in evaluation experiments.  Rank-Biased Precision, Evaluation, System Comparison
      </response>
      <response id="138" uni="National Taiwan University" weight="0.75" year="1997">
        A Multi-party Document Model  A multi-party document (MPD) is one that contains multiple parts intended for multiple recipients in that each recipient does not necessarily need to know the existence, and therefore the corresponding content, of the other parts of the document intended for other recipients. MPDs pose new requirements, namely, transparency and security, to the underlying system. This paper reviews the traditional document models and their shortcomings with respect to the stated requirements and proposes a new model that satisfies those requirements. The major implementation issues including document construction and encryption for MPD supports are also discussed. Shifting these capabilities to the system level will relieve burdens of users and enhance the security measure of the system for protecting the integrity and privacy of document contents. The issues discussed in this paper are important for making a sound underlying support system for the development of information systems, in particular in the Intranets environment. Keywords Document management, document structure, security, encryption, Intranets.
      </response>
      <response id="160" uni="DSTO, Lloyd-Jones Consulting" weight="0.65" year="1999">
        Automatic document metadata extraction and manipulation: a working system for the Intelligence Analyst  This paper discusses the design and implementation of an operational system to aid health intelligence analysts. The HINTS system provides automated support to undertake tasks such as specific health related research and report writing in the face on an ever-growing body of electronic information, available on the web, and on local file systems. Our approach is to provide automated support for document analysis and discovery from technologies that support ad-hoc searching, consistent filtering for specific pieces of information such as hospital facilities, diseases and locations, and that provide document summarisation and keywording. Document metadata is stored in XML in a data structure that allows a variety of searches and views of the document space to be performed. The user interfaces to the system by web browser and a map-based geospatial application. Document Analysis, Document Databases Information Retrieval, XML, Information Extraction
      </response>
      <response id="74" uni="RMIT University" weight="0.62" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="72" uni="RMIT University" weight="0.61" year="2007">
        Predicting Query Performance for User-based Search Tasks  Query performance prediction aims to determine in advance whether a user's search request will return a useful answer set. The success of such prediction attempts are currently evaluated by calculating the correlation between the predicted performance and standard information retrieval metrics of system performance such as average precision. However, recent work suggests that there is little relationship between average precision and the performance of users when carrying out search tasks. Direct measures of user performance offer another way of evaluating the effectiveness of search systems; this is of particular importance in the framework of query prediction, since one of the goals of prediction is to warn users when search results are likely to be poor. We therefore investigate the relationship between current prediction techniques and user-based performance measures. Our preliminary results show that the performance of the predictors differs strongly when using system-based compared to user-based performance measures: predictors that are significantly correlated with one measurement are often not correlated with the other. In general, the predictors are more correlated with average precision rather than with user performance.  Query performance prediction, information retrieval, user study
      </response>
      <response id="79" uni="The University of Sydney" weight="0.60" year="2008">
        MetaView: Dynamic metadata based views of user files  Hierarchical file systems are the most common way of organising large collections of documents. However, there are several desirable features they do lack. These include: good support for placing files in multiple locations; dynamic views on the users' data; and explicit ordering of files. This paper introduces MetaView, a new approach to enhancing file systems so that they can present users with a fluid and dynamic view of their files based on metadata. MetaView allows users to describe how they wish to view their files by specifying an organisational structure based on a metadata path. Experiments indicate that this approach is viable for collections of up to several thousand files in size, enabling flexible organisation of substantial parts of a user's file system.  Document Management, Metadata, Nonhierarchical file system.
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.57" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.55" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="117" uni="Queensland University of Technology, CSIRO" weight="0.52" year="2010">
        Rule-based Approach for Identifying Assertions in Clinical Free-Text Data  A rule-based approach for classifying previously identified medical concepts in the clinical free text into an assertion category is presented.There are six different categories of assertions for the task: Present, Absent, Possible, Conditional, Hypothetical and Not associated with the patient. The assertion classification algorithms were largely based on extending the popular NegEx and Context algorithms. In addition, a health based clinical terminology called SNOMED CT and other publicly available dictionaries were used to classify assertions, which did not fit the NegEx/Context model. The data for this task includes discharge summaries from Partners HealthCare and from Beth Israel Deaconess Medical Centre, as well as discharge summaries and progress notes from University of Pittsburgh Medical Centre. The set consists of 349 discharge reports, each with pairs of ground truth concept and assertion files for system development, and 477 reports for evaluation. The system's performance on the evaluation data set was 0.83, 0.83 and 0.83 for recall, precision and F1-measure, respectively. Although the rule-based system shows promise, further improvements can be made by incorporating machine learning approaches.  rule-based, medical concept, assertion, NegEx, Context, SNOMED CT.
      </response>
      <response id="15" uni="University of Sydney" weight="0.48" year="2002">
        Liquid Miro: Semantic Softlinking to Support Cooperative Document Exploration  In  this  position  paper  we  present  a  non-intrusive mechanisim  for  evolving  the  overall  quality  of semantic  relationships  between  elements  of information  in  hypermedia  document  systems.  The evolutionary  aspect  of  this  work  is  an  application framework  that  includes a  combination of hard  links and temporal soft links between existing documents. A hard  link  completes  the  binding  between  two documents in the system upon creation, wheras a soft link  delays  the  binding  until  some  later  time.  The ablity  to monitor, weight,  integrate,  delay,  and  then order the soft links is what offers the power in Liquid Miro document systems. Here we focus on  the use of hypermedia document systems which support existing online  communities,  ranging  from  social  to professional groups..  Personalised  Documents,  Document Management
      </response>
      <response id="103" uni="Queensland University of Technology" weight="0.47" year="2009">
        Investigating the use of Association Rules in Improving Recommender Systems  Recommender systems are widely used online to help users find other products, items etc that they may be interested in based on what is known about that user in their profile. Often however user profiles may be short on information and thus when there is not sufficient knowledge on a user it is difficult for a recommender system to make quality recommendations. This problem is often referred to as the cold-start problem. Here we investigate whether association rules can be used as a source of information to expand a user profile and thus avoid this problem, leading to improved recommendations to users. Our pilot study shows that indeed it is possible to use association rules to improve the performance of a recommender system. This we believe can lead to further work in utilising appropriate association rules to lessen the impact of the cold-start problem.  Information Retrieval, Personalised Documents, Recommender Systems, Association Rules.
      </response>
      <response id="39" uni="University of Wollongong" weight="0.44" year="2005">
        ePOC: Mobile Clinical Information Access and Diffusion in Ambulatory Care Service Settings  This paper represents a preliminary overview (work-in-progress) of a mobile e-Health research and development project and the intrinsic considerations which arise when designing such patient data management systems tailored to ambulatory care. Its purpose is to give an outline of the issues that allow technological enablement of electronic patient data management in the delivery of home-based medical care. While the replacement of more traditional paper-based patient data management using Personal Digital Assistants as a collection platform is technically straightforward, the organizational realignment of an electronic document management system requires careful study and deployment in order to maximize success. We outline the methodological considerations for document management diffusion within this e-Health setting and describe the issues, architecture and proposed rollout of an electronic Point-Of-Care (ePOC) system.  e-Health, document management and workflow, information access and diffusion
      </response>
      <response id="33" uni="University of Southern Queensland" weight="0.41" year="2004">
        GOOD Publishing System: Generic Online/Offline Delivery    GOOD is a tailor-made, fully integrated publishing system that creates output documents for multiple media types used in both online and offline teaching modes at the University of Southern Queensland. It is used in the Distance and e-Learning Centre of USQ to create course material for thousands of on-campus, online and external students. Among the end products generated from a single XML input document containing study material for a specific course are study books, introductory books, and web sites in a variety of formats. Future end products currently being investigated include voice rendering. The GOOD system is entirely based on open standards such as XML, XSLT, DOM, and XSL:FO and implemented with JAVA/J2EE technology. Among its features is a smart editing client to allow technically non-proficient staff to edit their own course material.  Document Management and Publishing, XML authoring.
      </response>
      <response id="137" uni="The Boeing Company" weight="0.40" year="1997">
        Analyzing Image Content for a Large Scale Hypermedia System  The Boeing Company maintains tens of millions of pages of information associated with the manufacture and delivery of its products. Much of this information must be made available electronically. We have developed tools to automatically convert and integrate electronic data into industry standard formats. Some of the technical challenges include I) handling a wide variety of source formats, 2) making sure that the tools scale up to handle millions of pages of information, and 3) adding functionality to graphics. Our system contains over four million pages of text including tens of thousands of graphics. In this paper we describe tools that recognize and use information within airplane-related vector and raster images. Such images include troubleshooting charts, fault reporting diagrams, component location diagrams, component index tables, wiring diagrams, system schematics, parts illustrations, standards tables, and structural and tooling drawings. Each airplane requires conversion of over 20,000 graphics including over 900,000 pieces of cross-referenced information. We are also exploring visual information retrieval strategies, including content-based and similarity-based methods for both vector and raster graphics. information retrieval, hypermedia
      </response>
      <response id="104" uni="Queensland University of Technology, University of Otago" weight="0.40" year="2009">
        The Methodology of Manual Assessment in the Evaluation of Link Discovery  The link graph extracted from the Wikipedia has often been used as the ground truth for measuring the performance of automated link discovery systems. Extensive manual assessments experiments at INEX 2008 recently showed that this is unsound and that manual assessment is essential. This paper describes the methodology for link discovery evaluation which was developed for use in the INEX 2009 Link-the-Wiki track. In this approach both manual and automatic assessment sets are generated and runs are evaluated using both. The approach offers a more reliable evaluation of link discovery methods than just automatic assessment. A new evaluation measure for focused link discovery is also introduced.  Wikipedia, Link Quality, Manual Assessment, Evaluation.
      </response>
      <response id="116" uni="CSIRO, Queensland University of Technology" weight="0.37" year="2010">
        Analysis of the effect of negation on information retrieval of medical data  Most information retrieval (IR) models treat the presence of a term within a document as an indication that the document is somehow 'about' that term, they do not take into account when a term might be explicitly negated. Medical data, by its nature, contains a high frequency of negated terms - e.g. 'review of systems showed no chest pain or shortness of breath'. This papers presents a study of the effects of negation on information retrieval. We present a number of experiments to determine whether negation has a significant negative effect on IR performance and whether language models that take negation into account might improve performance. We use a collection of real medical records as our test corpus. Our findings are that negation has some effect on system performance, but this will likely be confined to domains such as medical data where negation is prevalent. Keywords Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="69" uni="CSIRO ICT Centre" weight="0.34" year="2007">
        Document Composition and Content Selection Evaluation  Our work is concerned with the design of adaptive hypertext systems that produce documents tailored to their intended reader. In our approach, a system composes document on-the-fly, assembling existing text fragments. One of our challenges in this approach is to support the technical writer who configures the system. The task of the technical writer is to specify the structure of the documents to be generated, together with their applicability conditions. To perform their task, authors need to know what information is available. In this paper, we examine the impact of different strategies for presenting the existing text fragments on the task of document composition. We focus in particular on the impact on the quality of the resulting documents. We found that people compose better documents when existing text fragments are presented in a structured way.  document composition, information reuse, document quality, evaluation, method.
      </response>
      <response id="12" uni="The University of Queensland" weight="0.33" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="35" uni="CSIRO ICT Centre" weight="0.33" year="2005">
        Document modelling for customised information delivery  As the amount of information available to people multiplies at an increasing speed, it becomes ever more important to deliver information customised to users' specific needs. Natural Language Generation systems coupled with user modeling techniques have been built to address this issue, to produce information that is relevant to the users . A common approach adopted by such systems is an approach based on planning, starting from a discourse (or communicative) goal, and planning the text to be presented to the users. However, these systems are not easy to build and difficult to change by domain experts. One of the problems is that it is hard to specify the plans employed, because they often require knowledge about writing, domain expertise, knowledge of computational linguistics and, finally, knowledge about how to obtain data from the underlying information sources. In this paper, we present our first step to address this problem.   Document modeling, information retrieval, document generation, personalized documents.
      </response>
      <response id="36" uni="RMIT" weight="0.32" year="2005">
        Readability of French as a Foreign Language and its Uses  Reading is an important means of foreign language acquisition, particularly for vocabulary. Providing reading material that is of a suitable level of difficulty allows users to acquire vocabulary the most efficiently. Thus an on-line reading material recommender system for language learners requires a readability measure so that the difficulty of texts can be automatically assessed. However, most readability measures were developed for native child speakers of English. In this article I discuss an experiment in readability for learners of French. I conclude that using the average number of words per sentence correlates more closely with human judgements than many commonly available readability measures. I propose a new readability measure for learners of French that have English as their main language, which combines sentence length with the number of words that are similar in both languages (cognates). This measure slightly improves on sentence length for modelling French readability.  Text readability, Information retrieval
      </response>
      <response id="101" uni="University of Sydney" weight="0.30" year="2009">
        An Automatic Question Generation Tool for Supporting Sourcing and Integration in Students' Essays   This paper presents a domain independent Automatic Question Generation (AQG) tool that generates questions which can be used as a form of support for students to revise their essay. The focus here is on generating questions based on semantic and syntactic information acquired from citations. The semantic information includes the author's name, the citation type (describing the aim of the cited study, its results or an opinion), the author's expressed sentiment, and the syntactic information of the citation. Pedagogically, the question templates are designed using Bloom's learning taxonomy where the questions reach the Analysis Level. We used 40 undergraduate students essays for our experiment and the Name Entity Recognition component is trained on 20 essays. The result of our experiment shows that the question coverage is 96% and accuracy of generated questions can reach 78%. This AQG tool will be integrated into our peer review system to scaffold feedback from peers.  Question Generation, Electronic Feedback System for Sourcing and Integration in Students' Essay
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.30" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="18" uni="University of South Australia" weight="0.28" year="2002">
        Towards a System to Improve Administrative Processes for Front-Line Academic Staff  Current  economic  pressures  are  causing  severe problems for many enterprises in maintaining service standards  with  shrinking  headcounts.  Groupware, Workflow  and  Agent  technologies  have  been  widely advocated  as  a  solution,  but  there  are  few  reported success  stories.  The  project  described  in  this  paper addresses  the  case  of  running  large  undergraduate courses. A preliminary vision of a possible integrated administrative  support  system  is  presented,  and  the future  activities  necessary  to  advance  such  a  vision are outlined.  Administrative  Applications,  User Interface Design, Software Agents
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.27" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="149" uni="CSIRO Mathematical and Information Sciences" weight="0.26" year="1998">
        Automatic Document Creation from Software Specifications  Software documentation, and in particular, on-line help is a crucial aspect of a software system. Producing and maintaining it, however, is both labor intensive and tedious, making it a candidate for automation. This paper presents our work on automatically generating hypertext based on-line help, starting from software specifications. Our approach is motivated by practical considerations, such as the impossibility to construct by hand the semantic knowledge base typically required by a generation system. Natural Language Generation, Software documentation, hypertext
      </response>
      <response id="82" uni="RMIT University, University of Technology Dresden" weight="0.25" year="2008">
        WebKnox: Web Knowledge Extraction  The paper describes and evaluates a system for extracting knowledge from the web that uses a domain independent fact extraction approach and a self supervised learning algorithm. Using a trust algorithm, the precision of the system is improved to over 70% compared with a baseline of 52%.  Information Extraction, Web Mining
      </response>
      <response id="2" uni="University of Sydney" weight="0.24" year="2002">
        Generating and Comparing Models within an Ontology An ontology is useful for providing a conceptually concise basis for developing and communicating knowledge. This paper discusses an application of an automatically constructed ontology of computer science and its use for comparison of models in the computer science domain. We present the architecture, algorithms and current results for MECUREO, a system which builds an extensive model of a computer science entity from limited information. The entity might be a document such as an email message, a course description document or a biography. It is intended to give a measure of the similarity of any two such models. We describe MECUREO's mechanism for constructing and representing models and its present performance in comparing models. Keywords information retrieval, ontologies, acquisition, sharing and reuse of conceptual structures
      </response>
      <response id="120" uni="CSIRO, Australian National University, University of Applied Science Technikum Wien, Funnelback" weight="0.24" year="2011">
        Automatic identification of the most important elements in an XML collection  An important problem in XML retrieval is determining the most useful element types to retrieve - e.g. book, chapter, section, paragraph or caption. An automated system for doing this could be based on features of element types related to size, depth, frequency of occurrence, etc. We consider a large number of such features and assess their usefulness in predicting the types of elements judged relevant in INEX evaluations for the IEEE and Wikipedia 2006 corpora. For each feature we automatically assign Useful / Not-Useful labels to element types using Fuzzy c-Means Clustering. We then rank the features by the accuracy with which they predict the manual judgments. We find strong overlap between the top-ten most predictive features for the two collections and that seven features achieve high average accuracy (F-measure &gt; 65%) acrosss them. We hypothesize that an XML retrieval system working on an unlabelled corpus could use these features to decide which retrieval units are most appropriate to return to the user.  XML Retrieval, Fuzzy C-Means Clustering, F-Measure.
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.22" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="14" uni="CRC for Enterprise Distributed Systems Technology (DSTC)" weight="0.21" year="2002">
        The Nexus information hub for exploring social-informational context  The Nexus system is an &quot;information hub&quot; that helps users collaboratively manage and organise &quot;contextualised social notifications&quot;. The purpose of the prototype is to act as a foundation for research into human information-sharing activity. The paper describes contextualised social notifications in more depth, links this prototype to the earlier Scuttlebutt prototype (presented at ADCS-6) and describes the architecture of the system and research questions.  Information Retrieval, Personalised Documents, Social Information Sharing
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.21" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="133" uni="Funnelback" weight="0.19" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval - Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.17" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="44" uni="University of Otago" weight="0.16" year="2005">
        Recommending Geocaches  Players downloading GPS coordinates from the internet, hiking to the given spot, and hunting for a hidden box - this is the new sport of geocaching. Today there are nearly 200,000 such boxes in over 200 countries. With so many to find, a recommender is needed, one that takes into account not only the boxes, but also the geospatial and temporal nature of the sport. A database of geocaches in the South Island of New Zealand is made by trawling a prominent geocaching web site. This is then used to estimate the home-coordinates (geospatial playing centre) of players. Predictions are verified against a set of correct coordinates solicited from players. Several geocache recommenders are discussed and compared. The precision, computed using mean of mean reciprocal rank (MMRR), of each is measured. The best method tried is a collaborative filter using intersection over mean to find similar players and a voting scheme to recommend geocaches. This method is proposed as a replacement for the currently used distance from home-coordinate; doing so will increase the precision of existing systems such as geocaching. com.  Information Retrieval.
      </response>
      <response id="1" uni="University of Queensland" weight="0.16" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="125" uni="RMIT" weight="0.16" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.16" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.15" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="143" uni="RMIT" weight="0.14" year="1997">
        Supporting the Answering Process  This paper is concerned with the way information access systerns support the question answering process. This process includes three stages: question formulation, information gathering, and analysis and synthesis. Standard information access technologies are mainly concerned with the second of these stages, providing little or no support for the last stage. However, the raw information gathered at this point can seldom be used directly as an answer. This paper discusses issues relating to the support of the analysis and synthesis stage, and suggests how information access systems might be extended to better support it. This paper also describes a WWW-based experimental interface that permits the evaluation of alternate ways of supporting this analysis and synthesis stage of the answering process. Information Retrieval, Question Answering, Passage Retrieval, Answer Presentation, Hypertext, WWW.
      </response>
      <response id="148" uni="Macquarie University, CSIRO Mathematical and Information Sciences" weight="0.13" year="1998">
        Using Natural Language Generation Techniques to Produce Virtual Documents  With the increasing importance of Web publishing, there has been considerable interest in the production of virtual documents on demand. The bulk of this work has used existing documents annotated with meta-data as a source. We suggest that more flexibility and functionality can be obtained if virtual documents are generated instead from raw data. This capability can be achieved by using natural language generation techniques. In this paper, we describe a project concerned with automatically generating natural language descriptions of museum artefacts directly from a museum's Collection Information System. Natural Language Generation, Databases
      </response>
      <response id="164" uni="Griffith University" weight="0.13" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="13" uni="University of Sydney" weight="0.12" year="2002">
        Visualisation of Document and Concept Spaces  Collections of documents with conceptual relationships exist in many domains. Teaching systems often contain numerous learning resource documents. University policies are often large collections of related documents. The visualisation of the structure of these collections can be useful as it allows the exploration of the collection. This paper describes a graphical interface for visualising document spaces. The interface makes it simple for the user to explore the documents and the relationships between them. metadata, ITS, ontology extraction, user modelling, visualisation
      </response>
      <response id="92" uni="University of Otago" weight="0.12" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.12" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.12" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="153" uni="Mathematical &amp; Information Sciences CSIRO" weight="0.11" year="1999">
        TML: A Thesaural Markup Language  Thesauri are used to provide controlled vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular thesaurus classifies documents according to an information community's needs. As a result, there are many different thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Electronic Documents, Metadata, Ontology, Thesaurus, XML
      </response>
      <response id="41" uni="The University of Queensland, The University of Wollongong" weight="0.11" year="2005">
        Applying Formal Concept Analysis to Semantic File Systems Leveraging Wordnet  Formal Concept Analysis can be used to obtain both a natural clustering of documents along with a partial ordering over those clusters. The application of Formal Concept Analysis requires input to be in the form of a binary relation between two sets. This paper investigates how a semantic filesystem can be used to generate such binary relations. The manner in which the binary relation is generated impacts how useful the result of Formal Concept Analysis will be for navigating one's filesystem.  Document Databases, Document Management
      </response>
      <response id="59" uni="Queensland Unversity of Technology, The Pennsylvania State University" weight="0.11" year="2007">
        Multimedia Web Searching on a Meta-Search Engine  This paper provides preliminary results from a major study of multimedia Web searching by Dogpile meta-search engine users, including queries and session characteristics, and changes or differences in image, video and audio searching. The resutls are compares with multimedia Web searching studies from 1997 to 2002. Image and sexual queries are dominant in multimedia We searching. The paper provides important implications for the design of multimedia information retrieval systems.
      </response>
      <response id="21" uni="The University of Sydney" weight="0.10" year="2002">
        A Framework for Text Categorization In this paper we discuss the architecture of an object-oriented application framework (OOAF) for text categorization. We describe the system requirements and the software engineering strategies that form the basis of the design and implementation of the framework. We show how designing a highly reusable OOAF architecture facilitates the development of new applications. We also highlight the key text categorization features of the framework, as well as practical considerations for application developers. Document Management, Text Categorization, Application Frameworks
      </response>
      <response id="42" uni="University of Sydney" weight="0.10" year="2005">
        Biomedical Named Entity Recognition System  We propose a machine learning approach, using a Maximum Entropy (ME) model to construct a Named Entity Recognition (NER) classifier to retrieve biomedical names from texts. In experiments, we utilize a blend of various linguistic features incorporated into the ME model to assign class labels and location within an entity sequence, and a postprocessing strategy for corrections to sequences of tags to produce a state of the art solution. The experimental results on the GENIA corpus achieved an F-score of 68.2% for semantic classification of 23 categories and achieved F-score of 78.1% on identification.  Named Entity Recognition, ME model, Information Retrieval.
      </response>
      <response id="144" uni="RMIT" weight="0.09" year="1997">
        Collection Selection via Lexicon Inspection  A distributed text database consists of multiple individual text collections. When a query is posed to a distributed text database, significant computational resources can be saved by identifing the individual collections that are the most likely to contain answers, as unnecessary accesses to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be effective, but depends on how performance is measured.
      </response>
      <response id="159" uni="University of Ballarat, La Trobe University" weight="0.09" year="1999">
        The use of argumentation to assist in the generation of legal documents  Many text documents in the legal domain are created in order to express the reasoning steps a decision maker followed in reaching conclusions. For example, refugee law determinations are documents that express the reasoning steps a member of the Refugee Review Tribunal in Australia followed in order to infer conclusions regarding the status of an applicant. Although, it is reasonable to expect that a mapping between the reasoning steps used by a decision maker and the structure of the document produced would clearly be apparent, a number of authors have discovered that such a mapping is by no means obvious. In order to develop legal knowledge based systems that generate documents from their own reasoning steps, discourse analysis is invoked to bridge the gap and perform the mapping. In this paper, we articulate a heuristic that we use to generate a plausible document structure without the use of discourse analysis. Without discourse analysis, the heuristic cannot contribute to our understanding of the process employed by decision makers to convert reasoning to text. Nevertheless, the heuristic can mimic the process. The heuristic has been trialed with a small sample of refugee law determinations by extracting the reasoning steps from each determination and applying the heuristic to reproduce each document's structure. Figure 11: The watermarked lena image Keywords refugee law.
      </response>
      <response id="85" uni="NICTA Victoria Research Laboratory The University of Melbourne" weight="0.08" year="2008">
        Extraction of Named Entities from Tables in Gene Mutation Literature  Information extraction and text mining are receiving growing attention as useful techniques for addressing the crucial information bottleneck in the biomedical domain. We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how this technique can be leveraged to improve on existing named entity detection systems for mutations.
      </response>
    </responses>
  </theme>
  <theme id="12" title="Theme 12">
    <words>
      <word weight="5.44727702114">
        index
      </word>
      <word weight="2.45600554362">
        order
      </word>
      <word weight="2.37539806694">
        collect
      </word>
      <word weight="1.60258204329">
        effect
      </word>
      <word weight="1.54657819444">
        retriev
      </word>
      <word weight="1.44342270733">
        search
      </word>
    </words>
    <responses>
      <response id="133" uni="Funnelback" weight="1.95" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="119" uni="CSIRO, KMITL, University of Glasgow" weight="1.03" year="2011">
        Indexing without Spam  The presence of spam in a document ranking is a major issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages that are likely to contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user's query. In this paper we propose removing spam pages at indexing time, therefore obtaining a pruned index that is virtually 'spam-free'. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performance. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection's index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches.  Information Retrieval; Index Pruning; Spam; Web search; Efficiency.
      </response>
      <response id="126" uni="University of Otago" weight="0.78" year="2012">
        Effects of Spam Removal on Search Engine Efficiency and Effectiveness  Spam has long been identified as a problem that web search engines are required to deal with. Large collection sizes are also an increasing issue for institutions that do not have the necessary resources to process them in their entirety. In this paper we investigate the effect that withholding documents identified as spam has on the resources required to process large collections. We also investigate the resulting search effectiveness and efficiency when different amounts of spam are withheld. We find that by removing spam at indexing time we are able to decrease the index size without affecting the indexing throughput, and are able to improve search precision for some thresholds.  Information Search and Retrieval, Content Analysis and Indexing - Indexing methods Information Filtering
      </response>
      <response id="147" uni="CSIRO, Division of Mathematical and Information Science, RMIT" weight="0.71" year="1998">
        Evaluation of Indexing Methods for Clustering  In order to synthesize a better answer based on the retrieved documents, we are exploring the use of clustering methods. In this paper, we present an evaluation of some popular indexing methods used for term selection and term weighting. The aim of indexing here is to represent documents, to relate documents with similar topics, and distinguish documents with different topics from each other. Experiments have been conducted to examine how the clustering results are influenced by some index term selection methods, such as the term selection based on the document frequency, and some term weighting methods, such as the inverted document frequency weight, the signal-noise ratio, and the term discrimination value, will influence the result of clustering. Based on our experiments, we recommend the use of the discrimination value weighting method together with a suitable set of indexing terms for the purpose of clustering the retrieved documents. Keywords Index, Term Selection, Term Weighting, Clustering.
      </response>
      <response id="70" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.69" year="2007">
        Hybrid Bitvector Index Compression  Bitvector index representations provide fast resolution of conjunctive Boolean queries, but require a great deal of storage space. On the other hand, compressed index representations are space-efficient, but query evaluation tends to be slower than bitvector evaluation, because of the need for sequential or pseudo-random access into the compressed index lists. Here we investigate a simple hybrid mechanism that stores only a small fraction of the inverted lists as bitvectors and has no or negligible effect on compressed index size compared to the use of byte codes, but improves query processing throughput compared to both byte coded representations and entirely-bitvector arrangements.  Index compression, bitvector, byte code, intersection algorithm.
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.41" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="95" uni="Queensland University of Technology" weight="0.39" year="2009">
        Random Indexing K-tree   Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using Ktree. The algorithms and data structures are defined, explained and motivated. Specific modifications to Ktree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm. Keywords Random Indexing, K-tree, Dimensionality Reduction, B-tree, Search Tree, Clustering, Document Clustering, Vector Quantization, k-means
      </response>
      <response id="78" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.39" year="2008">
        Term-Frequency Surrogates in Text Similarity Computations  Inverted indexes on external storage perform best when accesses are ordered and data is read sequentially, so that seek times are minimized. As a consequence, the various items required to compute Boolean, ranked and phrase queries are often interleaved in the inverted lists. While suitable for query types in which all items are required, this arrangement has the drawback that other query types - notably pure ranked queries and conjunctive Boolean queries - do not require access to word position information, and that component of each posting must be bypassed when these queries are being handled. In this paper we show that the term frequency component of each posting can be completely replaced by a surrogate that allows skipping of positional information interleaved in inverted lists, and obtain significant speedups in ranked query execution without increasing the index size, and without harming retrieval effectiveness. We also explore two methods of reconstituting approximations to the original term frequencies that can be employed if use of the surrogates is deemed too risky. Our simple improvement can thus be used with all ranking functions that make use of term frequencies.  Information retrieval, inverted index, skip pointer, proximity query, efficiency, effectiveness.
      </response>
      <response id="28" uni="RMIT University" weight="0.34" year="2004">
        Is CORI Effective for Collection Selection? An Exploration of Parameters, Queries, and Data  In distributed information retrieval, a wide range of techniques have been proposed for choosing collections to interrogate. Many of these collection-selection techniques are based on ranking the lexicons; of these, arguably the best known is the CORI collection ranking metric, which includes several parameters that, in principle, should be tuned for different data sets. However, parameters chosen in early work on CORI have been used without alteration in almost all subsequent work, despite drastic differences in the data collections. We have explored the behaviour of CORI for a range of data sets and parameter values. It appears that parameters cannot reliably be chosen for CORI: not only do the optimal choices vary between data sets, but they also vary between query types and, indeed, vary wildly within query sets. Coupled with the observation that even CORI with optimal parameters is usually less effective than other methods, we conclude that the use of CORI as a benchmark collection selection method is inappropriate. Keywords Lexicon indexing, distributed retrieval, information retrieval.
      </response>
      <response id="144" uni="RMIT" weight="0.29" year="1997">
        Collection Selection via Lexicon Inspection  A distributed text database consists of multiple individual text collections. When a query is posed to a distributed text database, significant computational resources can be saved by identifing the individual collections that are the most likely to contain answers, as unnecessary accesses to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be effective, but depends on how performance is measured.
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.28" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="162" uni="University of Sydney" weight="0.22" year="2000">
        Keyword Association Network: A Statistical Multi-term Indexing Approach for Document Categorization  A Keyword Association Network (KAN) is the network of keywords extracted from a collection of documents. In this network, the relationship between keywords is represented by a confidence value. It is argued in this paper thai the semantics and importance of a word can be more clearly and accurately measured by making use of other words that are co-occurring in a given document. The term frequency used for measuring the importance of terms in most document categorization methods ignores this important aspect. A KAN is constructed on the basis of co-occurring terms in documents. If two tenns appear more than a certain number of times in the same documents, they are considered as having close relationship. This paper proposes using KAN as a basis for finding informative keywords and using a confidence value in the process of document categorization. The process of constructing and application of KAN for document categorization is presented and the performance comparison with a typical statistical single-term document categorization algorithm - TFIDF classifier - will be shown. The experimental results show that KAN gives significant benefits. Keywords Document Categorization, Machine Learning. Statistical Multi-term indexing, Semantic- Meaning.
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.20" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="75" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.20" year="2007">
        Score Standardization for Robust Comparison of Retrieval Systems  Information retrieval systems are evaluated by applying them to standard test collections of documents, topics, and relevance judgements. An evaluation metric is then used to score a system's output for each topic; these scores are averaged to obtain an overall measure of effectiveness. However, different topics have differing degrees of difficulty and differing variability in scores, leading to inconsistent contributions to aggregate system scores and problems in comparing scores between different test collections. In this paper, we propose that per-topic scores be standardized on the observed score distributions of the runs submitted to the original experiment from which the test collection was created. We demonstrate that standardization equalizes topic contributions to system effectiveness scores and improves inter-collection comparability.  Retrieval system evaluation, average precision, standardization.
      </response>
      <response id="67" uni="The Australian National University, CSIRO ICT Centre" weight="0.18" year="2007">
        A Framework for Measuring the Impact of Web Spam  Web spam potentially causes three deleterious effects: unnecessary work for crawlers and search engines; diversion of traffic away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here we focus on the deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web spam. We index the 80 million page UK2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments.  Web Information Retrieval, Web Spam, Adversarial Information Retrieval
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.17" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="107" uni="The University of Melbourne, University of Malaya" weight="0.16" year="2010">
        Estimating System Effectiveness ScoresWith Incomplete Evidence  It is common for only partial relevance judgments to be used when comparing retrieval system effectiveness, in order to control experimental cost. Using TREC data, we consider the uncertainty introduced into per-topic effectiveness scores by pooled judgments, and measure the effect that incomplete evidence has on both the systems scores that are generated, and also on the quality of paired system comparisons. We measure system behavior from three different points of view: the trend in effectiveness scores; the separability of system pairs; and the number of reversals in significance outcomes as the depth of judgments increases. Our results show that when shallow pooled judgments are used system separability remains relatively high, but that there is also a high rate of significance reversal. We then show that explicitly adjusting effectiveness scores to allow for the known amount of uncertainty gives a reduced number of reversals, and hence more consistent experimental outcomes.  Retrieval evaluation, effectiveness metric, pooling
      </response>
      <response id="131" uni="Queensland University of Technology" weight="0.16" year="2012">
        Pairwise Similarity of TopSig Document Signatures  This paper analyses the pairwise distances of signatures produced by the TopSig retrieval model on two document collections. The distribution of the distances are compared to purely random signatures. It explains why TopSig is only competitive with state of the art retrieval models at early precision. Only the local neighbourhood of the signatures is interpretable. We suggest this is a common property of vector space models.   [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval Models Signature Files, Topology, Vector Space IR, Random Indexing, Document Signatures, Search Engines, Document Clustering, Near Duplicate Detection, Relevance Feedback
      </response>
      <response id="79" uni="The University of Sydney" weight="0.14" year="2008">
        MetaView: Dynamic metadata based views of user files  Hierarchical file systems are the most common way of organising large collections of documents. However, there are several desirable features they do lack. These include: good support for placing files in multiple locations; dynamic views on the users' data; and explicit ordering of files. This paper introduces MetaView, a new approach to enhancing file systems so that they can present users with a fluid and dynamic view of their files based on metadata. MetaView allows users to describe how they wish to view their files by specifying an organisational structure based on a metadata path. Experiments indicate that this approach is viable for collections of up to several thousand files in size, enabling flexible organisation of substantial parts of a user's file system.  Document Management, Metadata, Nonhierarchical file system.
      </response>
      <response id="155" uni="RMIT" weight="0.13" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.13" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="92" uni="University of Otago" weight="0.12" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.11" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="37" uni="The University of Melbourne" weight="0.11" year="2005">
        In Search of Reliable Retrieval Experiments  There are several ways in which an 'improved' technique for solving some computational problem can be defended: by mathematical argument; by simulation; and by experimental validation. Each of these has risks. In this paper we describe some of the issues that arose during an experimental validation of architectures for distributed text query evaluation, and the approaches that were taken to resolve them. In particular, collections and clusters must be scaled in a way that maximizes comparability between different data sizes; query sets must be appropriate to the target collection; and hardware issues such as file placement on disk must also be considered. Our intention is to report on our experience in a practical sense, and thereby assist others to avoid the same problems.
      </response>
      <response id="84" uni="RMIT University" weight="0.10" year="2008">
        The Effect of Using Pitch and Duration for Symbolic Music Retrieval  Quite reasonable retrieval effectiveness is achieved for retrieving polyphonic (multiple notes at once) music that is symbolically encoded via melody queries, using relatively simple pattern matching techniques based on pitch sequences. Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files.  Music information retrieval, Information retrieval, Multimedia resource discovery, Pattern matching
      </response>
      <response id="152" uni="Griffith University" weight="0.10" year="1999">
        DYNAMIC HYPER-LINKING BY QUERYING FOR A FCA-BASED QUERY SYSTEM  This paper presents a mechanism for hyper-linking documents by search-terms. Search-terms are selected by the user interactively building a formal concept lattice. In order to explain this interface we give some background to Formal Concept Analysis and an example is developed which illustrates the use of the concept lattice. Selected search-terms are used to create hyper-links, based on term repetition. As the search-terms differ between queries, we need a mechanism from which to dynamically create the target hyper-linked HTML documents. Therefore, documents are stored in a structure which is based on a word-list rather than plain text format. The documents are represented as links between the individual words within the word-list In so doing the word-list becomes a full-text-retrieval index into each word in each of those documents and therefore provides a good basis for the fast creation of an HTML document set from specific queries by keywords. To have the words in a word-list from which the documents are created also allows easy classification of words which should be hyper-linked within specific HTML documents. Furthermore, both documents and hyper-linking keywords are stored as well in this structure since any word in any document is indexed by the word-list. Document Databases, WWW and Internet.
      </response>
      <response id="127" uni="Queensland University of Technology, University of Otago" weight="0.10" year="2012">
        An English-Translated Parallel Corpus for the CJK Wikipedia Collections  In this paper, we describe a machine-translated parallel English corpus for the NTCIR Chinese, Japanese and Korean (CJK) Wikipedia collections. This document collection is named CJK2E Wikipedia XML corpus. The corpus could be used by the information retrieval research community and knowledge sharing in Wikipedia in many ways; for example, this corpus could be used for experimentations in cross-lingual information retrieval, cross-lingual link discovery, or omni-lingual information retrieval research. Furthermore, the translated CJK articles could be used to further expand the current coverage of the English Wikipedia.  Information Storage and Retrieval Digital Libraries - collection.
      </response>
      <response id="106" uni="RMIT University" weight="0.10" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.09" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="141" uni="Monash University" weight="0.09" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="51" uni="University of Sydney" weight="0.08" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
      <response id="13" uni="University of Sydney" weight="0.08" year="2002">
        Visualisation of Document and Concept Spaces  Collections of documents with conceptual relationships exist in many domains. Teaching systems often contain numerous learning resource documents. University policies are often large collections of related documents. The visualisation of the structure of these collections can be useful as it allows the exploration of the collection. This paper describes a graphical interface for visualising document spaces. The interface makes it simple for the user to explore the documents and the relationships between them. metadata, ITS, ontology extraction, user modelling, visualisation
      </response>
      <response id="10" uni="The University of Queensland" weight="0.08" year="2002">
        Visual Displays for Browsing RDF Documents  Hyperbolic browsers are motivated by the 'Circle Limit IV' woodcut of M.C. Escher. The hyperbolic tree view was introduced in graph drawing by Lamping and Rao [2] who observed that large structures could be compactly displayed by projecting a tree onto a hyperbolic plane. The effect of the projection is that components appear diminishing in size and radius exponentially the further they move from the centre of the diagram. The arguments for the hyperbolic display are twofold: an order of magnitude more nodes of a tree can be rendered in the same display space and the focus is maintained on the central vertex of the display and its immediate neighbourhood. The hyperbolic view is particularly useful for hierarchical diagrams with large numbers of leaves and branches and where neighbourhood relationships are meaningful. Examples of the hyperbolic view are INXIGHT's Star Tree1 and HYPERPROF2. Given that the pure hyperbolic geometric projection is patented, projection onto a sphere, and diminishing radial layout views are the drawing approaches we have experimented with.
      </response>
    </responses>
  </theme>
  <theme id="13" title="Theme 13">
    <words>
      <word weight="3.45281331973">
        tabl
      </word>
      <word weight="2.11143102676">
        process
      </word>
      <word weight="1.63782989366">
        text
      </word>
      <word weight="1.51221829394">
        approach
      </word>
      <word weight="0.919400643189">
        reus
      </word>
      <word weight="0.83088595943">
        paper
      </word>
    </words>
    <responses>
      <response id="163" uni="Cardiff University" weight="2.65" year="2000">
        The TREATS Approach to Reuse of Tables in Plain Text Documents  In this paper we present the table processing approach employed by the TREATS (Table Recognition, Extraction, Analysis and Tranformation System) software toolkit. In order to support the large variety of table layouts that appear in plain text documents, our system aims to identify the layout of cells that exist within a table and to tailor processing accordingly. This results in more effective processing than preexisting approaches that apply one general technique to all types of table. The classification process is the key to processing and exploits a cellular automaton (CA) based approach to the identification of cell structure within a table. The input to the CA is a simple representation of the content of the source table. This is evolved via the application of intelligent transformation rules, resulting in a representation of the cell structure that exists within the table. Based on the combination of cell types that appear in this representation, the layout of the table can be detennined and appropriate processing can be performed. During this processing, the content of the source table is transformed into a relational form suitable for reuse in other applications. discovery.
      </response>
      <response id="156" uni="Cardiff University" weight="1.83" year="1999">
        Effective Reuse of Textual Documents Containing Tabular Information  This paper presents an overview of a toolkit that can facilitate efficient reuse of tables appearing within textual documents [1]. In order to effectively reuse information contained in these documents, it is important that we process the accompanying text as well as any tables that appear. From this text, it may be possible to extract metadata such as descriptions of table content and related formulae, mappings and constraints. This metadata can then be exploited to enhance the value of extracted tables during their subsequent reuse. In this paper we present a discussion of the techniques used to process tables and associated text, both of which rely heavily on the use of regular expressions. Our techniques for locating tables utilise similar visual clues used by other table processing techniques discussed in the literature [5,6,7,8], although our approach to exploiting them is quite different. Our tools have been designed to provide a high level of support for the numerous types of table layout encountered in plain text tables, an area that has previously been somewhat overlooked.
      </response>
      <response id="115" uni="Macquarie University" weight="0.61" year="2010">
        A Rule-based Approach for Automatic Identification of Publication Types of Medical Papers  The medical domain has an abundance of textual resources of varying quality. The quality of medical articles depends largely on their publication types. However, identifying high-quality medical articles from search results is till date a manual and time-consuming process. We present a simple, rule-based, post-retrieval approach to automatically identify medical articles belonging to three high-quality publication types. Our approach simply uses title and abstract information of the articles to perform this. Our experiments show that such a rule-based approach has close to 100% precision and recall for the three publication types.   Medical Document Classification, Postretrieval Classification, Rule-based Classification, Evidence-based Medicine
      </response>
      <response id="159" uni="University of Ballarat, La Trobe University" weight="0.52" year="1999">
        The use of argumentation to assist in the generation of legal documents  Many text documents in the legal domain are created in order to express the reasoning steps a decision maker followed in reaching conclusions. For example, refugee law determinations are documents that express the reasoning steps a member of the Refugee Review Tribunal in Australia followed in order to infer conclusions regarding the status of an applicant. Although, it is reasonable to expect that a mapping between the reasoning steps used by a decision maker and the structure of the document produced would clearly be apparent, a number of authors have discovered that such a mapping is by no means obvious. In order to develop legal knowledge based systems that generate documents from their own reasoning steps, discourse analysis is invoked to bridge the gap and perform the mapping. In this paper, we articulate a heuristic that we use to generate a plausible document structure without the use of discourse analysis. Without discourse analysis, the heuristic cannot contribute to our understanding of the process employed by decision makers to convert reasoning to text. Nevertheless, the heuristic can mimic the process. The heuristic has been trialed with a small sample of refugee law determinations by extracting the reasoning steps from each determination and applying the heuristic to reproduce each document's structure. Figure 11: The watermarked lena image Keywords refugee law.
      </response>
      <response id="164" uni="Griffith University" weight="0.49" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="117" uni="Queensland University of Technology, CSIRO" weight="0.47" year="2010">
        Rule-based Approach for Identifying Assertions in Clinical Free-Text Data  A rule-based approach for classifying previously identified medical concepts in the clinical free text into an assertion category is presented.There are six different categories of assertions for the task: Present, Absent, Possible, Conditional, Hypothetical and Not associated with the patient. The assertion classification algorithms were largely based on extending the popular NegEx and Context algorithms. In addition, a health based clinical terminology called SNOMED CT and other publicly available dictionaries were used to classify assertions, which did not fit the NegEx/Context model. The data for this task includes discharge summaries from Partners HealthCare and from Beth Israel Deaconess Medical Centre, as well as discharge summaries and progress notes from University of Pittsburgh Medical Centre. The set consists of 349 discharge reports, each with pairs of ground truth concept and assertion files for system development, and 477 reports for evaluation. The system's performance on the evaluation data set was 0.83, 0.83 and 0.83 for recall, precision and F1-measure, respectively. Although the rule-based system shows promise, further improvements can be made by incorporating machine learning approaches.  rule-based, medical concept, assertion, NegEx, Context, SNOMED CT.
      </response>
      <response id="101" uni="University of Sydney" weight="0.41" year="2009">
        An Automatic Question Generation Tool for Supporting Sourcing and Integration in Students' Essays   This paper presents a domain independent Automatic Question Generation (AQG) tool that generates questions which can be used as a form of support for students to revise their essay. The focus here is on generating questions based on semantic and syntactic information acquired from citations. The semantic information includes the author's name, the citation type (describing the aim of the cited study, its results or an opinion), the author's expressed sentiment, and the syntactic information of the citation. Pedagogically, the question templates are designed using Bloom's learning taxonomy where the questions reach the Analysis Level. We used 40 undergraduate students essays for our experiment and the Name Entity Recognition component is trained on 20 essays. The result of our experiment shows that the question coverage is 96% and accuracy of generated questions can reach 78%. This AQG tool will be integrated into our peer review system to scaffold feedback from peers.  Question Generation, Electronic Feedback System for Sourcing and Integration in Students' Essay
      </response>
      <response id="143" uni="RMIT" weight="0.38" year="1997">
        Supporting the Answering Process  This paper is concerned with the way information access systerns support the question answering process. This process includes three stages: question formulation, information gathering, and analysis and synthesis. Standard information access technologies are mainly concerned with the second of these stages, providing little or no support for the last stage. However, the raw information gathered at this point can seldom be used directly as an answer. This paper discusses issues relating to the support of the analysis and synthesis stage, and suggests how information access systems might be extended to better support it. This paper also describes a WWW-based experimental interface that permits the evaluation of alternate ways of supporting this analysis and synthesis stage of the answering process. Information Retrieval, Question Answering, Passage Retrieval, Answer Presentation, Hypertext, WWW.
      </response>
      <response id="85" uni="NICTA Victoria Research Laboratory The University of Melbourne" weight="0.38" year="2008">
        Extraction of Named Entities from Tables in Gene Mutation Literature  Information extraction and text mining are receiving growing attention as useful techniques for addressing the crucial information bottleneck in the biomedical domain. We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how this technique can be leveraged to improve on existing named entity detection systems for mutations.
      </response>
      <response id="31" uni="Carnegie Mellon University, University of Sydney" weight="0.34" year="2004">
        Phrases and Feature Selection in E-Mail Classification  In this paper we study the effectiveness of using a phrase-based representation in e-mail classification, and the affect this approach has on a number of machine learning algorithms. We also evaluate various feature selection methods and reduction levels for the bag-of-words representation on several learning algorithms and corpora. The results show that the phrasebased representation and feature selection methods can be used to increase the performance of e-mail classifiers.  E-Mail Classification, Text Categorization, Feature Selection
      </response>
      <response id="43" uni="The University of Sydney" weight="0.34" year="2005">
        Cross Training and Under Sampling in Categorization of Company Announcements  To process the documents in a share market is crucial. It is because financial activities are socio-economic driven and text documents contain a lot of valuable information. In this paper, we focus on one of these documents, the Company Announcement. Each of these documents requires to be labelled as price sensitive or not before presenting to the general public. In our experiments, we study two specific issues in this text categorization, namely the effectiveness of a feature vector obtained from the corpus belonging to another market sector and the imbalanced nature of the dataset. Our results indicate that the classification can benefit from a different (but related) set of corpus because of a more diversified and generalised nature of the feature set. Regarding the skewness of the dataset, the under-sampling of the majority class in the training process does not have a strong effect on the performance in the test set, while keeping the computational cost minimised.  Document Management, Text Categorization
      </response>
      <response id="69" uni="CSIRO ICT Centre" weight="0.34" year="2007">
        Document Composition and Content Selection Evaluation  Our work is concerned with the design of adaptive hypertext systems that produce documents tailored to their intended reader. In our approach, a system composes document on-the-fly, assembling existing text fragments. One of our challenges in this approach is to support the technical writer who configures the system. The task of the technical writer is to specify the structure of the documents to be generated, together with their applicability conditions. To perform their task, authors need to know what information is available. In this paper, we examine the impact of different strategies for presenting the existing text fragments on the task of document composition. We focus in particular on the impact on the quality of the resulting documents. We found that people compose better documents when existing text fragments are presented in a structured way.  document composition, information reuse, document quality, evaluation, method.
      </response>
      <response id="7" uni="The University of Sydney" weight="0.31" year="2002">
        A Multi-Learner Approach to Email Classification    The volume of email which most users receive has grown over teh past few years. Most users try to cope with this problem by sorting email into folders. In this paper we look at machine learning approaches to performing this classification automatically. In particular we describe a test and select approach to choosing both single learners and ensembles of learners to classify an individual user's email. The results show that is possible to select the most effective single learner for an individual user, but that selecting an ensemble without overfitting is more difficult. We also show that Widrow-Hoff is a highly effective algorithm for email classification and discuss the reasons for this.  Document Management, Email Management, Text Categorization, Machine Learning
      </response>
      <response id="137" uni="The Boeing Company" weight="0.30" year="1997">
        Analyzing Image Content for a Large Scale Hypermedia System  The Boeing Company maintains tens of millions of pages of information associated with the manufacture and delivery of its products. Much of this information must be made available electronically. We have developed tools to automatically convert and integrate electronic data into industry standard formats. Some of the technical challenges include I) handling a wide variety of source formats, 2) making sure that the tools scale up to handle millions of pages of information, and 3) adding functionality to graphics. Our system contains over four million pages of text including tens of thousands of graphics. In this paper we describe tools that recognize and use information within airplane-related vector and raster images. Such images include troubleshooting charts, fault reporting diagrams, component location diagrams, component index tables, wiring diagrams, system schematics, parts illustrations, standards tables, and structural and tooling drawings. Each airplane requires conversion of over 20,000 graphics including over 900,000 pieces of cross-referenced information. We are also exploring visual information retrieval strategies, including content-based and similarity-based methods for both vector and raster graphics. information retrieval, hypermedia
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.30" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="21" uni="The University of Sydney" weight="0.29" year="2002">
        A Framework for Text Categorization In this paper we discuss the architecture of an object-oriented application framework (OOAF) for text categorization. We describe the system requirements and the software engineering strategies that form the basis of the design and implementation of the framework. We show how designing a highly reusable OOAF architecture facilitates the development of new applications. We also highlight the key text categorization features of the framework, as well as practical considerations for application developers. Document Management, Text Categorization, Application Frameworks
      </response>
      <response id="155" uni="RMIT" weight="0.27" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.26" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="1" uni="University of Queensland" weight="0.26" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="144" uni="RMIT" weight="0.25" year="1997">
        Collection Selection via Lexicon Inspection  A distributed text database consists of multiple individual text collections. When a query is posed to a distributed text database, significant computational resources can be saved by identifing the individual collections that are the most likely to contain answers, as unnecessary accesses to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be effective, but depends on how performance is measured.
      </response>
      <response id="73" uni="RMIT University, INRIA" weight="0.21" year="2007">
        Use of Wikipedia Categories in Entity Ranking  Wikipedia is a useful source of knowledge that has many applications in language processing and knowledge representation. The Wikipedia category graph can be compared with the class hierarchy in an ontology; it has some characteristics in common as well as some differences. In this paper, we present our approach for answering entity ranking queries from the Wikipedia. In particular, we explore how to make use of Wikipedia categories to improve entity ranking effectiveness. Our experiments show that using categories of example entities works significantly better than using loosely defined target categories.
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.21" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="161" uni="France" weight="0.20" year="2000">
        Towards an Efficient Retrieval of Medical Imaging  Image description is not an easy task. The same image can be described through different views: on the basis of either low-level properties, such as texture or color; context, such as date of acquisition or author: or semantic content, such as real-world objects and relations. Our approach consists in providing a global description solution capable of integrating different dimensions (or views) of a medical image. Via our approach, we are able to propose a solution that takes into consideration the heterogeneity of user competence (physician, researcher, student, etc.) and a high expressive power for medical imaging description. Visual solutions are recommended and are the most suited for non &quot;novice&quot; users in computing. However, current visual languages suffer from several problems as imprecision and no respect of integrity of spatial relations. Particularly, resolution of ambiguities generated by the user and/or the system at different levels of image description remains a challenge. In this paper, we present our solution for resolving these issues. A prototype has been implemented. Information Retrieval, Medical Imaging, Spatial Relations, Ambiguity Resolving.
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.20" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.20" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="30" uni="RMIT University" weight="0.19" year="2004">
        A Testbed for Indonesian Text Retrieval  Indonesia is the fourth most populous country and a close neighbour of Australia. However, despite media and intelligence interest in Indonesia, little work has been done on evaluating Information Retrieval techniques for Indonesian, and no standard testbed exists for such a purpose. An effective testbed should include a collection of documents, realistic queries, and relevance judgements. The TREC and TDT testbeds have provided such an environment for the evaluation of English, Mandarin, and Arabic text retrieval techniques. The NTCIR testbed provides a similar environment for Chinese, Korean, Japanese, and English. This paper describes an Indonesian TREC-like testbed we have constructed and made available for the evaluation of ad hoc retrieval techniques. To illustrate how the test collection is used, we briefly report the effect of stemming for Indonesian text retrieval, showing - similarly to English - that it has little effect on accuracy.  Indonesian, queries, collection, relevance judgements, stemming
      </response>
      <response id="98" uni="University of Sydney" weight="0.18" year="2009">
        Feature Selection and Weighting Methods in Sentiment Analysis  Sentiment analysis is the task of identifying whether the opinion expressed in a document is positive or negative about a given topic. Unfortunately, many of the potential applications of sentiment analysis are currently infeasible due to the huge number of features found in standard corpora. In this paper we systematically evaluate a range of feature selectors and feature weights with both Naive Bayes and Support Vector Machine classifiers. This includes the introduction of two new feature selection methods and three new feature weighting methods. Our results show that it is possible to maintain a state-of-the art classification accuracy of 87.15% while using less than 36% of the features.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="41" uni="The University of Queensland, The University of Wollongong" weight="0.17" year="2005">
        Applying Formal Concept Analysis to Semantic File Systems Leveraging Wordnet  Formal Concept Analysis can be used to obtain both a natural clustering of documents along with a partial ordering over those clusters. The application of Formal Concept Analysis requires input to be in the form of a binary relation between two sets. This paper investigates how a semantic filesystem can be used to generate such binary relations. The manner in which the binary relation is generated impacts how useful the result of Formal Concept Analysis will be for navigating one's filesystem.  Document Databases, Document Management
      </response>
      <response id="48" uni="CSIRO ICT Centre" weight="0.16" year="2006">
        My Instant Expert  This paper gives an overview of a mobile device question answering application that has recently been developed in the CSIRO ICT Centre. The application makes use of data in an open-domain encyclopaedia to answer general knowledge questions. The paper presents the techniques used, results and error analysis on the project.  Information retrieval, Question answering
      </response>
      <response id="106" uni="RMIT University" weight="0.16" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="42" uni="University of Sydney" weight="0.15" year="2005">
        Biomedical Named Entity Recognition System  We propose a machine learning approach, using a Maximum Entropy (ME) model to construct a Named Entity Recognition (NER) classifier to retrieve biomedical names from texts. In experiments, we utilize a blend of various linguistic features incorporated into the ME model to assign class labels and location within an entity sequence, and a postprocessing strategy for corrections to sequences of tags to produce a state of the art solution. The experimental results on the GENIA corpus achieved an F-score of 68.2% for semantic classification of 23 categories and achieved F-score of 78.1% on identification.  Named Entity Recognition, ME model, Information Retrieval.
      </response>
      <response id="110" uni="University of Otago" weight="0.14" year="2010">
        Efficient Accumulator Initialisation  IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. Traditionally, the performance of search engines is dominated by slow disk I/O, CPU-intensive decompression, complex similarity ranking functions and sorting a large number of candidate documents. However, after we have applied a number of optimisation techniques, our search engine is bottlenecked by accumulator initialisation. In this paper, we propose an efficient accumulator initialisation algorithm, which represents the traditional static accumulator array as a logical two dimensional table and uses a number of flags to track the initialisation status of the accumulators. The efficiency of the algorithm is verified by a simulation program and a search engine. The overall performance can be as good as a 93% increase in throughput.  Accumulator Initialisation, Efficiency, Postings Pruning.
      </response>
      <response id="104" uni="Queensland University of Technology, University of Otago" weight="0.14" year="2009">
        The Methodology of Manual Assessment in the Evaluation of Link Discovery  The link graph extracted from the Wikipedia has often been used as the ground truth for measuring the performance of automated link discovery systems. Extensive manual assessments experiments at INEX 2008 recently showed that this is unsound and that manual assessment is essential. This paper describes the methodology for link discovery evaluation which was developed for use in the INEX 2009 Link-the-Wiki track. In this approach both manual and automatic assessment sets are generated and runs are evaluated using both. The approach offers a more reliable evaluation of link discovery methods than just automatic assessment. A new evaluation measure for focused link discovery is also introduced.  Wikipedia, Link Quality, Manual Assessment, Evaluation.
      </response>
      <response id="29" uni="The University of Sydney" weight="0.13" year="2004">
        Co-training on Textual Documents with a Single Natural Feature Set  Co-training is a semi-supervised technique that allows classifiers to learn with fewer labelled documents by taking advantage of the more abundant unclassified documents. However, conventional cotraining requires the dataset to be described by two disjoint and natural feature sets that are redundantly sufficient. In many practical situations datasets have a single set of features and it is not obvious how to split it into two. This paper investigates the performance of co-training with only one natural feature set in two applications: Web page classification and email filtering. Text categorization, Web page classification, spam filtering, co-training
      </response>
      <response id="121" uni="RMIT University, Gunma University" weight="0.13" year="2011">
        Language Independent Ranked Retrieval with NeWT   In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt. To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks. We evaluate the query effectiveness of our indexes using Japanese and English. We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.  Text Indexing, Language Independent Text Indexing, Data Storage Representations, Experimentation, Measurement, Performance, Data Compression
      </response>
      <response id="94" uni="RMIT University" weight="0.13" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="109" uni="RMIT University" weight="0.13" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.13" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="66" uni="RMIT University" weight="0.13" year="2007">
        Source Code Authorship Attribution using n-grams  Plagiarism and copyright infringement are major problems in academic and corporate environments. Existing solutions for detecting infringements in structured text such as source code are restricted to textual similarity comparisons of two pieces of work. In this paper, we examine authorship attribution as a means for tackling plagiarism detection. Given several samples of work from several authors, we attempt to correctly identify the author of work presented as a query. On a collection of 1 640 documents written by 100 authors, we show that we can attribute authorship in up to 67% of cases. This work can be a valuable additional indicator for the more difficult plagiarism investigations.  Authorship Attribution, Plagiarism Detection, Co-derivative Documents
      </response>
      <response id="10" uni="The University of Queensland" weight="0.12" year="2002">
        Visual Displays for Browsing RDF Documents  Hyperbolic browsers are motivated by the 'Circle Limit IV' woodcut of M.C. Escher. The hyperbolic tree view was introduced in graph drawing by Lamping and Rao [2] who observed that large structures could be compactly displayed by projecting a tree onto a hyperbolic plane. The effect of the projection is that components appear diminishing in size and radius exponentially the further they move from the centre of the diagram. The arguments for the hyperbolic display are twofold: an order of magnitude more nodes of a tree can be rendered in the same display space and the focus is maintained on the central vertex of the display and its immediate neighbourhood. The hyperbolic view is particularly useful for hierarchical diagrams with large numbers of leaves and branches and where neighbourhood relationships are meaningful. Examples of the hyperbolic view are INXIGHT's Star Tree1 and HYPERPROF2. Given that the pure hyperbolic geometric projection is patented, projection onto a sphere, and diminishing radial layout views are the drawing approaches we have experimented with.
      </response>
      <response id="87" uni="NICTA Victoria Laboratory, The University of Melbourne" weight="0.11" year="">
        Parameter Sensitivity in Rank-Biased Precision  Rank-Biased Precision (RBP) is a retrieval evaluation metric that assigns an effectiveness score to a ranking by computing a geometricly weighted sum of document relevance values, with the monotonicly decreasing weights in the geometric distribution determined via a persistence parameter p. Despite exhibiting various advantageous traits over well known existing measures such as Average Precision, RBP has the drawback of requiring the designer of any experiment to choose a value for p. Here we present a method that allows retrieval systems evaluated using RBP with different p values to be compared. The proposed approach involves calculating two critical bounding relevance vectors for the original RBP score, and using those vectors to calculate the range of possible RBP scores for any other value of p. Those bounds may then be sufficient to allow the outright superiority of one system over the other to be established. In addition, the process can be modified to handle any RBP residuals associated with either of the two systems. We believe the adoption of the comparison process described in this paper will greatly aid the uptake of RBP in evaluation experiments.  Rank-Biased Precision, Evaluation, System Comparison
      </response>
      <response id="65" uni="University of Melbourne" weight="0.11" year="2007">
        Automatic Thread Classification for Linux User Forum Information Access  We experiment with text classification of threads from Linux web user forums, in the context of improving information access to the problems and solutions described in the threads. We specifically focus on classifying threads according to: (1) them describing a specific problem vs. containing a more general discussion; (2) the completeness of the initial post in the thread; and (3) whether problem(s) in the initial post are resolved in the thread or not. We approach these tasks in both classification and regression frameworks using a range of machine learners and evaluation metrics.  Web Documents, Document Management
      </response>
      <response id="76" uni="Queensland University of Technology" weight="0.11" year="2007">
        Efficient Neighbourhood Estimation for Recommenders with Large Datasets  In this paper, we present a novel neighbourhood estimation method which is not only both memory and computation efficient but can also achieves better estimation accuracy than other cluster based neighbourhood formation techniques. In this paper we have successfully incorporated the proposed technique with a taxonomy based product recommender, and with the proposed neighbourhood formation technique both time efficiency and recommendation quality of the recommender are improved.  Recommender Systems, Neighbourhood Estimation, Product Taxonomy
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.10" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="5" uni="The University of Sydney" weight="0.10" year="2002">
        Automatic Categorization of Announcements on the Australian Stock Exchange This paper compares the performance of several machine learning algorithms for the automatic categorization of corporate announcements in the Australian Stock Exchange (ASX) Signal G data stream. The article also describes some of the applications that the categorization of corporate announcements may enable. We have performed tests on two categorization tasks: market sensitivity, which indicates whether an announcement will have an impact on the market, and report type, which classifies each announcement into one of the report categories defined by the ASX. We have tried Neural Networks, a Naive Bayes classifier, and Support Vector Machines and achieved good results. Keywords Document Management, Document Workflow
      </response>
      <response id="149" uni="CSIRO Mathematical and Information Sciences" weight="0.10" year="1998">
        Automatic Document Creation from Software Specifications  Software documentation, and in particular, on-line help is a crucial aspect of a software system. Producing and maintaining it, however, is both labor intensive and tedious, making it a candidate for automation. This paper presents our work on automatically generating hypertext based on-line help, starting from software specifications. Our approach is motivated by practical considerations, such as the impossibility to construct by hand the semantic knowledge base typically required by a generation system. Natural Language Generation, Software documentation, hypertext
      </response>
      <response id="120" uni="CSIRO, Australian National University, University of Applied Science Technikum Wien, Funnelback" weight="0.10" year="2011">
        Automatic identification of the most important elements in an XML collection  An important problem in XML retrieval is determining the most useful element types to retrieve - e.g. book, chapter, section, paragraph or caption. An automated system for doing this could be based on features of element types related to size, depth, frequency of occurrence, etc. We consider a large number of such features and assess their usefulness in predicting the types of elements judged relevant in INEX evaluations for the IEEE and Wikipedia 2006 corpora. For each feature we automatically assign Useful / Not-Useful labels to element types using Fuzzy c-Means Clustering. We then rank the features by the accuracy with which they predict the manual judgments. We find strong overlap between the top-ten most predictive features for the two collections and that seven features achieve high average accuracy (F-measure &gt; 65%) acrosss them. We hypothesize that an XML retrieval system working on an unlabelled corpus could use these features to decide which retrieval units are most appropriate to return to the user.  XML Retrieval, Fuzzy C-Means Clustering, F-Measure.
      </response>
      <response id="105" uni="University of Otago" weight="0.10" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="60" uni="CSIRO ICT Centre Macquarie University" weight="0.10" year="2007">
        Can Requests-for-Action and Commitments-to-Act be Reliably Identified in Email Messages?  This paper reports on the results of an exploratory annotation task where three coders classified the presence and strength of Requests-for- Action (requests) and Commitments-to-Act (promises) in workplace email messages. The purpose of our annotation task was to explore levels of human agreement to establish whether this is a repeatable task that lends itself to automation. The results from our annotation task suggest that there is relatively high agreement about which sentences embody Requests-for-Action (= 0.78), but poorer agreement about Commitments-to-Act (= 0.54). Analysis of cases of coder disagreement highlighted several areas of systematic disagreement which we believe can be addressed through refining our annotation guidelines. Given this scope for improving agreement, we believe the results presented here are encouraging for our intention to perform largerscale annotation work leading to automation of the detection and classification of Requests-for-Action and Commitments-to-Act in email communication.  Email, document workflow, document management, Speech Acts, task management
      </response>
      <response id="40" uni="Victoria University" weight="0.09" year="2005">
        An Experimental Study of Workflow and Collaborative Document Authoring in Medical Research  Workflow is asynchronous technology widely used in the automation of organisational processes. Workflow provides benefits such as greater efficiency in an organisation, better worker productivity and greater process control. Synchronous collaborative authoring tools are technologies that allow a group of dispersed authors to write a document at the same time. These tools are beneficial in assisting authors to write some proportion, if not all of a document from an experiment.  This paper presents findings from an experiment combining both workflow and collaborative authoring tools workflow and collaborative authoring tools in a medical research environment. Studies investigating the combination of these tools are few, resulting in a lack of understanding of how this combination can effectively assist organisations in document-based processes. Overall, the combined workflow/collaborative authoring solution was found effective in the generation of a medical research paper.  Workflow, collaborative document authoring, medical research, experimental study
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.09" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="54" uni="CSIRO ICT Centre" weight="0.09" year="2006">
        InexBib - Retrieving XML elements based on external evidence  Creating a scientific bibliography on a given topic is currently a task which requires a great deal of manual effort. We attempt to reduce this effort by developing a tool for automatically generating a bibliography from a collection of articles represented in XML. We evaluate the use of elements around the references as anchortexts to improve search results. We find that users of the tool prefer lists generated using anchortext over those generated from the bibliography entry only and that the preference is statistically significant. We tentatively find no significant preference for results generated using paragraph as opposed to sentence level anchortext, but note that this finding may result from lack of sophistication in resolving text including multiple references.  Information Retrieval, XML, Element Retrieval, Bibliography
      </response>
      <response id="88" uni="University of Melbourne" weight="0.09" year="2008">
        Querying Linguistic Annotations  Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.  Information Retrieval, Natural Language Techniques and Documents, XML Document Standards
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.08" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="18" uni="University of South Australia" weight="0.08" year="2002">
        Towards a System to Improve Administrative Processes for Front-Line Academic Staff  Current  economic  pressures  are  causing  severe problems for many enterprises in maintaining service standards  with  shrinking  headcounts.  Groupware, Workflow  and  Agent  technologies  have  been  widely advocated  as  a  solution,  but  there  are  few  reported success  stories.  The  project  described  in  this  paper addresses  the  case  of  running  large  undergraduate courses. A preliminary vision of a possible integrated administrative  support  system  is  presented,  and  the future  activities  necessary  to  advance  such  a  vision are outlined.  Administrative  Applications,  User Interface Design, Software Agents
      </response>
    </responses>
  </theme>
  <theme id="14" title="Theme 14">
    <words>
      <word weight="5.49496714566">
        style
      </word>
      <word weight="3.58494047099">
        user
      </word>
      <word weight="3.42334203331">
        navig
      </word>
      <word weight="2.94123306156">
        cognit
      </word>
      <word weight="2.22650339263">
        search
      </word>
      <word weight="2.22394529874">
        inform
      </word>
    </words>
    <responses>
      <response id="113" uni="Queensland University of Technology" weight="2.66" year="2010">
        The Impact of Users' Cognitive Style on Their Navigational Behaviors in Web Searching  User-Web interactions have emerged as an important area of research in the field of information science. In this study, we investigate the effects of users' cognitive styles on their Web navigational styles and information processing strategies. We report results from the analyses of 594 minutes recorded Web search sessions of 18 participants engaged in 54 scenariobased search tasks. We use questionnaires, cognitive style test, Web session logs and think-aloud as the data collection instruments. We classify users' cognitive styles as verbalisers and imagers based on Riding's (1991) Cognitive Style Analysis test. Two classifications of navigational styles and three categories of information processing strategies are identified. Our study findings show that there exist relationships between users' cognitive style, and their navigational styles and information processing strategies. Verbal users seem to display sporadic navigational styles, and adopt a scanning strategy to understand the content of the search result page, while imagery users follow a structured navigational style and reading approach. We develop a matrix and a model that depicts the relationships between users' cognitive styles, and their navigational style and information processing strategies. We discuss how the findings from this study could help search engine designers to provide an adaptive navigation support to users.  Web Searching, Navigational Style, Information Processing Strategy, User Cognitive Style.
      </response>
      <response id="135" uni="CSIRO" weight="0.17" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="97" uni="University of Otago" weight="0.14" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="114" uni="RMIT University" weight="0.13" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="164" uni="Griffith University" weight="0.12" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="68" uni="Cambridge, UK" weight="0.12" year="2007">
        Search and Navigation in Structured Document Retrieval: Comparison of User Behaviour in Search on Document Passages and XML Elements  This paper investigates search and browsing behaviour of users presented with two types of structured document retrieval approaches: passage retrieval and XML element retrieval. Our findings, based on the system logs gathered from 82 participants of the INEX 2006 interactive track experiment (iTrack), indicate that XML element retrieval leads to increased task performance. In addition, qualitative analysis of our video study, where we recorded the interactions of four participants, highlights potential issues with the experimental design employed at iTrack 2006.  XML element retrieval, passage retrieval, INEX interactive track, video user study.
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.11" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="94" uni="RMIT University" weight="0.10" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="92" uni="University of Otago" weight="0.10" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="46" uni="Melbourne" weight="0.09" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="45" uni="The University of Melbourne" weight="0.08" year="2006">
        Some Observations on User Search Behavior  We explore some issues that arise in the way that users interact with a web search engine, as evidenced by the records of their interaction provided by query and clickthrough log data. Our observations are derived from approximately fifteen million user queries recorded by the search.msn.com search service in May 2006.  Log analysis, user behavior, search.
      </response>
    </responses>
  </theme>
  <theme id="15" title="Theme 15">
    <words>
      <word weight="4.95606206828">
        workflow
      </word>
      <word weight="2.89146399889">
        document
      </word>
      <word weight="1.45625422795">
        model
      </word>
      <word weight="1.28346631572">
        process
      </word>
      <word weight="1.10658995215">
        appli
      </word>
      <word weight="1.04701830777">
        conceptu
      </word>
    </words>
    <responses>
      <response id="140" uni="The University of Queensland, CRC for Distributed Systems Technology" weight="2.75" year="1997">
        Applying a Generic Conceptual Workflow Modeling Technique to Document Workflows  The workflow technology is emerging as an appropriate platform for the automated coordination of business activities. The documents represent the primary medium of business communication. Almost all kinds of business activities have some associated documents. The workflow management systems could be applied to coordinate the flow of business documents. However, before this automated document workflows could be implemented, we need to apply some conceptual modeling methodology to capture, analyse, and describe the role and flow of documents in a business process. In this paper, we present a generic conceptual workflow modeling technique that should be applicable to all kinds of workflows. The document workflows represent a specialized application of workflows. We identify basic characteristics of document workflows and discuss some issues that should be targeted during the modeling process. We also apply the proposed conceptual modeling technique to model an example document workflows application for handling postgraduate admission process of a university. conceptual modeling of workflows, document workflows.
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="1.89" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
      <response id="40" uni="Victoria University" weight="1.20" year="2005">
        An Experimental Study of Workflow and Collaborative Document Authoring in Medical Research  Workflow is asynchronous technology widely used in the automation of organisational processes. Workflow provides benefits such as greater efficiency in an organisation, better worker productivity and greater process control. Synchronous collaborative authoring tools are technologies that allow a group of dispersed authors to write a document at the same time. These tools are beneficial in assisting authors to write some proportion, if not all of a document from an experiment.  This paper presents findings from an experiment combining both workflow and collaborative authoring tools workflow and collaborative authoring tools in a medical research environment. Studies investigating the combination of these tools are few, resulting in a lack of understanding of how this combination can effectively assist organisations in document-based processes. Overall, the combined workflow/collaborative authoring solution was found effective in the generation of a medical research paper.  Workflow, collaborative document authoring, medical research, experimental study
      </response>
      <response id="123" uni="University of Waikato" weight="0.48" year="2011">
        A Workflow for Document Level Interoperability  This article describes a software environment called the Exchange Center that helps digital librarians manage the workflow of sourcing documents and metadata from various repositories. The software is built on Greenstone but does not require its use as the final digital library server. After describing the software architecture we provide two scenarios of its use: a private library of recipes, which ultimately involves collaboration with other cooks; and a digital library that aggregates the collections of various host institutions that use different repository software.  Digital Library Interoperability, Software Architecture, Workflow
      </response>
      <response id="9" uni="University of Sydney" weight="0.45" year="2002">
        Workflow Based Just-in-time Training  This paper focuses on the problem of information overload for newcomers in an organisation. We propose to address it by constructing a smart personal training assistant based upon workflow tools to drive temporal management of a just-in time workplace training system which will deliver a personalised and structured presentation of organisational documents.  Document Workflow, Document Management, Information Retrieval.
      </response>
      <response id="60" uni="CSIRO ICT Centre Macquarie University" weight="0.30" year="2007">
        Can Requests-for-Action and Commitments-to-Act be Reliably Identified in Email Messages?  This paper reports on the results of an exploratory annotation task where three coders classified the presence and strength of Requests-for- Action (requests) and Commitments-to-Act (promises) in workplace email messages. The purpose of our annotation task was to explore levels of human agreement to establish whether this is a repeatable task that lends itself to automation. The results from our annotation task suggest that there is relatively high agreement about which sentences embody Requests-for-Action (= 0.78), but poorer agreement about Commitments-to-Act (= 0.54). Analysis of cases of coder disagreement highlighted several areas of systematic disagreement which we believe can be addressed through refining our annotation guidelines. Given this scope for improving agreement, we believe the results presented here are encouraging for our intention to perform largerscale annotation work leading to automation of the detection and classification of Requests-for-Action and Commitments-to-Act in email communication.  Email, document workflow, document management, Speech Acts, task management
      </response>
      <response id="39" uni="University of Wollongong" weight="0.26" year="2005">
        ePOC: Mobile Clinical Information Access and Diffusion in Ambulatory Care Service Settings  This paper represents a preliminary overview (work-in-progress) of a mobile e-Health research and development project and the intrinsic considerations which arise when designing such patient data management systems tailored to ambulatory care. Its purpose is to give an outline of the issues that allow technological enablement of electronic patient data management in the delivery of home-based medical care. While the replacement of more traditional paper-based patient data management using Personal Digital Assistants as a collection platform is technically straightforward, the organizational realignment of an electronic document management system requires careful study and deployment in order to maximize success. We outline the methodological considerations for document management diffusion within this e-Health setting and describe the issues, architecture and proposed rollout of an electronic Point-Of-Care (ePOC) system.  e-Health, document management and workflow, information access and diffusion
      </response>
      <response id="2" uni="University of Sydney" weight="0.25" year="2002">
        Generating and Comparing Models within an Ontology An ontology is useful for providing a conceptually concise basis for developing and communicating knowledge. This paper discusses an application of an automatically constructed ontology of computer science and its use for comparison of models in the computer science domain. We present the architecture, algorithms and current results for MECUREO, a system which builds an extensive model of a computer science entity from limited information. The entity might be a document such as an email message, a course description document or a biography. It is intended to give a measure of the similarity of any two such models. We describe MECUREO's mechanism for constructing and representing models and its present performance in comparing models. Keywords information retrieval, ontologies, acquisition, sharing and reuse of conceptual structures
      </response>
      <response id="4" uni="University of Sydney" weight="0.25" year="2002">
        Supporting user task based conversations via e-mail  Email is commonly used for conversations. These consist of a sequence od messages which deal with a common task. It would be helpful if mail clients could automatically group messages from one conversation. This would facilitate the user's processing of them as it would enable the user to establish the context of the task that is at the core of the conversation.   This paper describes IETMS, a mail client which can employ a range of approaches for this task: standard mail header elements; a TF-IDF classifier and user-lists. As a foundation for improving our understanding of the effectiveness of these mechanisms, we have performed a detailed, small-scale study involving a corpus of mail which contains a collection of conversations about an important subclass of conversations, those concerned with organisational meetings. The corpus size was chosen to be comparable to the number of conversations that might  run in parallel fo rone user who is a quite heavy e-mail user. Our study indicates the relative power of each of these as well as their combined power. It also gives insight into the value of modelling individual user's email behaviors and the ways that these interact with classification mechanisms.  Document Databases, Document Workflow, Document Management, Information Retrieval
      </response>
      <response id="92" uni="University of Otago" weight="0.20" year="2009">
        Id - Dynamic Views on Static and Dynamic Disassembly Listings  Disassemblers are tools which allow software developers and researchers to analyse the machine code of computer programs. Typical disassemblers convert a compiled program into a static disassembly document which lists the machine instructions of the program. Information which would indicate the purpose of routines, such as comments and symbol names, are not present in the compiled program. Researchers must hand-annotate the disassembly in a text editor to record their findings about the purpose of the code. Although running programs can change their layout dynamically, the disassembly can only show a snapshot of a program's layout. If a different view of a program is required, the document must be recreated from scratch, making it difficult to preserve user annotations. In this paper we demonstrate a system which allows a disassembly listing to be refined by user input while retaining user annotations. Users are able to dynamically change the interpretation of the layout of the program in order to effectively analyse programs which can alter their own memory layout. We allow users to combine the independent analysis of several program modules in order to examine the interaction between modules. By exploring the obsolete 'Poly' computer system, we demonstrate that our disassembler can be used to reconstruct and document entire software distributions.  Digital Libraries, Cognitive Aspects of Documents, Document Workflow
      </response>
      <response id="18" uni="University of South Australia" weight="0.18" year="2002">
        Towards a System to Improve Administrative Processes for Front-Line Academic Staff  Current  economic  pressures  are  causing  severe problems for many enterprises in maintaining service standards  with  shrinking  headcounts.  Groupware, Workflow  and  Agent  technologies  have  been  widely advocated  as  a  solution,  but  there  are  few  reported success  stories.  The  project  described  in  this  paper addresses  the  case  of  running  large  undergraduate courses. A preliminary vision of a possible integrated administrative  support  system  is  presented,  and  the future  activities  necessary  to  advance  such  a  vision are outlined.  Administrative  Applications,  User Interface Design, Software Agents
      </response>
      <response id="5" uni="The University of Sydney" weight="0.16" year="2002">
        Automatic Categorization of Announcements on the Australian Stock Exchange This paper compares the performance of several machine learning algorithms for the automatic categorization of corporate announcements in the Australian Stock Exchange (ASX) Signal G data stream. The article also describes some of the applications that the categorization of corporate announcements may enable. We have performed tests on two categorization tasks: market sensitivity, which indicates whether an announcement will have an impact on the market, and report type, which classifies each announcement into one of the report categories defined by the ASX. We have tried Neural Networks, a Naive Bayes classifier, and Support Vector Machines and achieved good results. Keywords Document Management, Document Workflow
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.14" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.12" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="1" uni="University of Queensland" weight="0.11" year="2002">
        XML-Based Offline Website Generation The approach and the tool XWeb, presented in this paper, shows one way to create websites from XML and other input _les which can then be uploaded onto standard web-servers. The system uses an extra input _le describing the structure of the content and the processes for creating the website. This information is also used to create the navigational elements in the output. Generating the content offline avoids having additional requirements on the server side such as CGI interfaces or Servlet engines. Document Management, XML, Hypermedia, Website Generation, Processing Model
      </response>
      <response id="150" uni="Dublin City University" weight="0.10" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="11" uni="The University of Queensland" weight="0.09" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="138" uni="National Taiwan University" weight="0.09" year="1997">
        A Multi-party Document Model  A multi-party document (MPD) is one that contains multiple parts intended for multiple recipients in that each recipient does not necessarily need to know the existence, and therefore the corresponding content, of the other parts of the document intended for other recipients. MPDs pose new requirements, namely, transparency and security, to the underlying system. This paper reviews the traditional document models and their shortcomings with respect to the stated requirements and proposes a new model that satisfies those requirements. The major implementation issues including document construction and encryption for MPD supports are also discussed. Shifting these capabilities to the system level will relieve burdens of users and enhance the security measure of the system for protecting the integrity and privacy of document contents. The issues discussed in this paper are important for making a sound underlying support system for the development of information systems, in particular in the Intranets environment. Keywords Document management, document structure, security, encryption, Intranets.
      </response>
    </responses>
  </theme>
  <theme id="16" title="Theme 16">
    <words>
      <word weight="3.21739491862">
        weight
      </word>
      <word weight="2.58545187446">
        set
      </word>
      <word weight="2.4217924029">
        retriev
      </word>
      <word weight="2.24368098282">
        structur
      </word>
      <word weight="1.95539243362">
        optim
      </word>
      <word weight="1.94068289581">
        queri
      </word>
    </words>
    <responses>
      <response id="25" uni="University of Otago" weight="2.96" year="2004">
        Optimal Structure Weighted Retrieval  Improving ranking functions for structured information retrieval has received much attention since the inception of XML. Weighting document structures is one method providing significant improvement - but how good can these improvements be? Optimal structure weighted retrieval occurs when each query is processed using the optimal set of weights for that query. Optimal retrieval for a set of queries occurs when a set of weights optimized for that set of queries is used. Measuring mean average precision for each of these will give a performance upper bound for document structure weighted retrieval. In this investigation a near optimal set of weights is learned for TREC WSJ collection topics 101-200 using a genetic algorithm. Weights are learned for vector space inner product, naive probability and BM25 ranking functions and a performance upper bound is calculated. The upper bound using a different set of weights for each query, gives mean average precision improvements of about 15% for BM25 and naive probability; about 30% for inner product. This suggests structure weighting might be useful for relevance feedback. Optimal weights for the set of queries shows improvements of about 5% for naive probability and inner product, but of only about 1% for BM25; suggesting this technique is not as effective for ad hoc retrieval.  Information Retrieval.
      </response>
      <response id="53" uni="Queensland University of Technology" weight="0.79" year="2006">
        Comparing XML-IR Query Formation Interfaces  XML information retrieval (XML-IR) systems differ from traditional information retrieval systems by using structure of XML documents to retrieve more specific units of information than the documents themselves. Users interact with XML-IR systems via structured queries that express their content and structural requirements. Historically, it has been common belief within the XML-IR community that structured queries will perform better than traditional keyword-only queries. However, recent system-orientated analysis has show that this assumption may be incorrect when system performance is averaged over a set of queries. Here, we test this assumption with users via a simulated work task experiment. We compare a keyword only interface with two user friendly XML-IR interfaces: NLPX, a natural language interface and Bricks, a query-bytemplate interface. This is the first time that a XML-IR natural language interface has been tested in user experiments. We compare the retrieval performance of all three interfaces and the usability of the two structured interfaces. Our results correspond to those of the system-orientated evaluation and indicate that structured queries do not aid retrieval performance. They also show that in terms of retrieval performance and usability the structured interfaces are comparable.  Users, Information Retrieval, XML
      </response>
      <response id="28" uni="RMIT University" weight="0.61" year="2004">
        Is CORI Effective for Collection Selection? An Exploration of Parameters, Queries, and Data  In distributed information retrieval, a wide range of techniques have been proposed for choosing collections to interrogate. Many of these collection-selection techniques are based on ranking the lexicons; of these, arguably the best known is the CORI collection ranking metric, which includes several parameters that, in principle, should be tuned for different data sets. However, parameters chosen in early work on CORI have been used without alteration in almost all subsequent work, despite drastic differences in the data collections. We have explored the behaviour of CORI for a range of data sets and parameter values. It appears that parameters cannot reliably be chosen for CORI: not only do the optimal choices vary between data sets, but they also vary between query types and, indeed, vary wildly within query sets. Coupled with the observation that even CORI with optimal parameters is usually less effective than other methods, we conclude that the use of CORI as a benchmark collection selection method is inappropriate. Keywords Lexicon indexing, distributed retrieval, information retrieval.
      </response>
      <response id="98" uni="University of Sydney" weight="0.37" year="2009">
        Feature Selection and Weighting Methods in Sentiment Analysis  Sentiment analysis is the task of identifying whether the opinion expressed in a document is positive or negative about a given topic. Unfortunately, many of the potential applications of sentiment analysis are currently infeasible due to the huge number of features found in standard corpora. In this paper we systematically evaluate a range of feature selectors and feature weights with both Naive Bayes and Support Vector Machine classifiers. This includes the introduction of two new feature selection methods and three new feature weighting methods. Our results show that it is possible to maintain a state-of-the art classification accuracy of 87.15% while using less than 36% of the features.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="147" uni="CSIRO, Division of Mathematical and Information Science, RMIT" weight="0.35" year="1998">
        Evaluation of Indexing Methods for Clustering  In order to synthesize a better answer based on the retrieved documents, we are exploring the use of clustering methods. In this paper, we present an evaluation of some popular indexing methods used for term selection and term weighting. The aim of indexing here is to represent documents, to relate documents with similar topics, and distinguish documents with different topics from each other. Experiments have been conducted to examine how the clustering results are influenced by some index term selection methods, such as the term selection based on the document frequency, and some term weighting methods, such as the inverted document frequency weight, the signal-noise ratio, and the term discrimination value, will influence the result of clustering. Based on our experiments, we recommend the use of the discrimination value weighting method together with a suitable set of indexing terms for the purpose of clustering the retrieved documents. Keywords Index, Term Selection, Term Weighting, Clustering.
      </response>
      <response id="62" uni="University of Otago" weight="0.34" year="2007">
        IR Evaluation Using Multiple Assessors per Topic  Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006. Traditionally the assessment of a single topic is performed by a single human. Herein we examine the consequences of using multiple assessors per topic. A set of 15 topics were used. The mean topic pool contained 98 documents. Between 3 and 5 separate assessors (per topic) assessed all documents in a pool. One assessor was designated baseline. All were then used to generate 10,000 synthetic multi-assessor assessment sets. The baseline relative rank order of all runs submitted to the INEX 2006 relevant-in-context task was compared to those of the synthetics. The mean Spearman's rank correlation coefficient was 0.986 and all coefficients were above 0.95 - the correlation is very strong. Non matching rank-orders are seen when the mean average precision difference between runs is less than 0.05. In the top 10 runs no significantly different runs were ranked in a different order in more than 5% of the synthetics. Using multiple assessors per topic is very unlikely to affect the outcome of an evaluation forum.  Information Retrieval
      </response>
      <response id="72" uni="RMIT University" weight="0.34" year="2007">
        Predicting Query Performance for User-based Search Tasks  Query performance prediction aims to determine in advance whether a user's search request will return a useful answer set. The success of such prediction attempts are currently evaluated by calculating the correlation between the predicted performance and standard information retrieval metrics of system performance such as average precision. However, recent work suggests that there is little relationship between average precision and the performance of users when carrying out search tasks. Direct measures of user performance offer another way of evaluating the effectiveness of search systems; this is of particular importance in the framework of query prediction, since one of the goals of prediction is to warn users when search results are likely to be poor. We therefore investigate the relationship between current prediction techniques and user-based performance measures. Our preliminary results show that the performance of the predictors differs strongly when using system-based compared to user-based performance measures: predictors that are significantly correlated with one measurement are often not correlated with the other. In general, the predictors are more correlated with average precision rather than with user performance.  Query performance prediction, information retrieval, user study
      </response>
      <response id="87" uni="NICTA Victoria Laboratory, The University of Melbourne" weight="0.33" year="">
        Parameter Sensitivity in Rank-Biased Precision  Rank-Biased Precision (RBP) is a retrieval evaluation metric that assigns an effectiveness score to a ranking by computing a geometricly weighted sum of document relevance values, with the monotonicly decreasing weights in the geometric distribution determined via a persistence parameter p. Despite exhibiting various advantageous traits over well known existing measures such as Average Precision, RBP has the drawback of requiring the designer of any experiment to choose a value for p. Here we present a method that allows retrieval systems evaluated using RBP with different p values to be compared. The proposed approach involves calculating two critical bounding relevance vectors for the original RBP score, and using those vectors to calculate the range of possible RBP scores for any other value of p. Those bounds may then be sufficient to allow the outright superiority of one system over the other to be established. In addition, the process can be modified to handle any RBP residuals associated with either of the two systems. We believe the adoption of the comparison process described in this paper will greatly aid the uptake of RBP in evaluation experiments.  Rank-Biased Precision, Evaluation, System Comparison
      </response>
      <response id="164" uni="Griffith University" weight="0.32" year="2000">
        Recovering Structure from Unstructured Web-accessible Classified Advertisements   This paper describes a research prototype system called RFCA for structuring Web-accessible rental classified advertisements based on semantic content. A hand crafted parser is used to extract various facets of the rental property being advertised including amongst others; member of room, type of garage, dwelling type (unit, house, or high rise apartment), price and contact details. The performance of the parser is measured in terms precision and recall by comparing its output to that of human expert. Tile structured information once, extracted is stored in a relational database and users searching for rental properties are presented with a graphical organisation of rental properties according to predefined themes. The overall result is a suite of tools for extracting, cleaning, structuring, and visually querying/lmnasing collection of web-accessible venial advertisements. The mathematical and. methodological foundation for the graphical organisation of the structured information is provided by fannul concept analysis. Using formal concept analysis each property is understood to be. an object possessing attributes with attribute values. The data is then conceptually organised via concept lattices dynamically according to ] ire-defined conceptual scales. The. concept lattice, organises rental properties into conceptual groupings. The, user then has the opportunity to view the attributes of all properties in a grouping as well as navigate back to the source advertisements. The. interface, is delivered over the web using a CGI interface and dynamic creation of image and image maps. The. ideas presented are general enough to be relevant to other web-accessible unstructured, text sources.
      </response>
      <response id="11" uni="The University of Queensland" weight="0.28" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="17" uni="RMIT University" weight="0.27" year="2002">
        Studying the Evolution of XML Document Structures  The structure of XML documents often evolve over time, leading to reformulation and new version releases of the document structure. This occurs independently of the different wways in which XML document structure can be expressed. Major structural cahnges can cause version incompatibility issues and ensuing resource costs of updating existing documents. Software applications may be required to accomodate to both the odd and new document structures. We present the case here for a study on the development process and evolution of XML document structures.   XML, XML Scheme, DTD, Document Management
      </response>
      <response id="29" uni="The University of Sydney" weight="0.25" year="2004">
        Co-training on Textual Documents with a Single Natural Feature Set  Co-training is a semi-supervised technique that allows classifiers to learn with fewer labelled documents by taking advantage of the more abundant unclassified documents. However, conventional cotraining requires the dataset to be described by two disjoint and natural feature sets that are redundantly sufficient. In many practical situations datasets have a single set of features and it is not obvious how to split it into two. This paper investigates the performance of co-training with only one natural feature set in two applications: Web page classification and email filtering. Text categorization, Web page classification, spam filtering, co-training
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.23" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="64" uni="CSIRO ICT Centre, Australian National University" weight="0.22" year="2007">
        Does brandname influence perceived search result quality? Yahoo!, Google, and WebKumara  Improving the quality of search engine results is the goal of costly efforts by major Web search engine companies. Using in situ side-by-side result set comparisons and random assignment of brandnames to result sets, we investigated whether perceptions of quality were influenced by brand association. In the first experiment (15 searchers) we found no significant preference for or against results labelled 'Google' relative to those labelled 'Yahoo!'. In the second experiment (20 searchers) result sets were again generated by Google and Yahoo! but were randomly labelled 'Yahoo!' or 'WebKumara' (a fictitious name). Again, we found no significant preference for one brandname label over the other. Contrary to previous findings, we found a statistically significant preference for Googlegenerated results over those of Yahoo! when data from three separate experiments (total 70 subjects) was combined.  Information retrieval
      </response>
      <response id="20" uni="The University of Melbourne" weight="0.22" year="2002">
        Vector Space Ranking: Can We Keep it Simple?  The vector-space model is used widely for document retrieval, based upon the TF-IDF rule for calculating similarity scores between a set of documents and a query. One of the drawbacks of this approach is the need to select a specific formulation for the similarity computation. Here we present an initial attempt to simplify the heuristic, by hiding the various detailed calculations, and evaluating the term importance qualitatively rather than quantitatively. A new technique, called local reordering is introduced. Local reordering still relies on the vector-space model, as it employs a scalar vector product for calculating similarity scores. But there is no longer a requirement for precise values of the document or query vectors to be determined. Initial experiments on two data sets shows that it is highly competitive in terms of retrieval effectiveness. As a useful side effect, the method allows extremely fast query processing.  Information retrieval, text indexing, vectorspace ranking, similarity heuristic.
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.16" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="144" uni="RMIT" weight="0.16" year="1997">
        Collection Selection via Lexicon Inspection  A distributed text database consists of multiple individual text collections. When a query is posed to a distributed text database, significant computational resources can be saved by identifing the individual collections that are the most likely to contain answers, as unnecessary accesses to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be effective, but depends on how performance is measured.
      </response>
      <response id="44" uni="University of Otago" weight="0.16" year="2005">
        Recommending Geocaches  Players downloading GPS coordinates from the internet, hiking to the given spot, and hunting for a hidden box - this is the new sport of geocaching. Today there are nearly 200,000 such boxes in over 200 countries. With so many to find, a recommender is needed, one that takes into account not only the boxes, but also the geospatial and temporal nature of the sport. A database of geocaches in the South Island of New Zealand is made by trawling a prominent geocaching web site. This is then used to estimate the home-coordinates (geospatial playing centre) of players. Predictions are verified against a set of correct coordinates solicited from players. Several geocache recommenders are discussed and compared. The precision, computed using mean of mean reciprocal rank (MMRR), of each is measured. The best method tried is a collaborative filter using intersection over mean to find similar players and a voting scheme to recommend geocaches. This method is proposed as a replacement for the currently used distance from home-coordinate; doing so will increase the precision of existing systems such as geocaching. com.  Information Retrieval.
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.15" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="26" uni="The University of Melbourne" weight="0.15" year="2004">
        Collection-Independent Document-Centric Impacts  An information retrieval system employs a similarity heuristic to estimate the probability that documents and queries match each other. The heuristic is usually formulated in the context of a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this - it means that impact-sorted inverted indexes can be built with just a single parse of the source text. Information Retrieval.
      </response>
      <response id="74" uni="RMIT University" weight="0.15" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="43" uni="The University of Sydney" weight="0.14" year="2005">
        Cross Training and Under Sampling in Categorization of Company Announcements  To process the documents in a share market is crucial. It is because financial activities are socio-economic driven and text documents contain a lot of valuable information. In this paper, we focus on one of these documents, the Company Announcement. Each of these documents requires to be labelled as price sensitive or not before presenting to the general public. In our experiments, we study two specific issues in this text categorization, namely the effectiveness of a feature vector obtained from the corpus belonging to another market sector and the imbalanced nature of the dataset. Our results indicate that the classification can benefit from a different (but related) set of corpus because of a more diversified and generalised nature of the feature set. Regarding the skewness of the dataset, the under-sampling of the majority class in the training process does not have a strong effect on the performance in the test set, while keeping the computational cost minimised.  Document Management, Text Categorization
      </response>
      <response id="105" uni="University of Otago" weight="0.14" year="2010">
        Extricating Meaning from Wikimedia Article Archives  Wikimedia article archives (Wikipedia, Wiktionary, and so on) assemble open-access, authoritative corpora for semantic-informed datamining, machine learning, information retrieval, and natural language processing. In this paper, we show the MediaWiki wikitext grammar to be context-sensitive, thus precluding application of simple parsing techniques. We show there exists a worst-case bound on time complexity for all fully compliant parsers, and that this bound makes parsing intractable as well as constituting denial-of-service (DoS) and degradation-of-service (DegoS) attacks against all MediaWiki wikis. We show there exists a worse-case bound on storage complexity for fully compliant onepass parsing, and that contrary to expectation such parsers are no more scalable than equivalent two-pass parsers. We claim these problems to be the product of deficiencies in the MediaWiki wikitext grammar and, as evidence, comparatively review 10 contemporary wikitext parsers for noncompliance with a partially compliant Parsing Expression Grammar (PEG).  Document Standards, Information Retrieval, Web Documents, Wikipedia
      </response>
      <response id="117" uni="Queensland University of Technology, CSIRO" weight="0.13" year="2010">
        Rule-based Approach for Identifying Assertions in Clinical Free-Text Data  A rule-based approach for classifying previously identified medical concepts in the clinical free text into an assertion category is presented.There are six different categories of assertions for the task: Present, Absent, Possible, Conditional, Hypothetical and Not associated with the patient. The assertion classification algorithms were largely based on extending the popular NegEx and Context algorithms. In addition, a health based clinical terminology called SNOMED CT and other publicly available dictionaries were used to classify assertions, which did not fit the NegEx/Context model. The data for this task includes discharge summaries from Partners HealthCare and from Beth Israel Deaconess Medical Centre, as well as discharge summaries and progress notes from University of Pittsburgh Medical Centre. The set consists of 349 discharge reports, each with pairs of ground truth concept and assertion files for system development, and 477 reports for evaluation. The system's performance on the evaluation data set was 0.83, 0.83 and 0.83 for recall, precision and F1-measure, respectively. Although the rule-based system shows promise, further improvements can be made by incorporating machine learning approaches.  rule-based, medical concept, assertion, NegEx, Context, SNOMED CT.
      </response>
      <response id="125" uni="RMIT" weight="0.13" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="22" uni="RMIT University" weight="0.12" year="2002">
        Improved use of Contextual Information in Cross-language Information Retrieval  In this paper, we explore Dictionary based context sensitive translation, a framework for query translation to reduce the translation ambiguity and improve the translation quality in English to Chinese cross-language information retrieval (CLIR). Our paper explores the effect of the context window size on translation effectiveness. We assume that the correct translations of the query key terms tend to co-occur together at a high frequency and incorrect translations do not. Our experimental results showed that when using a window size of 10, context sensitive translation results in a dramatic improvement in retrieval performance, it brings about a 30% improvement compared to the results of previous Dictionary based approaches that used only Immediately adjacent words for context.
      </response>
      <response id="84" uni="RMIT University" weight="0.11" year="2008">
        The Effect of Using Pitch and Duration for Symbolic Music Retrieval  Quite reasonable retrieval effectiveness is achieved for retrieving polyphonic (multiple notes at once) music that is symbolically encoded via melody queries, using relatively simple pattern matching techniques based on pitch sequences. Earlier work showed that adding duration information was not particularly helpful for improving retrieval effectiveness. In this paper we demonstrate that defining the duration information as the time interval between consecutive notes does lead to more effective retrieval when combined with pitch-based pattern matching in our collection of over 14 000 MIDI files.  Music information retrieval, Information retrieval, Multimedia resource discovery, Pattern matching
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.11" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="37" uni="The University of Melbourne" weight="0.11" year="2005">
        In Search of Reliable Retrieval Experiments  There are several ways in which an 'improved' technique for solving some computational problem can be defended: by mathematical argument; by simulation; and by experimental validation. Each of these has risks. In this paper we describe some of the issues that arose during an experimental validation of architectures for distributed text query evaluation, and the approaches that were taken to resolve them. In particular, collections and clusters must be scaled in a way that maximizes comparability between different data sizes; query sets must be appropriate to the target collection; and hardware issues such as file placement on disk must also be considered. Our intention is to report on our experience in a practical sense, and thereby assist others to avoid the same problems.
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.10" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.10" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="49" uni="Queensland University of Technology" weight="0.09" year="2006">
        Preliminary Investigations into Ontology-based Collection Selection  This article tackles the collection selection problem from the query side. Queries are enhanced by mapping them to subjects in an ontology; the associated subject classification terms are then employed to retrieve collections. An experimental comparison was performed with the state of the art ReDDE system, which relies on estimates of collection size to rank collections. Although the research is preliminary, there is some support to the hypothesis that this approach mitigates the need for collection size estimates in collection selection.  Information Retrieval, Document Databases, Digital Libraries
      </response>
      <response id="31" uni="Carnegie Mellon University, University of Sydney" weight="0.09" year="2004">
        Phrases and Feature Selection in E-Mail Classification  In this paper we study the effectiveness of using a phrase-based representation in e-mail classification, and the affect this approach has on a number of machine learning algorithms. We also evaluate various feature selection methods and reduction levels for the bag-of-words representation on several learning algorithms and corpora. The results show that the phrasebased representation and feature selection methods can be used to increase the performance of e-mail classifiers.  E-Mail Classification, Text Categorization, Feature Selection
      </response>
      <response id="141" uni="Monash University" weight="0.09" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="155" uni="RMIT" weight="0.09" year="1999">
        On Using Hierarchies for Document Classification  Good management of large collections, such as world-wide web databases or newswire services, is essential to ensure that they remain useful resources. Large collection management tasks include storing, querying, retrieving, routing, filtering, and classifying documents. We focus in this paper on new approaches to the last of these tasks, classification. Classification is the process of assigning one or more identifiers from a list of classes to a document. The identifier or class label is useful to organise, retrieve, or present documents. Several factors affect the effectiveness of classification schemes, including the classification method, selection of training samples, selection of features, and class label assignment methods. We identify problems in classification, propose a new evaluation framework, and show that using hierarchical information, where parent classes and subclasses of labels are used, has potential to improve classification effectiveness. Document Management, Document Databases, Document Classification, Information Retrieval, SGML and Markup.
      </response>
      <response id="51" uni="University of Sydney" weight="0.09" year="2006">
        A Sequence Based Recommender System for Learning Resources  This paper presents a novel approach for recommending sequences of resources for users to view based on previous user feedback. It considers the order in which resources are viewed to be important in delivering the next set of suggestions and tries to learn these dependencies from users' ratings. Although we describe our approach in the context of e-learning, it can be applied to other domains where ordering is important. We also propose a novel algorithm for learning the dependencies between the resources. Preliminary results are encouraging: they show that, after a threshold in quantity of feedback, our algorithm provides better results than standard collaborative filtering.  Digital Libraries, Document Management, Information Retrieval
      </response>
    </responses>
  </theme>
  <theme id="17" title="Theme 17">
    <words>
      <word weight="3.14702915089">
        screen
      </word>
      <word weight="3.03244745748">
        search
      </word>
      <word weight="2.32596133432">
        result
      </word>
      <word weight="2.07364190216">
        small
      </word>
      <word weight="1.83378215485">
        scan
      </word>
      <word weight="1.55496572581">
        behaviour
      </word>
    </words>
    <responses>
      <response id="134" uni="The Australian National University, CSIRO ICT Centre" weight="3.55" year="2012">
        Comparing scanning behaviour in web search on small and large screens  Although web search on mobile devices is common, little is known about how users read search result lists on a small screen. We used eye tracking to compare users' scanning behaviour of web search engine result pages on a small screen (hand-held devices) and a large screen (desktops or laptops). The objective was to determine whether search result pages should be designed differently for mobile devices. To compare scanning behaviour, we considered only the fixation time and scanning strategy using our new method called 'Trackback'. The results showed that on a small screen, users spend relatively more time to conduct a search than they do on a large screen, despite tending to look less far ahead beyond the link that they eventually select. They also show a stronger tendency to seek information within the top three results on a small screen than on a large screen. The reason for this tendency may be difficulties in reading and the relative location of page folds. The results clearly indicated that scanning behaviour during web search on a small screen is different from that on a large screen. Thus, research efforts should be invested in improving the presentation of search engine result pages on small screens, taking scanning behaviour into account. This will help provide a better search experience in terms of search time, accuracy of finding correct links, and user satisfaction.   Scanning behaviour, small screen, Trackback
      </response>
      <response id="64" uni="CSIRO ICT Centre, Australian National University" weight="0.47" year="2007">
        Does brandname influence perceived search result quality? Yahoo!, Google, and WebKumara  Improving the quality of search engine results is the goal of costly efforts by major Web search engine companies. Using in situ side-by-side result set comparisons and random assignment of brandnames to result sets, we investigated whether perceptions of quality were influenced by brand association. In the first experiment (15 searchers) we found no significant preference for or against results labelled 'Google' relative to those labelled 'Yahoo!'. In the second experiment (20 searchers) result sets were again generated by Google and Yahoo! but were randomly labelled 'Yahoo!' or 'WebKumara' (a fictitious name). Again, we found no significant preference for one brandname label over the other. Contrary to previous findings, we found a statistically significant preference for Googlegenerated results over those of Yahoo! when data from three separate experiments (total 70 subjects) was combined.  Information retrieval
      </response>
      <response id="124" uni="Queensland University of Technology, Semantic Identity, The University of Southern Queensland" weight="0.35" year="2011">
        An Ontology-based Mining Approach for User Search Intent Discovery  Discovering proper search intents is a vital process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this paper, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate concept levels for matching user search intents. An iterative mining algorithm is designed for evaluating potential intents level by level until meeting the best result. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.  Ontology mining, Search intent, LCSH, World knowledge
      </response>
      <response id="11" uni="The University of Queensland" weight="0.31" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="46" uni="Melbourne" weight="0.28" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="151" uni="University of New South Wales, Murdoch University, University of Incheon, Korea" weight="0.28" year="1999">
        Reader's Preferences in the Formats of Web-based Academic Articles  No standard format exists for the many academic articles available on the Web and little is known about user reading patterns. This paper explores these issues using data from two online surveys: one email-based, the other Web-based. Our results suggests that people take an overview from the screen, and then, if they are interested in an article, print it out in order to read it properly. The simple two-frame format was regarded as the best by 47% of the respondents, whereas the cascade page format was regarded as the worst by 65% of the respondents. Interestingly, 26% considered the paper-like format, widely used in Web-based articles, to be the worst. Different results were obtained when interactive examples were embedded in the survey. Keywords: Web-based articles; Reading patterns and formats; Web-based survey; Digital library
      </response>
      <response id="97" uni="University of Otago" weight="0.27" year="2009">
        University Student Use of the Wikipedia  The 2008 proxy log covering all student access to the Wikipedia from the University of Otago is analysed. The log covers 17,635 student users for all 366 days in the year, amounting to over 577,973 user sessions. The analysis shows the Wikipedia is used every hour of the day, but seasonally. Use is low between semesters, rising steadily throughout the semester until it peaks at around exam time. The analysis of the articles that are retrieved as well as an analysis of which links are clicked shows that the Wikipedia is used for study-related purposes. Medical documents are popular reflecting the specialty of the university. The mean Wikipedia session length is about a minute and a half and consists of about three clicks. The click graph the users generated is compared to the link graph in the Wikipedia. In about 14% of the user sessions the user has chosen a sub-optimal path from the start of their session to the final document they view. In 33% the path is better than optimal suggesting that users prefer to search than to follow the link-graph. When they do click, they click links in the running text (93.6%) and rarely on 'See Also' links (6.4%), but this bias disappears when the frequency of these types of links' occurrence is corrected for. Several recommendations for changes to the link discovery methodology are made. These changes include using highly viewed articles from the log as test data and using user clicks as user judgements.  Information Retrieval, Link Discovery.
      </response>
      <response id="122" uni="Queensland University of Technology, University of Otago" weight="0.25" year="2011">
        Mobile Applications of Focused Link Discovery  Interaction with a mobile device remains difficult due to inherent physical limitations. This difficulty is particularly evident for search, which requires typing. We extend the One-Search-Only search paradigm by adding a novel link-browsing scheme built on top of automatic link discovery. A prototype was built for iPhone and tested with 12 subjects. A post-use interview survey suggests that the extended paradigm improves the mobile information seeking experience.  Focused Link Discovery, Wikipedia, Mobile Information Seeking, User Studies Involving Documents.
      </response>
      <response id="54" uni="CSIRO ICT Centre" weight="0.25" year="2006">
        InexBib - Retrieving XML elements based on external evidence  Creating a scientific bibliography on a given topic is currently a task which requires a great deal of manual effort. We attempt to reduce this effort by developing a tool for automatically generating a bibliography from a collection of articles represented in XML. We evaluate the use of elements around the references as anchortexts to improve search results. We find that users of the tool prefer lists generated using anchortext over those generated from the bibliography entry only and that the preference is statistically significant. We tentatively find no significant preference for results generated using paragraph as opposed to sentence level anchortext, but note that this finding may result from lack of sophistication in resolving text including multiple references.  Information Retrieval, XML, Element Retrieval, Bibliography
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.22" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="133" uni="Funnelback" weight="0.19" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="19" uni="Australian National University  CSIRO Mathematical and Information Sciences" weight="0.17" year="2002">
        Buying bestsellers online: A case study in Search &amp; Searchability  A website's design directly affects how well search engines can crawl, match and rank its pages. For this reason, searchability is an important concern in site design. We study the interaction between search engines and Web sites by means of a case study of online bookstores and general-purpose search engines. The task modelled is that of finding web pages from which a book, described by its title, may be purchased. We first compared the relative effectiveness of search engines in finding pages matching the criterion, regardless of bookstore. Then we compared the relative searchability of the bookstore websites by observing how many times each bookstore contributed useful answers to the search results. Large differences in the performance of both search engines and bookstores were observed. Two of the search engines performed better than their peers, and one bookstore was far more searchable than all others. To further explore these differences we tabulate the total number of pages from each bookshop which are included in the search engine indexes. We conclude with recommendations both to bookstores on how they may improve their Web presence, and to search engines on how they may improve their performance for product searches.  Information Retrieval Web search, evaluation, transactional search
      </response>
      <response id="94" uni="RMIT University" weight="0.14" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="114" uni="RMIT University" weight="0.13" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="109" uni="RMIT University" weight="0.12" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="104" uni="Queensland University of Technology, University of Otago" weight="0.12" year="2009">
        The Methodology of Manual Assessment in the Evaluation of Link Discovery  The link graph extracted from the Wikipedia has often been used as the ground truth for measuring the performance of automated link discovery systems. Extensive manual assessments experiments at INEX 2008 recently showed that this is unsound and that manual assessment is essential. This paper describes the methodology for link discovery evaluation which was developed for use in the INEX 2009 Link-the-Wiki track. In this approach both manual and automatic assessment sets are generated and runs are evaluated using both. The approach offers a more reliable evaluation of link discovery methods than just automatic assessment. A new evaluation measure for focused link discovery is also introduced.  Wikipedia, Link Quality, Manual Assessment, Evaluation.
      </response>
      <response id="47" uni="ICT Centre CSIRO" weight="0.11" year="2006">
        Improving rankings in small-scale web search using click-implied descriptions    When a searcher submits a query Q and clicks on document R in the corresponding result set, we may plausibly interpret the click as a vote that Q is a description of R. We call the Q and R pairing a 'click description'. Click descriptions thus derived from search engine logs can be accumulated into surrogate documents and used to boost retrieval effectiveness in a similar fashion to anchor text. We investigate the usefulness of click description surrogate documents in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92% improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link  Information Storage and Retrieval, Content Analysis and Indexing [Indexing methods]
      </response>
      <response id="101" uni="University of Sydney" weight="0.10" year="2009">
        An Automatic Question Generation Tool for Supporting Sourcing and Integration in Students' Essays   This paper presents a domain independent Automatic Question Generation (AQG) tool that generates questions which can be used as a form of support for students to revise their essay. The focus here is on generating questions based on semantic and syntactic information acquired from citations. The semantic information includes the author's name, the citation type (describing the aim of the cited study, its results or an opinion), the author's expressed sentiment, and the syntactic information of the citation. Pedagogically, the question templates are designed using Bloom's learning taxonomy where the questions reach the Analysis Level. We used 40 undergraduate students essays for our experiment and the Name Entity Recognition component is trained on 20 essays. The result of our experiment shows that the question coverage is 96% and accuracy of generated questions can reach 78%. This AQG tool will be integrated into our peer review system to scaffold feedback from peers.  Question Generation, Electronic Feedback System for Sourcing and Integration in Students' Essay
      </response>
      <response id="95" uni="Queensland University of Technology" weight="0.10" year="2009">
        Random Indexing K-tree   Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using Ktree. The algorithms and data structures are defined, explained and motivated. Specific modifications to Ktree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm. Keywords Random Indexing, K-tree, Dimensionality Reduction, B-tree, Search Tree, Clustering, Document Clustering, Vector Quantization, k-means
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.09" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
    </responses>
  </theme>
  <theme id="18" title="Theme 18">
    <words>
      <word weight="6.51151549181">
        word
      </word>
      <word weight="1.94731812591">
        document
      </word>
      <word weight="1.80286580484">
        text
      </word>
      <word weight="1.66516762746">
        wst
      </word>
      <word weight="1.46285038027">
        categor
      </word>
      <word weight="1.43738503407">
        segment
      </word>
    </words>
    <responses>
      <response id="108" uni="Gunma University" weight="1.39" year="2010">
        Composition and Decomposition of Japanese Katakana and Kanji Morphemes for Decision Rule Induction from Patent Documents  We propose a new method to construct a word list for rule induction from Japanese patent documents. For word segmentation in Japanese, statistical morphological analyzers have been used in many applications. However, the output of these morphological analyzers presents defects when analyzing unknown words, specifically words that contain Kanji/Katakana morphemes. Some words are overly segmented, and their original meanings are obscured. Furthermore, boundaries between compound nouns are uncertain, which impedes investigation in the initial stages of the application. In our method, we first perform morphological analysis to segment sentences into morphemes. Second, segmented compound words are filtered by character types and Katakana/Kanji morphemes in the compound words are concatenated. Third, the concatenated morphemes are truncated to reduce verbosity. Then, words comprising Katakana/Kanji are retained for use in a word list for rule induction. The experiment results show that our method is effective for extracting decision rules for patent classification.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="99" uni="NICTA VRL, National University of Singapore" weight="1.33" year="2009">
        The Use of Topic Representative Words in Text Categorization  We present a novel way to identify the representative words that are able to capture the topic of documents for use in text categorization. Our intuition is that not all word n-grams equally represent the topic of a document, and thus using all of them can potentially dilute the feature space. Hence, our aim is to investigate methods for identifying good indexing words, and empirically evaluate their impact on text categorization. To this end, we experiment with five different word sub-spaces: title words, first sentence words, keyphrases, domain-specific words, and named entities. We also test TF-IDF-based unsupervised methods for extracting keyphrases and domain-specific words , and empirically verify their feasibility for text categorization. We demonstrate that using representative words outperforms a simple 1-gram model.  Natural Language Techniques and Documents, Text Categorization
      </response>
      <response id="150" uni="Dublin City University" weight="1.12" year="1998">
        User-Mediated Word Shape Tokens for Querying Document Images  Word Shape Tokens (WSTs) are tokens used to represent words based on the overall shape or contour of a word as it appears in printed text. A character shape code (CSC) mapping function is used to aggregate similarly shaped letters such as &quot;g&quot; and &quot;y&quot; into one single code to represent those letters. The rationale behind this is that it is far easier and more accurate to map a scanned image of a word or letter into its WST representation than it is to map into full ASCII- WSTs were initially applied to the task of language recognition and have proved useful in implementing a computationally lightweight form of OCR- In previous work, we have applied WST representations to information retrieval based on automatically deriving query WSTs from topic descriptions. In the work reported here we extend this to allow a user to judiciously select WSTs as search terms based on the number of surface forms of words which share that WST. We also factor into our experiments for the first time, the WST recognition errors found from an implementation of the WST recognition process. Our results encourage us to further develop the idea of using WSTs for retrieving scanned images of text documents. Document management; Retrieval of document images;
      </response>
      <response id="100" uni="Queensland University of Technology University of Otago" weight="0.69" year="2009">
        Word Segmentation for Chinese Wikipedia Using N-Gram Mutual Information  In this paper, we propose an unsupervised segmentation approach, named &quot;n-gram mutual information&quot;, or NGMI, which is used to segment Chinese documents into n-character words or phrases, using langauge statistics drwan from the hinese Wikipedia corpus. This approach alleviates the themendous effor that is required in preparing and maintaining the manually segmented Chinese test for training purposes, and manually maintaining ever expanding lexicons. Previously, mutual information was used to achieve automated segmentation into 2-character words. The NGMI approach extends the approach to handle longer n-character words. Experiments with heterogeneous documents from the Chinese Wikipedia collection show good results.   Chinese word segmentation, mutual information, n-gram mutual information, boundary confidence
      </response>
      <response id="152" uni="Griffith University" weight="0.54" year="1999">
        DYNAMIC HYPER-LINKING BY QUERYING FOR A FCA-BASED QUERY SYSTEM  This paper presents a mechanism for hyper-linking documents by search-terms. Search-terms are selected by the user interactively building a formal concept lattice. In order to explain this interface we give some background to Formal Concept Analysis and an example is developed which illustrates the use of the concept lattice. Selected search-terms are used to create hyper-links, based on term repetition. As the search-terms differ between queries, we need a mechanism from which to dynamically create the target hyper-linked HTML documents. Therefore, documents are stored in a structure which is based on a word-list rather than plain text format. The documents are represented as links between the individual words within the word-list In so doing the word-list becomes a full-text-retrieval index into each word in each of those documents and therefore provides a good basis for the fast creation of an HTML document set from specific queries by keywords. To have the words in a word-list from which the documents are created also allows easy classification of words which should be hyper-linked within specific HTML documents. Furthermore, both documents and hyper-linking keywords are stored as well in this structure since any word in any document is indexed by the word-list. Document Databases, WWW and Internet.
      </response>
      <response id="36" uni="RMIT" weight="0.37" year="2005">
        Readability of French as a Foreign Language and its Uses  Reading is an important means of foreign language acquisition, particularly for vocabulary. Providing reading material that is of a suitable level of difficulty allows users to acquire vocabulary the most efficiently. Thus an on-line reading material recommender system for language learners requires a readability measure so that the difficulty of texts can be automatically assessed. However, most readability measures were developed for native child speakers of English. In this article I discuss an experiment in readability for learners of French. I conclude that using the average number of words per sentence correlates more closely with human judgements than many commonly available readability measures. I propose a new readability measure for learners of French that have English as their main language, which combines sentence length with the number of words that are similar in both languages (cognates). This measure slightly improves on sentence length for modelling French readability.  Text readability, Information retrieval
      </response>
      <response id="162" uni="University of Sydney" weight="0.35" year="2000">
        Keyword Association Network: A Statistical Multi-term Indexing Approach for Document Categorization  A Keyword Association Network (KAN) is the network of keywords extracted from a collection of documents. In this network, the relationship between keywords is represented by a confidence value. It is argued in this paper thai the semantics and importance of a word can be more clearly and accurately measured by making use of other words that are co-occurring in a given document. The term frequency used for measuring the importance of terms in most document categorization methods ignores this important aspect. A KAN is constructed on the basis of co-occurring terms in documents. If two tenns appear more than a certain number of times in the same documents, they are considered as having close relationship. This paper proposes using KAN as a basis for finding informative keywords and using a confidence value in the process of document categorization. The process of constructing and application of KAN for document categorization is presented and the performance comparison with a typical statistical single-term document categorization algorithm - TFIDF classifier - will be shown. The experimental results show that KAN gives significant benefits. Keywords Document Categorization, Machine Learning. Statistical Multi-term indexing, Semantic- Meaning.
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.35" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="46" uni="Melbourne" weight="0.27" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="125" uni="RMIT" weight="0.25" year="2011">
        The Interplay of Information Retrieval and Query by Singing with Words  Speech recognition can be used in music retrieval systems to identify the words in users' sung queries. Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words. We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system. The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct. However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.  Pattern Matching, Ranking, Speech Recognition,Music Information Retrieval.
      </response>
      <response id="146" uni="RMIT" weight="0.23" year="1997">
        Conflation-based Comparison of Stemming Algorithms  In text database systems, query terms are stemmed to allow them to be conflated with variant forms of the same word. On the one hand, stemming allows the query mechanism to find documents that would otherwise not contain matches to the query terms; on the other hand, automatic stemming is prone to error, and can lead to retrieval of inappropriate documents. In this paper we investigate several stemming algorithms, measuring their ability to correctly conflate terms from a large text collection. We show that stemming is indeed worthwhile, but that each of the stemming algorithms we consider has distinct advantages and disadvantages; choice of stemming algorithm affects the behaviour of the retrieval mechanism. information retrieval, document databases, digital libraries, word disambiguation.
      </response>
      <response id="147" uni="CSIRO, Division of Mathematical and Information Science, RMIT" weight="0.18" year="1998">
        Evaluation of Indexing Methods for Clustering  In order to synthesize a better answer based on the retrieved documents, we are exploring the use of clustering methods. In this paper, we present an evaluation of some popular indexing methods used for term selection and term weighting. The aim of indexing here is to represent documents, to relate documents with similar topics, and distinguish documents with different topics from each other. Experiments have been conducted to examine how the clustering results are influenced by some index term selection methods, such as the term selection based on the document frequency, and some term weighting methods, such as the inverted document frequency weight, the signal-noise ratio, and the term discrimination value, will influence the result of clustering. Based on our experiments, we recommend the use of the discrimination value weighting method together with a suitable set of indexing terms for the purpose of clustering the retrieved documents. Keywords Index, Term Selection, Term Weighting, Clustering.
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.18" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="141" uni="Monash University" weight="0.17" year="1997">
        An experimental study of moment invariants and Fourier descriptors for shape based image retrieval  Retrieval of images based on object shape is one of the most challenging aspects of content based image retrieval systems. In this paper we describe Fourier descriptors and moment invariants for shape based image retrieval and present results of an experimental study of the performance of the two techniques. The comparison between these two methods is done by indexing the shapes in a database for both the methods and making the same queries for both the methods. It is found that both the methods are comparable. shape representation, image retrieval, pattern recognition, moment invariants, Fourier descriptors
      </response>
      <response id="21" uni="The University of Sydney" weight="0.16" year="2002">
        A Framework for Text Categorization In this paper we discuss the architecture of an object-oriented application framework (OOAF) for text categorization. We describe the system requirements and the software engineering strategies that form the basis of the design and implementation of the framework. We show how designing a highly reusable OOAF architecture facilitates the development of new applications. We also highlight the key text categorization features of the framework, as well as practical considerations for application developers. Document Management, Text Categorization, Application Frameworks
      </response>
      <response id="93" uni="Queensland University of Technology" weight="0.16" year="2009">
        Interestingness Measures for Multi-Level Association Rules  Association rule mining is one technique that is widely used when querying databases, especially those that are transactional, in order to obtain useful associations or correlations among sets of items. Much work has been done focusing on efficiency, effectiveness and redundancy. There has also been a focusing on the quality of rules from single level datasets with many interestingness measures proposed. However, with multi-level datasets now being common there is a lack of interestingness measures developed for multi-level and cross-level rules. Single level measures do not take into account the hierarchy found in a multi-level dataset. This leaves the Support-Confidence approach, which does not consider the hierarchy anyway and has other drawbacks, as one of the few measures available. In this paper we propose two approaches which measure multi-level association rules to help evaluate their interestingness. These measures of diversity and peculiarity can be used to help identify those rules from multi-level datasets that are potentially useful.  Information Retrieval, Interestingness Measures, Association Rules, Multi-Level Datasets
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.13" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="31" uni="Carnegie Mellon University, University of Sydney" weight="0.12" year="2004">
        Phrases and Feature Selection in E-Mail Classification  In this paper we study the effectiveness of using a phrase-based representation in e-mail classification, and the affect this approach has on a number of machine learning algorithms. We also evaluate various feature selection methods and reduction levels for the bag-of-words representation on several learning algorithms and corpora. The results show that the phrasebased representation and feature selection methods can be used to increase the performance of e-mail classifiers.  E-Mail Classification, Text Categorization, Feature Selection
      </response>
      <response id="98" uni="University of Sydney" weight="0.12" year="2009">
        Feature Selection and Weighting Methods in Sentiment Analysis  Sentiment analysis is the task of identifying whether the opinion expressed in a document is positive or negative about a given topic. Unfortunately, many of the potential applications of sentiment analysis are currently infeasible due to the huge number of features found in standard corpora. In this paper we systematically evaluate a range of feature selectors and feature weights with both Naive Bayes and Support Vector Machine classifiers. This includes the introduction of two new feature selection methods and three new feature weighting methods. Our results show that it is possible to maintain a state-of-the art classification accuracy of 87.15% while using less than 36% of the features.  Information Retrieval, Natural Language Techniques and Documents
      </response>
      <response id="16" uni="Macquarie University" weight="0.11" year="2002">
        How to Write a Document in Controlled Natural Language  This paper shows how a computer-processable document can be written in a controlled natural language (PENG) with the help of a sophisticated lookahead editor (ECOLE). The editor provides syntactic hints after each word form entered and indicates how the author can continue the text. This way the author does not need to learn or to remember the restrictions of the controlled language. PENG documents are automatically translated into first-order logic via discourse representation structures. These formal entities can be checked by a theorem prover for inconsistency or consistency can be revealed by a model builder.  Document Processing, Controlled Languages, Authoring Tools.
      </response>
      <response id="58" uni="Queensland University of Technology" weight="0.11" year="2006">
        Enhanced web-based translation extraction for English- Chinese CLIR  Dictionary based translation is a traditional approach in use by cross-language information retrieval systems. However, significant performance degradation is often observed when queries contain words that do not appear in the dictionary. This is called the Out of Vocabulary (OOV) problem. The common methods for translation selection for web-based translation always rely on word frequency calculation but the results are not always satisfactory. Our experiments show marked improvement in translation accuracy over other commonly used approaches.
      </response>
      <response id="43" uni="The University of Sydney" weight="0.11" year="2005">
        Cross Training and Under Sampling in Categorization of Company Announcements  To process the documents in a share market is crucial. It is because financial activities are socio-economic driven and text documents contain a lot of valuable information. In this paper, we focus on one of these documents, the Company Announcement. Each of these documents requires to be labelled as price sensitive or not before presenting to the general public. In our experiments, we study two specific issues in this text categorization, namely the effectiveness of a feature vector obtained from the corpus belonging to another market sector and the imbalanced nature of the dataset. Our results indicate that the classification can benefit from a different (but related) set of corpus because of a more diversified and generalised nature of the feature set. Regarding the skewness of the dataset, the under-sampling of the majority class in the training process does not have a strong effect on the performance in the test set, while keeping the computational cost minimised.  Document Management, Text Categorization
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.10" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
      <response id="62" uni="University of Otago" weight="0.09" year="2007">
        IR Evaluation Using Multiple Assessors per Topic  Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006. Traditionally the assessment of a single topic is performed by a single human. Herein we examine the consequences of using multiple assessors per topic. A set of 15 topics were used. The mean topic pool contained 98 documents. Between 3 and 5 separate assessors (per topic) assessed all documents in a pool. One assessor was designated baseline. All were then used to generate 10,000 synthetic multi-assessor assessment sets. The baseline relative rank order of all runs submitted to the INEX 2006 relevant-in-context task was compared to those of the synthetics. The mean Spearman's rank correlation coefficient was 0.986 and all coefficients were above 0.95 - the correlation is very strong. Non matching rank-orders are seen when the mean average precision difference between runs is less than 0.05. In the top 10 runs no significantly different runs were ranked in a different order in more than 5% of the synthetics. Using multiple assessors per topic is very unlikely to affect the outcome of an evaluation forum.  Information Retrieval
      </response>
      <response id="5" uni="The University of Sydney" weight="0.09" year="2002">
        Automatic Categorization of Announcements on the Australian Stock Exchange This paper compares the performance of several machine learning algorithms for the automatic categorization of corporate announcements in the Australian Stock Exchange (ASX) Signal G data stream. The article also describes some of the applications that the categorization of corporate announcements may enable. We have performed tests on two categorization tasks: market sensitivity, which indicates whether an announcement will have an impact on the market, and report type, which classifies each announcement into one of the report categories defined by the ASX. We have tried Neural Networks, a Naive Bayes classifier, and Support Vector Machines and achieved good results. Keywords Document Management, Document Workflow
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.08" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
    </responses>
  </theme>
  <theme id="19" title="Theme 19">
    <words>
      <word weight="3.71648694931">
        web
      </word>
      <word weight="3.09218730548">
        search
      </word>
      <word weight="2.23906131604">
        spam
      </word>
      <word weight="2.1110365427">
        engin
      </word>
      <word weight="1.9360819637">
        page
      </word>
      <word weight="1.19139339899">
        result
      </word>
    </words>
    <responses>
      <response id="67" uni="The Australian National University, CSIRO ICT Centre" weight="2.08" year="2007">
        A Framework for Measuring the Impact of Web Spam  Web spam potentially causes three deleterious effects: unnecessary work for crawlers and search engines; diversion of traffic away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here we focus on the deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web spam. We index the 80 million page UK2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments.  Web Information Retrieval, Web Spam, Adversarial Information Retrieval
      </response>
      <response id="19" uni="Australian National University  CSIRO Mathematical and Information Sciences" weight="1.92" year="2002">
        Buying bestsellers online: A case study in Search &amp; Searchability  A website's design directly affects how well search engines can crawl, match and rank its pages. For this reason, searchability is an important concern in site design. We study the interaction between search engines and Web sites by means of a case study of online bookstores and general-purpose search engines. The task modelled is that of finding web pages from which a book, described by its title, may be purchased. We first compared the relative effectiveness of search engines in finding pages matching the criterion, regardless of bookstore. Then we compared the relative searchability of the bookstore websites by observing how many times each bookstore contributed useful answers to the search results. Large differences in the performance of both search engines and bookstores were observed. Two of the search engines performed better than their peers, and one bookstore was far more searchable than all others. To further explore these differences we tabulate the total number of pages from each bookshop which are included in the search engine indexes. We conclude with recommendations both to bookstores on how they may improve their Web presence, and to search engines on how they may improve their performance for product searches.  Information Retrieval Web search, evaluation, transactional search
      </response>
      <response id="118" uni="Australian National University, CSIRO, Funnelback" weight="1.14" year="2011">
        The usefulness of web spam  Spam comprises at least 60% of the public web, and search engine companies invest considerable effort in rejecting these apparently useless pages. But how bad are spam pages in search results? Can spam be dealt with as a side-effect of dealing with page utility, or is the relationship more complex? Thirty-four volunteer judges rated selected individual documents first on usefulness to a specified task and then on degree of 'spamminess'. Our results show that the relationship between spamminess and utility is far from clear cut; judges found that an important proportion of spam documents were useful. We conclude that evaluation should consider both utility and spamminess, as separate factors; and that search engines should not summarily discard spam pages but should take their utility into account as well.  User Studies Involving Documents; Web Documents
      </response>
      <response id="46" uni="Melbourne" weight="1.13" year="2006">
        Examining the Pseudo-Standard Web Search Engine Results Page  Nearly every web search engine presents its results in an identical format: a ranked list of web page summaries. Each summary comprises a title; some sentence fragments usually containing words used in the query; and URL information about the page. In this study we present data from our pilot experiments with eye tracking equipment to examine how users interact with this standard list of results as presented by the Australian sensis.com.au web search service. In particular, we observe: different behaviours for navigational and informational queries; that users generally scan the list top to bottom; and that eyes rarely wander from the left of the page. We also attempt to correlate the number of bold words (query words) in a summary with the amount of time spent reading the summary. Unfortunately there is no substantial correlation, and so studies relying heavily on this assumption in the literature should be treated with caution.  web search engine, eye tracking, web page summaries
      </response>
      <response id="135" uni="CSIRO" weight="0.81" year="2012">
        Explaining difficulty navigating a website using page view data  A user's behaviour on a web site can tell us something about that user's experience. In particular, we believe there are simple signals-including circling back to previous pages, and swapping out to a search engine-that indicate difficulty navigating a site. Simple page view patterns from web server logs correlate with these signals and may explain them. Extracting these patterns can help web authors understand where, and why, their sites are confusing or hard to navigate. We illustrate these ideas with data from almost a million sessions on a government website. In this case a small number of page view patterns are present in almost a third of difficult sessions, suggesting possible improvements to website language or design. We also introduce a tool for web authors, which makes this analysis available in the context of the site itself.  [Information Interfaces and Presentation]: Hypertext and Hypermedia General Terms: Human Factors; Measurement Keywords: Web documents
      </response>
      <response id="167" uni="DSTO C3 Research Centre" weight="0.76" year="2000">
        Two Case Studies for KAKTUS  KAKTUS is a step toward what Tim Bemers-Lee, the creator of the World Wide Web, calls a &quot;semantic Web&quot; where people, software agents, search engines and other programs can gain access to meaning-rather than just content-on a Web site. A semantic Web potentially also lets programs utilize all the data on Web pages, allowing them to gain knowledge from one site and apply it to logical mappings on other sites. Keywords Semantic Web, Enterprise model. Textual recognition.
      </response>
      <response id="119" uni="CSIRO, KMITL, University of Glasgow" weight="0.69" year="2011">
        Indexing without Spam  The presence of spam in a document ranking is a major issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages that are likely to contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user's query. In this paper we propose removing spam pages at indexing time, therefore obtaining a pruned index that is virtually 'spam-free'. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performance. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection's index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches.  Information Retrieval; Index Pruning; Spam; Web search; Efficiency.
      </response>
      <response id="11" uni="The University of Queensland" weight="0.67" year="2002">
        Tibianna: A Learning-Based Search Engine with Query Refinement  While web search engine technology has improved over time, there is often a fundamental reliance on keyword matching for searches. What happens however, when the user does not know what keywords to use? This paper presents preliminary learning results of a prototype learning search engine that attempts to address this problem. Tibianna allows a user to manually rank a set of results based on their own relevancy function. Once a required number of results are ranked, the set is downloaded, processed and presented to support vector machines (SVMs) for learning. Once learned, Tibianna can actively reorder or discard search engine results based on the model it has learned. This provides a way of improving search results without requiring query refinement. Learning outcomes from experimental trials with Tibianna are presented, demonstrating the implications of using different preprocessing techniques and corpus sizes. Query refinement functions are also available to the user, which can enable exploration of query words via the WordNet database, and allows quick query refinement via a dynamic HTML interface.  Information Retrieval, Personalised Documents, Search Engine Technology
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.64" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="165" uni="Bond University" weight="0.62" year="2000">
        Implementing Shared Document Preparation with Lightweight Editing  Virtually all web pages are read-only, yet the first web browser allowed users to read and edit every page. Special ad-hoc mechanisms are needed to make all or part of a page editable by a user. This paper describes Pardalote lightweight editing, a document management feature for allowing many users to share the editing of a web page using only a web browser. A brief oven'iew of how Pardalote is implemented is followed by examples of shared document preparation using Pardalote. The benefits of such web document management are discussed. Future Pardalote extensions using XML precede the closing remarks. Keywords Shared document management, cooperative document preparation, lightweight editing, I-grains, fraglets, user interface design, computer supported cooperative work.
      </response>
      <response id="126" uni="University of Otago" weight="0.61" year="2012">
        Effects of Spam Removal on Search Engine Efficiency and Effectiveness  Spam has long been identified as a problem that web search engines are required to deal with. Large collection sizes are also an increasing issue for institutions that do not have the necessary resources to process them in their entirety. In this paper we investigate the effect that withholding documents identified as spam has on the resources required to process large collections. We also investigate the resulting search effectiveness and efficiency when different amounts of spam are withheld. We find that by removing spam at indexing time we are able to decrease the index size without affecting the indexing throughput, and are able to improve search precision for some thresholds.  Information Search and Retrieval, Content Analysis and Indexing - Indexing methods Information Filtering
      </response>
      <response id="59" uni="Queensland Unversity of Technology, The Pennsylvania State University" weight="0.60" year="2007">
        Multimedia Web Searching on a Meta-Search Engine  This paper provides preliminary results from a major study of multimedia Web searching by Dogpile meta-search engine users, including queries and session characteristics, and changes or differences in image, video and audio searching. The resutls are compares with multimedia Web searching studies from 1997 to 2002. Image and sexual queries are dominant in multimedia We searching. The paper provides important implications for the design of multimedia information retrieval systems.
      </response>
      <response id="47" uni="ICT Centre CSIRO" weight="0.51" year="2006">
        Improving rankings in small-scale web search using click-implied descriptions    When a searcher submits a query Q and clicks on document R in the corresponding result set, we may plausibly interpret the click as a vote that Q is a description of R. We call the Q and R pairing a 'click description'. Click descriptions thus derived from search engine logs can be accumulated into surrogate documents and used to boost retrieval effectiveness in a similar fashion to anchor text. We investigate the usefulness of click description surrogate documents in processing queries for an external web site search service for four organisations. Using the mean reciprocal rank of best answers as the measure of performance, we show that, for popular queries, click description surrogates significantly outperform both anchor text surrogates and the original proprietary rankings. The amount of click data needed to achieve a high level of retrieval performance is surprisingly small for popular queries. Thanks to terms shared between queries, click description surrogates can answer queries for which no specific click data is available. We show a 92% improvement due to this effect for a set of lengthy, less popular queries. We also discuss issues such as spam rejection, unpopular queries, and how to combine click description scores with other evidence. We argue the potential of click descriptions in non-web applications where link  Information Storage and Retrieval, Content Analysis and Indexing [Indexing methods]
      </response>
      <response id="94" uni="RMIT University" weight="0.48" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="12" uni="The University of Queensland" weight="0.47" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="139" uni="University of Waterloo, Inforium Technologies Inc." weight="0.47" year="1997">
        LivePAGE - A multimedia database system to support World-Wide Web development  The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems. Architecture, World-Wide Web, multimedia, SGML, Web site development, document database, hypermedia.
      </response>
      <response id="64" uni="CSIRO ICT Centre, Australian National University" weight="0.45" year="2007">
        Does brandname influence perceived search result quality? Yahoo!, Google, and WebKumara  Improving the quality of search engine results is the goal of costly efforts by major Web search engine companies. Using in situ side-by-side result set comparisons and random assignment of brandnames to result sets, we investigated whether perceptions of quality were influenced by brand association. In the first experiment (15 searchers) we found no significant preference for or against results labelled 'Google' relative to those labelled 'Yahoo!'. In the second experiment (20 searchers) result sets were again generated by Google and Yahoo! but were randomly labelled 'Yahoo!' or 'WebKumara' (a fictitious name). Again, we found no significant preference for one brandname label over the other. Contrary to previous findings, we found a statistically significant preference for Googlegenerated results over those of Yahoo! when data from three separate experiments (total 70 subjects) was combined.  Information retrieval
      </response>
      <response id="109" uni="RMIT University" weight="0.37" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="29" uni="The University of Sydney" weight="0.33" year="2004">
        Co-training on Textual Documents with a Single Natural Feature Set  Co-training is a semi-supervised technique that allows classifiers to learn with fewer labelled documents by taking advantage of the more abundant unclassified documents. However, conventional cotraining requires the dataset to be described by two disjoint and natural feature sets that are redundantly sufficient. In many practical situations datasets have a single set of features and it is not obvious how to split it into two. This paper investigates the performance of co-training with only one natural feature set in two applications: Web page classification and email filtering. Text categorization, Web page classification, spam filtering, co-training
      </response>
      <response id="110" uni="University of Otago" weight="0.33" year="2010">
        Efficient Accumulator Initialisation  IR efficiency is normally addressed in terms of accumulator initialisation, disk I/O, decompression, ranking and sorting. Traditionally, the performance of search engines is dominated by slow disk I/O, CPU-intensive decompression, complex similarity ranking functions and sorting a large number of candidate documents. However, after we have applied a number of optimisation techniques, our search engine is bottlenecked by accumulator initialisation. In this paper, we propose an efficient accumulator initialisation algorithm, which represents the traditional static accumulator array as a logical two dimensional table and uses a number of flags to track the initialisation status of the accumulators. The efficiency of the algorithm is verified by a simulation program and a search engine. The overall performance can be as good as a 93% increase in throughput.  Accumulator Initialisation, Efficiency, Postings Pruning.
      </response>
      <response id="111" uni="CSIRO, Australian National University" weight="0.31" year="2010">
        Interaction differences in web search and browse logs  We use logfiles from two web servers (public and internal), two corresponding search engines, and two user populations (public and staff) to examine differences in behaviour across users and sites. We observe similar overall characteristics to other browsing and searching logs, but differences in behaviour between staff and the public and between external and internal sites. Staff familiarity with organisational language and structure does not translate to more effective search or navigation, although staff do expend considerable effort looking for information and often look in the wrong place. This would not be apparent from logs covering only search or only browsing behaviour.  Log analysis; user behaviour; information retrieval
      </response>
      <response id="132" uni="The University of Sydney" weight="0.25" year="2012">
        Putting the Public into Public Health Information Dissemination: Social Media and Health-related Web Pages   Public health information dissemination represents an interesting combination of broadcasting, sharing, and retrieving relevant health information. Social media-based public health information dissemination offers some particularly interesting characteristics, as individual users or members of the public actually carry out the actions that constitute the dissemination. These actions also may inherently provide novel evaluative information from a document computing perspective, providing information in relation to both documents and indeed the social media users or health consumers themselves. This paper discusses the novel aspects of social media-based public health information dissemination, including a comparison of its characteristics with search engine-based Web document retrieval. A preliminary analysis of a sample of public health advice tweets taken from a larger sample of over 4700 tweets sent by Australian health-related organization in February 2012 is described. Various preliminary measures are analyzed from this data to initially suggest possible characteristics of public health information dissemination and document evaluation in micro-blog-based systems based on this sample.  Twitter, Web documents, Public Health
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.24" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="45" uni="The University of Melbourne" weight="0.23" year="2006">
        Some Observations on User Search Behavior  We explore some issues that arise in the way that users interact with a web search engine, as evidenced by the records of their interaction provided by query and clickthrough log data. Our observations are derived from approximately fifteen million user queries recorded by the search.msn.com search service in May 2006.  Log analysis, user behavior, search.
      </response>
      <response id="83" uni="CSIRO ICT Centre and ANU DCS, Funnelback" weight="0.21" year="2008">
        Anonymous folksonomies for small enterprise webs: a case study  Tags and emergent folksonomies are a potentially rich new source of document annotations, offering query independent and dependent evidence for exploitation by information retrieval systems. Previous research has shown that tags may facilitate improved web search in an environment where each tagging action generates a (user, tag, resource) triple. For websites operated by a public institution, operational or privacy concerns may prevent the recording of data capable of identifying individuals. This leads to a simpler anonymous tagging system but is likely to reduce user motivation for tagging, since the user cannot access their own set of tags. It also means that votes for tags are not counted, and a potentially useful joining attribute is not available. Using webpage, metadata, query, click, anchortext and tag data provided by a public museum, we demonstrate that, despite these limitations, tag data collected by an anonymous tagging system has the potential to improve retrieval effectiveness.  Information Storage and Retrieval
      </response>
      <response id="82" uni="RMIT University, University of Technology Dresden" weight="0.19" year="2008">
        WebKnox: Web Knowledge Extraction  The paper describes and evaluates a system for extracting knowledge from the web that uses a domain independent fact extraction approach and a self supervised learning algorithm. Using a trust algorithm, the precision of the system is improved to over 70% compared with a baseline of 52%.  Information Extraction, Web Mining
      </response>
      <response id="61" uni="Queensland University of Technology" weight="0.17" year="2007">
        A Bottom-up Term Extraction Approach forWeb-based Translation in Chinese-English IR Systems  The extraction of Multiword Lexical Units (MLUs) in lexica is important to language related methods such as Natural Language Processing (NLP) and machine translation. As one word in one language may be translated into an MLU in another language, the extraction of MLUs plays an important role in Cross-Language Information Retrieval (CLIR), especially in finding the translation for words that are not in a dictionary. Web mining has been used for translating the query terms that are missing from dictionaries. MLU extraction is one of the key parts in search engine based translation. The MLU extraction result will finally affect the transition quality. Most statistical approaches to MLU extraction rely on large statistical information from huge corpora. In the case of search engine based translation, those approaches do not perform well because the size of corpus returned from a search engine is usually small. In this paper, we present a new string measurement and new Chinese MLU extraction process that works well on small corpora.   Cross-language Information retrieval, CLIR, query translation, web mining, OOV problem, term extraction
      </response>
      <response id="63" uni="Queensland University of Technology" weight="0.16" year="2007">
        Integration of Information Filtering and Data Mining Process for Web Information Retrieval  This paper examines a new approach to Web information retrieval, and proposes a new two stage scheme. The aim of the first stage is to quickly filter irrelevant information based on the user profiles. The proposed user profiles learning algorithm are very efficient and effective within a relevance feedback framework. The aim of the second stage is to apply data mining techniques to rationalize the data relevance on the reduced data set. Our experiments on RCV1 (Reuters Corpus Volume 1) data collection which is used by TREC in 2002 for filtering track show that more effective and efficient access Web information has been achieved by combining the strength of information filtering and data mining method.  Information filtering, User profiles, Data mining, Pattern taxonomic model
      </response>
      <response id="160" uni="DSTO, Lloyd-Jones Consulting" weight="0.16" year="1999">
        Automatic document metadata extraction and manipulation: a working system for the Intelligence Analyst  This paper discusses the design and implementation of an operational system to aid health intelligence analysts. The HINTS system provides automated support to undertake tasks such as specific health related research and report writing in the face on an ever-growing body of electronic information, available on the web, and on local file systems. Our approach is to provide automated support for document analysis and discovery from technologies that support ad-hoc searching, consistent filtering for specific pieces of information such as hospital facilities, diseases and locations, and that provide document summarisation and keywording. Document metadata is stored in XML in a data structure that allows a variety of searches and views of the document space to be performed. The user interfaces to the system by web browser and a map-based geospatial application. Document Analysis, Document Databases Information Retrieval, XML, Information Extraction
      </response>
      <response id="33" uni="University of Southern Queensland" weight="0.15" year="2004">
        GOOD Publishing System: Generic Online/Offline Delivery    GOOD is a tailor-made, fully integrated publishing system that creates output documents for multiple media types used in both online and offline teaching modes at the University of Southern Queensland. It is used in the Distance and e-Learning Centre of USQ to create course material for thousands of on-campus, online and external students. Among the end products generated from a single XML input document containing study material for a specific course are study books, introductory books, and web sites in a variety of formats. Future end products currently being investigated include voice rendering. The GOOD system is entirely based on open standards such as XML, XSLT, DOM, and XSL:FO and implemented with JAVA/J2EE technology. Among its features is a smart editing client to allow technically non-proficient staff to edit their own course material.  Document Management and Publishing, XML authoring.
      </response>
      <response id="166" uni="Division of Mathematical and Information Science CSIRO" weight="0.14" year="2000">
        An Experiment in Light Workflow  Workflow tools have been successfully applied to automate work in many situations where the work is well regulated, there is a stable pattern of work, and there is a sufficiently high volume or sufficiently high importance to justify the cost of automating the activities. In many other circumstances there is a very mixed story of success and failure of workflow implementation. The Web also has changed work practices and increased the role of electronic documents, in particular, forms, as a support for many distributed tasks. In this paper we explore using a workflow approach based on fully self descriptive documents, that embed the information and instructions necessary to support processing the document, within the document. The traditional workflow engine or server that is typical of current workflow tools is discarded, but the document still allows a full work process to be applied, without necessarily enforcing the process. Ideally one would need a Web browser, and an email client, and no workflow system at all. This paper shows how this is not quite possible, but one can build a very small supporting application to achieve light workflow. Keywords Workflow, Document Flow. Web-based Workflow, XML, XSLT, Co-operative work.
      </response>
      <response id="65" uni="University of Melbourne" weight="0.13" year="2007">
        Automatic Thread Classification for Linux User Forum Information Access  We experiment with text classification of threads from Linux web user forums, in the context of improving information access to the problems and solutions described in the threads. We specifically focus on classifying threads according to: (1) them describing a specific problem vs. containing a more general discussion; (2) the completeness of the initial post in the thread; and (3) whether problem(s) in the initial post are resolved in the thread or not. We approach these tasks in both classification and regression frameworks using a range of machine learners and evaluation metrics.  Web Documents, Document Management
      </response>
      <response id="133" uni="Funnelback" weight="0.13" year="2012">
        Reordering an index to speed query processing without loss of effectiveness.  Following Long and Suel, we empirically investigate the importance of document order in search engines which rank documents using a combination of dynamic (query-dependent) and static (queryindependent) scores, and use document-at-a-time (DAAT) processing. When inverted file postings are in collection order, assigning document numbers in order of descending static score supports lossless early termination while maintaining good compression. Since static scores may not be available until all documents have been gathered and indexed, we build a tool for reordering an existing index and show that it operates in less than 20% of the original indexing time. We note that this additional cost is easily recouped by savings at query processing time. We compare best early-termination points for several different index orders on three enterprise search collections (a whole-of-government index with two very different query sets, and a collection from a UK university). We also present results for the same orders for ClueWeb09-CatB . Our evaluation focuses on finding results likely to be clicked on by users of Web or website search engines - Nav and Key results in the TREC 2011 Web Track judging scheme. The orderings tested are Original, Reverse, Random, and QIE (descending order of static score). For three enterprise search test sets we find that QIE order can achieve close-to-maximal search effectiveness with much lower computational cost than for other orderings. Additionally, reordering has negligible impact on compressed index size for indexes that contain position information. Our results for an artificial query set against the TREC ClueWeb09 Category B collection are much more equivocal and we canvass possible explanations for future investigation.  [Information Systems]: Information Storage and Retrieval- Information Search and Retrieval;   Information Storage and Retrieval-Systems and Software  Enterprise search; inverted files; efficiency and effectiveness; information retrieval.
      </response>
      <response id="21" uni="The University of Sydney" weight="0.10" year="2002">
        A Framework for Text Categorization In this paper we discuss the architecture of an object-oriented application framework (OOAF) for text categorization. We describe the system requirements and the software engineering strategies that form the basis of the design and implementation of the framework. We show how designing a highly reusable OOAF architecture facilitates the development of new applications. We also highlight the key text categorization features of the framework, as well as practical considerations for application developers. Document Management, Text Categorization, Application Frameworks
      </response>
      <response id="44" uni="University of Otago" weight="0.09" year="2005">
        Recommending Geocaches  Players downloading GPS coordinates from the internet, hiking to the given spot, and hunting for a hidden box - this is the new sport of geocaching. Today there are nearly 200,000 such boxes in over 200 countries. With so many to find, a recommender is needed, one that takes into account not only the boxes, but also the geospatial and temporal nature of the sport. A database of geocaches in the South Island of New Zealand is made by trawling a prominent geocaching web site. This is then used to estimate the home-coordinates (geospatial playing centre) of players. Predictions are verified against a set of correct coordinates solicited from players. Several geocache recommenders are discussed and compared. The precision, computed using mean of mean reciprocal rank (MMRR), of each is measured. The best method tried is a collaborative filter using intersection over mean to find similar players and a voting scheme to recommend geocaches. This method is proposed as a replacement for the currently used distance from home-coordinate; doing so will increase the precision of existing systems such as geocaching. com.  Information Retrieval.
      </response>
    </responses>
  </theme>
  <theme id="20" title="Theme 20">
    <words>
      <word weight="4.61375600484">
        relev
      </word>
      <word weight="2.12503879929">
        assessor
      </word>
      <word weight="1.93703965731">
        judgement
      </word>
      <word weight="1.81149681997">
        document
      </word>
      <word weight="1.41504001088">
        topic
      </word>
      <word weight="1.38753367887">
        evalu
      </word>
    </words>
    <responses>
      <response id="112" uni="RMIT University, The University of Melbourne" weight="2.44" year="2010">
        Relatively Relevant: Assessor Shift in Document Judgements  The evaluation of information retrieval systems relies on relevance judgements - human assessments of whether a document is relevant to a specified search request. In the past, it was demonstrated that test collection assessors disagree with each other to some extent on the relevance of documents and can be inconsistent in themselves. This paper describes a series of investigations on assessor consistency, which demonstrate that the inconsistency of an assessor varies over time. We show that when documents are presented to assessors in a relevance independent order, documents judged as relevant appear to cluster. Examining pairs of documents in a sequence ordered by timeof- judgement, we find that relevance assessors judge highly similar document pairs more consistently when the pairs are seen soon after each other; the consistency reduces when the pairs are judged further apart. We contend that our analysis shows that changes are not due to random error, but instead reflect a relevance shift, whereby the assessor's conception of what constitutes a relevant document changes over time. Studying types of relevance judgement we find that the shift in judgements is greatest between highly and partially relevant documents. We also examine the impact of this inconsistency on how retrieval runs are ranked relative to each other and find that there appears to be a noticeable effect on such rankings.  Information retrieval evaluation, Cranfield approach, Relevance judgements, TREC.
      </response>
      <response id="62" uni="University of Otago" weight="0.97" year="2007">
        IR Evaluation Using Multiple Assessors per Topic  Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006. Traditionally the assessment of a single topic is performed by a single human. Herein we examine the consequences of using multiple assessors per topic. A set of 15 topics were used. The mean topic pool contained 98 documents. Between 3 and 5 separate assessors (per topic) assessed all documents in a pool. One assessor was designated baseline. All were then used to generate 10,000 synthetic multi-assessor assessment sets. The baseline relative rank order of all runs submitted to the INEX 2006 relevant-in-context task was compared to those of the synthetics. The mean Spearman's rank correlation coefficient was 0.986 and all coefficients were above 0.95 - the correlation is very strong. Non matching rank-orders are seen when the mean average precision difference between runs is less than 0.05. In the top 10 runs no significantly different runs were ranked in a different order in more than 5% of the synthetics. Using multiple assessors per topic is very unlikely to affect the outcome of an evaluation forum.  Information Retrieval
      </response>
      <response id="114" uni="RMIT University" weight="0.85" year="2010">
        Criteria that have an effect on users while making image relevance judgements  This paper reports the result of an exploratory user study investigating criteria that are important to users when judging relevance while performing an image search. Data was collected from 12 participants using questionnaires and screen capture recordings. Users were required to perform three image search tasks which are specific, general and abstract image search and judge relevance based on ten criteria identified from previous studies. Findings show that some criteria were important when making relevance judgements, with topicality, appeal of information and composition being the common criteria across the search tasks. However the order of importance of the criteria differ between the image search tasks.  Information retrieval, user studies involving documents, Web image search, Relevance criteria, Relevance judgment
      </response>
      <response id="24" uni="The Robert Gordon University" weight="0.84" year="2004">
        On the Effectiveness of Relevance Profiling  Relevance profiling is a general process for within-document retrieval. Given a query, a profile of retrieval status values is computed by sliding a fixed sized window across a document. In this paper, we report a series of bench experiments on relevance pro-filing, using an existing electronic book, and its associated book index. The book index is the source of queries and relevance judgements for the experiments. Three weighting functions based on a language modelling approach are investigated, and we demonstrate that the well-known query generation model outperforms one based on the Kullback-Leibler divergence, and one based on simple term frequency. The relevance profiling process proved highly effective in retrieving relevant pages within the electronic book, and exhibits stable performance over a range of slid-ing window sizes. The experimental study provides evidence for the effectiveness of relevance profiling for within-document retrieval, with the caveat that the experiment was conducted with a particular electronic book.  relevance profiling; within-document retrieval; language modelling; information retrieval experimentation.
      </response>
      <response id="109" uni="RMIT University" weight="0.66" year="2010">
        Evaluating the Effectiveness of Visual Summaries forWeb Search  With ever-increasing amounts of information on the World Wide Web, an effective interface for displaying search results is required. Recent studies have developed various novel approaches for visual summaries, aiming to improve the effectiveness of search results. In this study we evaluate the effectiveness of four types of visual summary: thumbnails, salient images, visual snippets and visual tags. Fifty participants carried out five informational topics using five different interfaces. The results show that visual summaries significantly impact on the behavior of users, but not on their performance when predicting the relevance of answer resources. Users spend significantly less time looking at the textual components of summaries with the visual summary interfaces. Comparing the performance of users in predicting the relevance of answer pages with a text interface versus visual interfaces suggests that the tested visual summaries can mislead users to select non relevant items on informational search topics.  Information Retrieval, User Studies Involving Documents, Web Documents, Visual Summaries, Eye Tracking.
      </response>
      <response id="96" uni="RMIT University" weight="0.64" year="2009">
        Modelling Disagreement Between Judges for Information Retrieval System Evaluation  The batch evaluation of information retrieval systems typically makes use of a testbed consisting of a collection of documents, a set of queries, and for each query, a set of judgements indicating which documents are relevant. This paper presents a probabilistic model for predicting IR system rankings in a batch experiment when using document relevance assessments from different judges, using the precision-at-n family of metrics. In particular, if a new judge agrees with the original judge with an agreement rate of ?, then a probability distribution of the difference between the P@n scores of the two systems is derived in terms of ?. We then examine how the model could be used to predict system performance based on user evaluation of two IR systems, given a previous batch assessment of the two systems together with a measure of the agreement between the users and the judges used to generate the original batch relevance judgements. From the analysis of data collected in previous user experiments, it can be seen that simple agreement (?) between users varies widely between search tasks and information needs. A practical choice of parameters for the model from the available data is therefore difficult. We conclude that gathering agreement rates from users of a live search system requires careful consideration of topic and task effects.  Information retrieval; Evaluation; User studies
      </response>
      <response id="30" uni="RMIT University" weight="0.46" year="2004">
        A Testbed for Indonesian Text Retrieval  Indonesia is the fourth most populous country and a close neighbour of Australia. However, despite media and intelligence interest in Indonesia, little work has been done on evaluating Information Retrieval techniques for Indonesian, and no standard testbed exists for such a purpose. An effective testbed should include a collection of documents, realistic queries, and relevance judgements. The TREC and TDT testbeds have provided such an environment for the evaluation of English, Mandarin, and Arabic text retrieval techniques. The NTCIR testbed provides a similar environment for Chinese, Korean, Japanese, and English. This paper describes an Indonesian TREC-like testbed we have constructed and made available for the evaluation of ad hoc retrieval techniques. To illustrate how the test collection is used, we briefly report the effect of stemming for Indonesian text retrieval, showing - similarly to English - that it has little effect on accuracy.  Indonesian, queries, collection, relevance judgements, stemming
      </response>
      <response id="75" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.41" year="2007">
        Score Standardization for Robust Comparison of Retrieval Systems  Information retrieval systems are evaluated by applying them to standard test collections of documents, topics, and relevance judgements. An evaluation metric is then used to score a system's output for each topic; these scores are averaged to obtain an overall measure of effectiveness. However, different topics have differing degrees of difficulty and differing variability in scores, leading to inconsistent contributions to aggregate system scores and problems in comparing scores between different test collections. In this paper, we propose that per-topic scores be standardized on the observed score distributions of the runs submitted to the original experiment from which the test collection was created. We demonstrate that standardization equalizes topic contributions to system effectiveness scores and improves inter-collection comparability.  Retrieval system evaluation, average precision, standardization.
      </response>
      <response id="23" uni="ANU, CSIRO ICT Centre" weight="0.32" year="2004">
        Focused crawling in depression portal search: A feasibility study  Previous work on domain specific search services in the area of depressive illness has documented the significant human cost required to setup and maintain closed-crawl parameters. It also showed that domain coverage is much less than that of whole-of-web search engines. Here we report on the feasibility of techniques for achieving greater coverage at lower cost. We found that acceptably effective crawl parameters could be automatically derived from a DMOZ depression category list, with dramatic saving in effort. We also found evidence that focused crawling could be effective in this domain: relevant documents from diverse sources are extensively interlinked; many outgoing links from a constrained crawl based on DMOZ lead to additional relevant content; and we were able to achieve reasonable precision (88%) and recall (68%) using a J48-derived predictive classifier operating only on URL words, anchor text and text content adjacent to referring links. Future directions include implementing and evaluating a focused crawler. Furthermore, the quality of information in returned pages (measured in accordance with the evidence based medicine) is vital when searchers are consumers. Accordingly, automatic estimation of web site quality and its possible incorporation in a focused crawler is the subject of a separate concurrent study. focused crawler, hypertext classification, mental health, depression, domain-specific search.
      </response>
      <response id="91" uni="NICTA and The University of Melbourne" weight="0.31" year="2009">
        External Evaluation of Topic Models  Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.  Topic Modeling, Evaluation, Document Similarity, Natural Language Processing, Information Retrieval
      </response>
      <response id="87" uni="NICTA Victoria Laboratory, The University of Melbourne" weight="0.30" year="">
        Parameter Sensitivity in Rank-Biased Precision  Rank-Biased Precision (RBP) is a retrieval evaluation metric that assigns an effectiveness score to a ranking by computing a geometricly weighted sum of document relevance values, with the monotonicly decreasing weights in the geometric distribution determined via a persistence parameter p. Despite exhibiting various advantageous traits over well known existing measures such as Average Precision, RBP has the drawback of requiring the designer of any experiment to choose a value for p. Here we present a method that allows retrieval systems evaluated using RBP with different p values to be compared. The proposed approach involves calculating two critical bounding relevance vectors for the original RBP score, and using those vectors to calculate the range of possible RBP scores for any other value of p. Those bounds may then be sufficient to allow the outright superiority of one system over the other to be established. In addition, the process can be modified to handle any RBP residuals associated with either of the two systems. We believe the adoption of the comparison process described in this paper will greatly aid the uptake of RBP in evaluation experiments.  Rank-Biased Precision, Evaluation, System Comparison
      </response>
      <response id="104" uni="Queensland University of Technology, University of Otago" weight="0.29" year="2009">
        The Methodology of Manual Assessment in the Evaluation of Link Discovery  The link graph extracted from the Wikipedia has often been used as the ground truth for measuring the performance of automated link discovery systems. Extensive manual assessments experiments at INEX 2008 recently showed that this is unsound and that manual assessment is essential. This paper describes the methodology for link discovery evaluation which was developed for use in the INEX 2009 Link-the-Wiki track. In this approach both manual and automatic assessment sets are generated and runs are evaluated using both. The approach offers a more reliable evaluation of link discovery methods than just automatic assessment. A new evaluation measure for focused link discovery is also introduced.  Wikipedia, Link Quality, Manual Assessment, Evaluation.
      </response>
      <response id="107" uni="The University of Melbourne, University of Malaya" weight="0.28" year="2010">
        Estimating System Effectiveness ScoresWith Incomplete Evidence  It is common for only partial relevance judgments to be used when comparing retrieval system effectiveness, in order to control experimental cost. Using TREC data, we consider the uncertainty introduced into per-topic effectiveness scores by pooled judgments, and measure the effect that incomplete evidence has on both the systems scores that are generated, and also on the quality of paired system comparisons. We measure system behavior from three different points of view: the trend in effectiveness scores; the separability of system pairs; and the number of reversals in significance outcomes as the depth of judgments increases. Our results show that when shallow pooled judgments are used system separability remains relatively high, but that there is also a high rate of significance reversal. We then show that explicitly adjusting effectiveness scores to allow for the known amount of uncertainty gives a reduced number of reversals, and hence more consistent experimental outcomes.  Retrieval evaluation, effectiveness metric, pooling
      </response>
      <response id="106" uni="RMIT University" weight="0.22" year="2010">
        Seeing the forest from trees : Blog Retrieval by Aggregating Post Similarity Scores  Blog retrieval is a new and challenging task. Instead of retrieving individual documents, this task requires retrieving collections of documents, or blog posts. It has been shown recently that the federated model of using post entries as retrieval units is an effective approach to blog retrieval, where aggregation of similarity scores for posts to rank blogs plays an important role in the final ranking of blogs. In this paper, we explore two approaches of aggregation describing the depth and width of topical relevance relationship between post entries and blogs. We further propose holistic approaches that combine both approaches. Our experiments show that the sum baseline has the best performance, although the performances of the probabilistic approach and the linear pooling approach are very similar.   blog retrieval, score aggregation
      </response>
      <response id="34" uni="Queensland University of Technology" weight="0.19" year="2004">
        NLPX - An XML-IR System with a Natural Language Interface  Traditional information retrieval (IR) systems respond to user queries with ranked lists of relevant documents. The separation of content and structure in XML documents allows individual XML elements to be selected in isolation. Thus, users expect XML-IR systems to return highly relevant results that are more precise than entire documents. This paper presents such a system. The system accepts queries in both natural language (English) and formal XPath-like format (NEXI) and matches to a set of relevant and appropriately-sized elements using an effective ranking scheme.  Information Retrieval, Natural Language Queries
      </response>
      <response id="63" uni="Queensland University of Technology" weight="0.16" year="2007">
        Integration of Information Filtering and Data Mining Process for Web Information Retrieval  This paper examines a new approach to Web information retrieval, and proposes a new two stage scheme. The aim of the first stage is to quickly filter irrelevant information based on the user profiles. The proposed user profiles learning algorithm are very efficient and effective within a relevance feedback framework. The aim of the second stage is to apply data mining techniques to rationalize the data relevance on the reduced data set. Our experiments on RCV1 (Reuters Corpus Volume 1) data collection which is used by TREC in 2002 for filtering track show that more effective and efficient access Web information has been achieved by combining the strength of information filtering and data mining method.  Information filtering, User profiles, Data mining, Pattern taxonomic model
      </response>
      <response id="145" uni="CSIRO Mathematical and Information Sciences" weight="0.15" year="1997">
        A Proximity Measure for Ranked Text Retrieval  In this paper we introduce a simple heuristic measure that gives higher scores to the documents where query terms co-occur in close proximity. This measure is aimed to increase performance of text retrieval by distinguishing dense regions of matching from a few matches scattered across a document. The ability to do this is important for large collections where document sizes vary significantly. We briefly discuss a few other techniques that make use of proximity information, then introduce our method and present results of its evaluation. This evaluation shows that the method gives a considerable advantage in comparison with the cosine similarity measure. We also have conducted additional experiments to prove that it works well in a combination with an automatic relevance feedback method. retrieval
      </response>
      <response id="12" uni="The University of Queensland" weight="0.14" year="2002">
        MyNewsWave: User-centered Web search and news delivery  MyNewsWave uses machine learning (including support vector machines) for a user-centred approach to full-text information retrieval as well as news delivery. The system uses knowledge sources such as WordNet to refine keyword queries and learns user-preferences with regard to web search. MyNewsWave includes an audio mining system for topic detection in conjunction with background search to facilitate the retrieval of relevant multimedia information. A special feature of MyNewsWave is the assessment of incoming information with regard to the 'mood' or personal relevance to a user. DigiMood is a component of MyNewsWave that classifies web pages into mood categories. Business news, for instance, can be classified by DigiMood to access market sentiment. Marconi analyses incoming news streams and uses machine learning to adjust parameters of a text-to-speech system. The objective is to learn the appropriate voice for news items as part of a speech user interface.  Multimedia resource discovery, Personalised documents, information retrieval.
      </response>
      <response id="52" uni="CSIRO ICT Centre" weight="0.13" year="2006">
        Information Access Efficiency: a Measure and a Case Study  One of the advantages we claim for information synthesis and aggregation is that it results in more efficient information access for end users, especially when the relevant information comes from multiple heterogeneous data sources. Although that claim is plausible, it has not been verified by any qualitative studies. It is even unclear how one would quantify the efficiency of information access. In this paper, we propose a measure and then report on a study to identify the information access efficiency gain of a potential application involving information synthesis and aggregation.1   information access efficiency, information aggregation, information access time and speed, information relevance.
      </response>
      <response id="71" uni="The University of Melbourne, NICTA Victoria Research Laboratory" weight="0.13" year="2007">
        On the distribution of user persistence for rank-biased precision  Rank-biased precision (RBP) is a new method of information retrieval system evaluation that takes into account any uncertainty due to incomplete relevance judgements for a given document and query set. To do so, RBP uses a model of user persistence. In this article, we will present a statistical analysis of the RBP user persistence model to observe how the user persistence value affects the user persistence distribution. We also provide a method of fitting data from existing users to the persistence model, in order to compute their persistence value. Using the Microsoft MSN query log, we were able to demonstrate a typical distribution of the user persistence value and show that it closely resembles a reverse lognormal distribution, with a mean of p = 0.78.  Evaluation, rank-biased precision, persistence distribution
      </response>
      <response id="128" uni="CSIRO, Queensland University of Technology" weight="0.11" year="2012">
        Exploiting Medical Hierarchies for Concept-based Information Retrieval    Search technologies are critical to enable clinical staff to rapidly and effectively access patient information contained in free-text medical records. Medical search is challenging as terms in the query are often general but those in relevant documents are very specific, leading to granularity mismatch. In this paper we propose to tackle granularity mismatch by exploiting subsumption relationships defined in formal medical domain knowledge resources. In symbolic reasoning, a subsumption (or 'is-a') relationship is a parent-child relationship where one concept is a subset of another concept. Subsumed concepts are included in the retrieval function. In addition, we investigate a number of initial methods for combining weights of query concepts and those of subsumed concepts. Subsumption relationships were found to provide strong indication of relevant information; their inclusion in retrieval functions yields performance improvements. This result motivates the development of formal models of relationships between medical concepts for retrieval purposes. Categories and Subject Descriptors  Information Storage and Retrieval: Information Search and Retrieval
      </response>
      <response id="81" uni="The University of Melbourne University College Dublin NICTA Victoria Research Laboratory" weight="0.11" year="2008">
        Exploring the benefit of contextual information for boosting TREC Genomic IR performance  Query Expansion is a widely used technique that augments a query with synonymous and related terms in order to address a common issue in ad hoc retrieval: the vocabulary mismatch problem, where relevant documents contain query terms that are semantically similar, but lexically distinct. Standard query expansion techniques include pseudo relevance feedback and ontology-based expansion. In this paper, we explore the use of contextual information as a means of expanding the context surrounding the unit of retrieval, rather than the query, which in this case is a document passage. The ad hoc retrieval task that we focus on in this paper was investigated at the TREC 2006 Genomic tracks, where systems were required to retrieve relevant answer passages. The most commonly reported indexing strategy was passage indexing. Although this simplifies post-retrieval processing, retrieval performance can be hurt as valuable contextual information in the containing document is lost. The focus of this paper is to investigate various contextual evidence of similarity outside of the passage such as: query/fulltext similarity, query/citation sentence similarity, query/title similarity, query/abstract similarity. These similarity scores are then used to boost the rank of passages that exhibit high contextual evidence of query similarity. Our experimental results suggest that document context provides the strongest evidence of contextual information for this task.  Passage Retrieval, Contextual Document Expansion and Ranking Strategies.
      </response>
      <response id="129" uni="Queensland University of Technology" weight="0.11" year="2012">
        Finding Additional Semantic Entity information for Search Engines  Entity-oriented search has become an essential component of modern search engines. It focuses on retrieving a list of entities or information about the specific entities instead of documents. In this paper, we study the problem of finding entity related information, referred to as attribute-value pairs, that play a significant role in searching target entities. We propose a novel decomposition framework combining reduced relations and the discriminative model, Conditional Random Field (CRF), for automatically finding entity-related attribute-value pairs from free text documents. This decomposition framework allows us to locate potential text fragments and identify the hidden semantics, in the form of attribute-value pairs for user queries. Empirical analysis shows that the decomposition framework outperforms pattern-based approaches due to its capability of effective integration of syntactic and semantic features.  Computing Methodologies: Natural Language Processing - Language parsing and understanding; Text analysis
      </response>
      <response id="94" uni="RMIT University" weight="0.11" year="2009">
        Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result Interfaces  The organisation, content and presentation of document surrogates has a substantial impact on the effectiveness of web search result interfaces. Most interfaces include textual information, including for example the document title, URL, and a short query-biased summary of the content. Other interfaces include additional browsing features, such as topic clustering, or thumbnails of the web pages. In this study we analyse three search interfaces, and compare the effectiveness of textual information and additional browsing features. Our analysis indicates that most users spend a substantially larger proportion of time looking at text information, and that those interfaces that focus on text-based representations of document content tend to lead to quicker task completion times for named-page finding search tasks.   Information Retrieval, User Studies Involving Documents, Web Documents, Eye Tracking
      </response>
      <response id="74" uni="RMIT University" weight="0.09" year="2007">
        A Comparison of Evaluation Measures Given How Users Perform on Search Tasks  Information retrieval has a strong foundation of empirical investigation: based on the position of relevant resources in a ranked answer list, a variety of system performance metrics can be calculated. One of the most widely reported measures, mean average precision (MAP), provides a single numerical value that aims to capture the overall performance of a retrieval system. However, recent work has suggested that broad measures such as MAP do not relate to actual user performance on a number of search tasks. In this paper, we investigate the relationship between various retrieval metrics, and consider how these reflect user search performance. Our results suggest that there are two distinct categories of measures: those that focus on high precision in an answer list, and those that attempt to capture a broader summary, for example by including a recall component. Analysis of runs submitted to the TREC terabyte track in 2006 suggests that the relative performance of systems can differ significantly depending on which group of measures is being used.  Information Retrieval, evaluation, metrics
      </response>
      <response id="86" uni="NICTA Victoria Research Laboratory, The University of Melbourne" weight="0.08" year="2008">
        Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification  Searching and selecting articles to be included in systematic reviews is a real challenge for healthcare agencies responsible for publishing these reviews. The current practice of manually reviewing all papers returned by complex hand-crafted boolean queries is human labour-intensive and difficult to maintain. We demonstrate a two-stage searching system that takes advantage of ranked queries and support-vector machine text classification to assist in the retrieval of relevant articles, and to restrict results to higher-quality documents. Our proposed approach shows significant work saved in the systematic review process over a baseline of a keyword-based retrieval system.  Information Retrieval, Machine Learning.
      </response>
      <response id="80" uni="Queensland University of Technology" weight="0.08" year="2008">
        On the relevance of documents for semantic representation  The subject of this paper is the quality of semantic vector representation with random projection under various conditions. The main effect we are watching is the size of the context in which words are observed. We are also interested in the stability of such representations since they rely on random initialisation. In particular we investigate the possibility of stabilising terms representations through documents representations. The quality of semantic representation was tested by means of synonym finding task using the TOEFL test on the TASA corpus. It was found that small context windows produces the best semantic vectors with 59.4 % of the questions correctly answered. Processing the projection between terms and documents representations several times was found not to improve the stability of the representation. It was also found not to improve the average quality of representations.  Natural Language Techniques and Documents, Semantic spaces, Random projection.
      </response>
    </responses>
  </theme>
</themes>
